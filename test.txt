============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.4, pluggy-1.5.0
rootdir: /home/kasinadhsarma/experiment/cognition-l3-experiment
plugins: cov-6.0.0, anyio-4.7.0
collected 45 items

tests/test_consciousness.py EEEEEE                                       [ 13%]
tests/test_environment.py FFF..                                          [ 24%]
tests/unit/attention/test_attention.py FFFF                              [ 33%]
tests/unit/attention/test_attention_mechanisms.py ....                   [ 42%]
tests/unit/integration/test_cognitive_integration.py EEEE                [ 51%]
tests/unit/integration/test_state_management.py F.F.                     [ 60%]
tests/unit/memory/test_integration.py FFFF                               [ 68%]
tests/unit/memory/test_memory.py EEEEE                                   [ 80%]
tests/unit/memory/test_memory_components.py EEEE                         [ 88%]
tests/unit/state/test_consciousness_state_management.py ...F.            [100%]

==================================== ERRORS ====================================
______ ERROR at setup of TestConsciousnessModel.test_model_initialization ______

self = <test_consciousness.TestConsciousnessModel object at 0x72229877db10>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim, num_heads):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
_______ ERROR at setup of TestConsciousnessModel.test_model_forward_pass _______

self = <test_consciousness.TestConsciousnessModel object at 0x72229877dcf0>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim, num_heads):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
__________ ERROR at setup of TestConsciousnessModel.test_model_config __________

self = <test_consciousness.TestConsciousnessModel object at 0x72229877ded0>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim, num_heads):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
___ ERROR at setup of TestConsciousnessModel.test_model_state_initialization ___

self = <test_consciousness.TestConsciousnessModel object at 0x72229877e110>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim, num_heads):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
_______ ERROR at setup of TestConsciousnessModel.test_model_state_update _______

self = <test_consciousness.TestConsciousnessModel object at 0x72229877e1a0>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim, num_heads):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
____ ERROR at setup of TestConsciousnessModel.test_model_attention_weights _____

self = <test_consciousness.TestConsciousnessModel object at 0x72229877e3e0>
hidden_dim = 64, num_heads = 4

    @pytest.fixture
    def model(self, hidden_dim, num_heads):
        """Create a consciousness model for testing."""
>       return ConsciousnessModel(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            num_layers=4,
            num_states=4,
            dropout_rate=0.1
        )

tests/test_consciousness.py:14: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_...se)
      (3): Linear(in_features=256, out_features=64, bias=True)
      (4): Dropout(p=0.1, inplace=False)
    )
  )
)
hidden_dim = 64, num_heads = 4, num_layers = 4, num_states = 4
dropout_rate = 0.1

    def __init__(self, hidden_dim: int, num_heads: int, num_layers: int, num_states: int, dropout_rate: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.num_states = num_states
        self.dropout_rate = dropout_rate
    
        # Global Workspace for conscious awareness
        self.global_workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=dropout_rate
        )
    
        # Working memory with GRU cells
>       self.working_memory = WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=dropout_rate
        )
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

models/consciousness_model.py:34: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_cross_modal_attention _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7222987ae0e0>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_modality_specific_processing _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7222987ae290>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_integration_stability _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7222987ae440>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
_ ERROR at setup of TestCognitiveProcessIntegration.test_cognitive_integration _

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7222987ae5f0>
device = device(type='cpu')

    @pytest.fixture
    def integration_module(self, device):
>       return CognitiveProcessIntegration(
            hidden_dim=64,
            num_heads=4,
            num_layers=3,
            dropout_rate=0.1
        ).to(device)
E       TypeError: CognitiveProcessIntegration.__init__() got an unexpected keyword argument 'num_layers'

tests/unit/integration/test_cognitive_integration.py:18: TypeError
________ ERROR at setup of TestMemoryComponents.test_gru_state_updates _________

self = <test_memory.TestMemoryComponents object at 0x722298621480>
hidden_dim = 64

    @pytest.fixture
    def gru_cell(self, hidden_dim):
        """Create GRU cell for testing."""
>       return GRUCell(hidden_dim=hidden_dim).to(self.device)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:50: TypeError
____ ERROR at setup of TestMemoryComponents.test_memory_sequence_processing ____

self = <test_memory.TestMemoryComponents object at 0x7222987afdc0>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_______ ERROR at setup of TestMemoryComponents.test_context_aware_gating _______

self = <test_memory.TestMemoryComponents object at 0x7222987acf10>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_____ ERROR at setup of TestMemoryComponents.test_information_integration ______

self = <test_memory.TestMemoryComponents object at 0x7222d2853c40>
hidden_dim = 64

    @pytest.fixture
    def info_integration(self, hidden_dim):
        """Create information integration module for testing."""
        return InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=4,
            dropout_rate=0.1
>       ).to(self.device)

tests/unit/memory/test_memory.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
args = (<bound method TestMemoryComponents.device of <test_memory.TestMemoryComponents object at 0x7222d2853c40>>,)
kwargs = {}

    def to(self, *args, **kwargs):
        r"""Move and/or cast the parameters and buffers.
    
        This can be called as
    
        .. function:: to(device=None, dtype=None, non_blocking=False)
           :noindex:
    
        .. function:: to(dtype, non_blocking=False)
           :noindex:
    
        .. function:: to(tensor, non_blocking=False)
           :noindex:
    
        .. function:: to(memory_format=torch.channels_last)
           :noindex:
    
        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
        floating point or complex :attr:`dtype`\ s. In addition, this method will
        only cast the floating point or complex parameters and buffers to :attr:`dtype`
        (if given). The integral parameters and buffers will be moved
        :attr:`device`, if that is given, but with dtypes unchanged. When
        :attr:`non_blocking` is set, it tries to convert/move asynchronously
        with respect to the host if possible, e.g., moving CPU Tensors with
        pinned memory to CUDA devices.
    
        See below for examples.
    
        .. note::
            This method modifies the module in-place.
    
        Args:
            device (:class:`torch.device`): the desired device of the parameters
                and buffers in this module
            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
                the parameters and buffers in this module
            tensor (torch.Tensor): Tensor whose dtype and device are the desired
                dtype and device for all parameters and buffers in this module
            memory_format (:class:`torch.memory_format`): the desired memory
                format for 4D parameters and buffers in this module (keyword
                only argument)
    
        Returns:
            Module: self
    
        Examples::
    
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> linear = nn.Linear(2, 2)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]])
            >>> linear.to(torch.double)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]], dtype=torch.float64)
            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)
            >>> gpu1 = torch.device("cuda:1")
            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
            >>> cpu = torch.device("cpu")
            >>> linear.to(cpu)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16)
    
            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.3741+0.j,  0.2382+0.j],
                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
            tensor([[0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
    
        """
>       device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
            *args, **kwargs
        )
E       TypeError: to() received an invalid combination of arguments - got (method), but expected one of:
E        * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1299: TypeError
_________ ERROR at setup of TestMemoryComponents.test_memory_retention _________

self = <test_memory.TestMemoryComponents object at 0x722298620cd0>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
>       return WorkingMemory(
            hidden_dim=hidden_dim,
            dropout_rate=0.1
        ).to(self.device)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory.py:33: TypeError
_____________ ERROR at setup of TestGRUCell.test_gru_state_updates _____________

self = <test_memory_components.TestGRUCell object at 0x722298621ed0>

    @pytest.fixture
    def gru_cell(self):
>       return GRUCell(hidden_dim=64)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:13: TypeError
______________ ERROR at setup of TestGRUCell.test_gru_reset_gate _______________

self = <test_memory_components.TestGRUCell object at 0x722298622080>

    @pytest.fixture
    def gru_cell(self):
>       return GRUCell(hidden_dim=64)
E       TypeError: GRUCell.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:13: TypeError
_________ ERROR at setup of TestWorkingMemory.test_sequence_processing _________

self = <test_memory_components.TestWorkingMemory object at 0x722298622380>

    @pytest.fixture
    def memory_module(self):
>       return WorkingMemory(hidden_dim=64, dropout_rate=0.1)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:73: TypeError
__________ ERROR at setup of TestWorkingMemory.test_memory_retention ___________

self = <test_memory_components.TestWorkingMemory object at 0x722298622530>

    @pytest.fixture
    def memory_module(self):
>       return WorkingMemory(hidden_dim=64, dropout_rate=0.1)
E       TypeError: WorkingMemory.__init__() missing 1 required positional argument: 'input_dim'

tests/unit/memory/test_memory_components.py:73: TypeError
=================================== FAILURES ===================================
______________________ EnvironmentTests.test_core_imports ______________________

self = <test_environment.EnvironmentTests testMethod=test_core_imports>

    def test_core_imports(self):
        """Test all core framework imports"""
        try:
            import torch
>           import torchvision

tests/test_environment.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164: in <module>
    def meta_nms(dets, scores, iou_threshold):
../../.local/lib/python3.10/site-packages/torch/library.py:795: in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
../../.local/lib/python3.10/site-packages/torch/library.py:184: in _register_fake
    handle = entry.fake_impl.register(func_to_register, source)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch._library.fake_impl.FakeImplHolder object at 0x72229868b220>
func = <function meta_nms at 0x7222986f4550>
source = '/home/kasinadhsarma/.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164'

    def register(self, func: Callable, source: str) -> RegistrationHandle:
        """Register an fake impl.
    
        Returns a RegistrationHandle that one can use to de-register this
        fake impl.
        """
        if self.kernel is not None:
            raise RuntimeError(
                f"register_fake(...): the operator {self.qualname} "
                f"already has an fake impl registered at "
                f"{self.kernel.source}."
            )
>       if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
E       RuntimeError: operator torchvision::nms does not exist

../../.local/lib/python3.10/site-packages/torch/_library/fake_impl.py:31: RuntimeError
___________________ EnvironmentTests.test_framework_versions ___________________

self = <test_environment.EnvironmentTests testMethod=test_framework_versions>

    def test_framework_versions(self):
        """Verify framework versions"""
        import torch
>       import torchvision

tests/test_environment.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26: in <module>
    def meta_roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function meta_roi_align at 0x7222986f6290>

    def wrapper(fn):
>       if torchvision.extension._has_ops():
E       AttributeError: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)

../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18: AttributeError
___________________ EnvironmentTests.test_hardware_detection ___________________

self = <test_environment.EnvironmentTests testMethod=test_hardware_detection>

    def test_hardware_detection(self):
        """Test hardware detection and configuration"""
        import torch
    
        # Check if CUDA is available
        cuda_available = torch.cuda.is_available()
>       self.assertTrue(cuda_available, "CUDA is not available")
E       AssertionError: False is not true : CUDA is not available

tests/test_environment.py:34: AssertionError
_______________ TestAttentionMechanisms.test_scaled_dot_product ________________

self = <test_attention.TestAttentionMechanisms object at 0x7222987ac1f0>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_scaled_dot_product(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test scaled dot-product attention computation."""
        # Create inputs
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:41: TypeError
_________________ TestAttentionMechanisms.test_attention_mask __________________

self = <test_attention.TestAttentionMechanisms object at 0x7222987ac400>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_attention_mask(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test attention mask handling."""
        # Create inputs and mask
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:56: TypeError
___________ TestAttentionMechanisms.test_consciousness_broadcasting ____________

self = <test_attention.TestAttentionMechanisms object at 0x72229877e2f0>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_consciousness_broadcasting(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test consciousness-aware broadcasting."""
>       inputs_q = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:69: TypeError
__________ TestAttentionMechanisms.test_global_workspace_integration ___________

self = <test_attention.TestAttentionMechanisms object at 0x7222987ac220>
batch_size = 2, seq_length = 8, hidden_dim = 128, num_heads = 4

    def test_global_workspace_integration(self, batch_size, seq_length, hidden_dim, num_heads):
        """Test global workspace integration."""
        workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=0.1
        )
    
>       inputs = self.create_inputs(batch_size, seq_length, hidden_dim)
E       TypeError: ConsciousnessTestBase.create_inputs() missing 1 required positional argument: 'hidden_dim'

tests/unit/attention/test_attention.py:88: TypeError
_______________ TestConsciousnessStateManager.test_state_updates _______________

self = <test_state_management.TestConsciousnessStateManager object at 0x7222987ad2a0>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_state_updates(self, device, state_manager):
        # Test dimensions
        batch_size = 2
        hidden_dim = 64
    
        # Create sample state and inputs
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        # Initialize parameters
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test output shapes
        assert new_state.shape == state.shape
        assert 'memory_gate' in metrics
        assert 'energy_cost' in metrics
        assert 'state_value' in metrics
    
        # Test memory gate properties
>       assert metrics['memory_gate'].shape == (batch_size, hidden_dim)
E       assert torch.Size([2, 1]) == (2, 64)
E         
E         At index 1 diff: 1 != 64
E         Use -v to get more diff

tests/unit/integration/test_state_management.py:44: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5560],
        [0.4816]])
state: tensor([[-0.1592,  0.6048,  3.3121, -1.7222,  0.1989, -0.3884,  0.9136,  0.6161,
          1.3679, -0.8690, -0.1496,  0.5197, -0.0163,  0.5932, -1.3102,  1.4252,
         -1.0375, -0.9599, -1.1119,  0.2859,  0.1409, -0.8480, -0.6554,  0.7941,
          0.6754,  0.0445,  0.1225, -1.0826, -1.2513, -0.8607, -0.1808,  0.7640,
         -0.2841,  0.5548, -0.7407, -1.6721,  0.5816,  0.0455, -0.8662,  1.8590,
          1.2583, -0.6584,  1.2882, -0.3595, -0.7082, -0.0758,  0.2565, -0.4953,
         -0.4299, -0.1593,  0.5269, -1.8650,  0.9647,  0.1861, -0.9503, -0.0987,
          0.6216,  0.0937,  0.2598, -0.1318, -0.0823,  0.7656, -0.1615, -2.3476],
        [ 0.0443, -1.0161, -0.3232,  0.6590,  0.3035,  1.1946,  0.1137, -0.7427,
          1.1819, -0.1011,  0.9352, -0.1946,  0.5365, -0.1376, -0.2363,  0.5718,
         -0.7074, -0.2791,  0.1362,  0.1572, -0.4887, -0.8174,  0.3478, -1.0698,
          0.8080,  0.2349, -0.1674,  1.2518,  0.5238,  1.0650,  0.3773, -0.6672,
         -0.4118, -0.3052,  1.2615,  1.5948,  1.5086,  0.1431, -0.0801, -1.1136,
          1.6104,  1.1565, -1.0234, -0.1544, -1.0709,  1.2028,  0.3622, -0.3500,
          0.1492, -0.1033,  0.1138, -2.0786,  0.2398,  0.6818,  1.1204,  0.5431,
         -1.8260, -2.3178, -0.2160, -0.6623,  0.6431,  0.9867, -0.2173,  1.4519]])
inputs: tensor([[ 0.3924,  1.7011,  0.2234,  0.9268, -1.2027, -2.0622, -0.8836, -1.1915,
          0.1591,  0.3747, -0.7075,  0.8014, -1.6902,  0.1859, -1.0313, -1.0001,
          0.4371,  0.2260, -0.6627, -0.0382,  0.1646,  0.2282, -0.7275,  0.4377,
         -1.2890,  0.7395, -1.5828, -0.5278, -1.0254,  0.2153, -0.1356, -0.2424,
         -0.3294, -0.6249,  1.0459, -1.0076, -0.0852, -1.2158, -0.6813,  0.2917,
          0.4693,  0.3180,  1.2373, -1.7305,  1.0017,  0.7187, -0.2099, -2.0792,
         -1.3219, -0.8440,  1.7527,  0.7824,  1.0387, -0.0263,  0.2016, -1.1403,
          0.5286, -0.7378,  0.7007,  2.0292,  0.0686, -1.1304, -0.4308, -0.7052],
        [-0.2377,  1.6532, -0.4278,  0.8545, -0.5659, -1.6246, -0.3079,  0.1175,
          0.8385,  1.8712,  1.0078,  2.1799, -1.7390, -0.7276,  0.1939,  0.4989,
         -0.3792,  0.0034,  1.4330, -0.2617, -0.7353,  0.1886, -0.5720, -0.7776,
         -0.4056,  0.6879,  0.2609,  0.4099,  0.7871,  0.3673,  0.4683, -1.6515,
         -0.4905,  1.0959,  0.3408,  0.4335, -1.6967, -0.6885, -0.8228,  0.3164,
          0.7861, -0.5945, -2.4821, -0.2090, -1.3245, -0.6711,  1.3676, -0.1895,
         -1.7315,  0.4603,  0.1144, -1.5362,  1.3537, -0.8739, -0.0899, -1.1320,
          1.8269,  0.4603,  0.5347, -1.0540,  1.5253,  0.7899,  0.6350,  0.0549]])
candidate_state: tensor([[-6.4484e-02, -1.6976e-01,  6.3737e-01, -3.3927e-02,  8.1995e-02,
         -1.3643e-01, -1.0346e-01, -1.4986e-01,  3.8532e-01, -1.6979e-01,
         -7.0241e-02, -2.7899e-02, -1.3041e-01, -1.0812e-01,  8.3525e-02,
         -1.3971e-01, -1.0973e-01,  1.1734e-01,  4.2108e-02,  4.0579e-01,
         -9.1563e-02, -1.0433e-01,  1.0046e+00, -1.6825e-01, -1.4399e-01,
         -1.6349e-01, -9.4282e-02,  2.0991e-01,  8.9688e-01,  4.4492e-01,
         -1.6255e-01, -1.6840e-01,  8.5873e-01, -1.6688e-01, -2.8581e-02,
          5.6366e-02,  6.7254e-01, -8.0249e-03, -5.9197e-02,  1.2005e+00,
         -6.3471e-02,  2.0603e-01,  1.6905e-01,  1.1309e-01, -1.6842e-01,
         -1.6918e-01, -1.6886e-01,  4.4361e-02, -1.0727e-01,  1.1803e+00,
          3.0029e-01,  3.3878e-01, -1.1307e-01, -1.0306e-01,  2.5784e-01,
         -1.6440e-01,  1.8417e-02, -7.8599e-02,  6.6008e-02, -1.0063e-01,
         -1.6921e-01, -1.2893e-03, -1.6919e-01,  6.7997e-02],
        [ 8.9818e-01, -1.6484e-01,  6.9074e-01,  7.0909e-01,  2.0643e-01,
          1.1724e+00, -1.6307e-02,  2.2374e-01, -1.6034e-01, -8.1083e-02,
          7.1372e-02,  6.7357e-01, -1.4334e-01, -1.5348e-01, -9.5006e-02,
         -1.6915e-01, -1.4122e-01,  2.6526e-01, -9.7538e-02, -9.6024e-02,
          4.2080e-01,  6.7681e-01, -1.6268e-02, -2.2090e-02, -1.6773e-01,
         -1.4316e-01,  1.1108e-03,  4.9152e-01,  5.5187e-01, -1.6996e-01,
          4.0434e-01,  3.9124e-01,  9.1636e-02, -1.6408e-01, -1.6966e-01,
         -1.5111e-01,  1.4816e-02,  2.2367e-01, -1.6937e-01,  1.4951e-01,
          2.3142e-01,  1.1661e-01,  3.5571e-02, -1.5810e-01,  1.9287e-01,
          6.0316e-01, -5.0235e-02, -1.0259e-01,  2.9341e-01,  1.0701e+00,
         -9.0688e-02,  7.4710e-01, -1.6822e-01,  1.1109e-01, -1.0113e-01,
         -1.4127e-01, -1.4622e-01, -5.4980e-02, -2.6050e-02,  6.9474e-02,
          4.9056e-01, -1.6997e-01, -1.3596e-01, -7.5123e-02]])
new_state: tensor([[-0.1171,  0.2609,  2.1246, -0.9726,  0.1470, -0.2765,  0.4620,  0.2760,
          0.9317, -0.5586, -0.1144,  0.2766, -0.0670,  0.2819, -0.6914,  0.7304,
         -0.6256, -0.4816, -0.5995,  0.3391,  0.0377, -0.5178,  0.0816,  0.3668,
          0.3116, -0.0479,  0.0263, -0.5087, -0.2976, -0.2810, -0.1727,  0.3500,
          0.2233,  0.2344, -0.4246, -0.9047,  0.6220,  0.0217, -0.5079,  1.5667,
          0.6715, -0.2746,  0.7913, -0.1497, -0.4685, -0.1173,  0.0677, -0.2557,
         -0.2867,  0.4354,  0.4263, -0.8866,  0.4862,  0.0577, -0.4139, -0.1279,
          0.3538,  0.0172,  0.1738, -0.1180, -0.1209,  0.4251, -0.1649, -1.2751],
        [ 0.4869, -0.5748,  0.2024,  0.6850,  0.2532,  1.1831,  0.0463, -0.2417,
          0.4861, -0.0907,  0.4874,  0.2554,  0.1841, -0.1458, -0.1631,  0.1877,
         -0.4139,  0.0030,  0.0150,  0.0259, -0.0172, -0.0429,  0.1591, -0.5267,
          0.3022,  0.0389, -0.0800,  0.8577,  0.5383,  0.4248,  0.3913, -0.1186,
         -0.1508, -0.2321,  0.5197,  0.6898,  0.7343,  0.1849, -0.1264, -0.4589,
          0.8956,  0.6175, -0.4745, -0.1563, -0.4158,  0.8920,  0.1484, -0.2217,
          0.2240,  0.5049,  0.0078, -0.6139,  0.0283,  0.3860,  0.4872,  0.1884,
         -0.9553, -1.1449, -0.1175, -0.2830,  0.5640,  0.3871, -0.1752,  0.6604]])
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x7222987aed40>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_adaptive_gating(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            # Test adaptation to different input patterns
            # Case 1: Similar input to current state
            similar_input = state + torch.randn_like(state) * 0.1
            _, metrics1 = state_manager(state, similar_input, threshold=0.5, deterministic=True)
    
            # Case 2: Very different input
            different_input = torch.randn(batch_size, hidden_dim, device=device)
            _, metrics2 = state_manager(state, different_input, threshold=0.5, deterministic=True)
    
        # Memory gate should be more open (lower values) for different inputs
>       assert torch.mean(metrics1['memory_gate']) > torch.mean(metrics2['memory_gate'])
E       assert tensor(0.5089) > tensor(0.5287)
E        +  where tensor(0.5089) = <built-in method mean of type object at 0x7222d16678c0>(tensor([[0.4804],\n        [0.5374]]))
E        +    where <built-in method mean of type object at 0x7222d16678c0> = torch.mean
E        +  and   tensor(0.5287) = <built-in method mean of type object at 0x7222d16678c0>(tensor([[0.5102],\n        [0.5472]]))
E        +    where <built-in method mean of type object at 0x7222d16678c0> = torch.mean

tests/unit/integration/test_state_management.py:97: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4804],
        [0.5374]])
state: tensor([[-2.1059e+00, -7.5321e-01, -4.3971e-01, -1.2554e+00, -7.4770e-01,
          4.2385e-01, -1.1314e+00, -1.9166e+00, -9.4060e-01, -6.7422e-01,
          1.2361e+00, -6.8364e-01, -4.4122e-01, -1.4620e+00,  6.8171e-01,
          7.8039e-02,  1.7689e-01, -1.5880e+00, -3.1964e-01, -6.2525e-01,
         -8.0810e-01, -1.4810e+00,  7.4233e-01,  1.4897e+00, -1.5358e+00,
          5.1281e-01, -1.2815e-01,  1.5281e+00,  9.5073e-01,  6.2493e-01,
          6.1251e-01, -1.4842e-02,  2.6488e-01,  8.0983e-01,  2.2514e-01,
         -2.6607e-01, -7.7667e-02, -2.9557e-02, -4.1915e-01,  8.8658e-01,
         -1.6472e+00, -1.3873e+00,  2.2522e-01,  5.5965e-02,  7.0133e-02,
         -9.3403e-01,  1.7817e+00, -4.1339e-01,  7.3375e-01, -7.8425e-01,
          3.2069e-01, -1.8905e+00,  1.2889e-01, -6.1698e-01, -1.1567e+00,
         -1.1663e-01,  1.5069e-01,  4.5738e-01,  1.2235e+00,  1.2066e+00,
         -4.6229e-01, -6.4960e-01,  1.0320e-01,  3.2293e-01],
        [ 1.4665e+00,  5.8829e-01,  4.9435e-04,  1.7249e+00, -1.1726e+00,
         -4.9551e-01, -5.4246e-01, -5.3885e-02,  1.7741e+00,  7.4282e-01,
         -5.2571e-01, -3.9245e-01, -4.4713e-01,  2.8039e+00, -1.7818e-01,
         -3.8924e-01, -1.2940e+00,  5.2898e-02,  1.2952e+00, -2.6612e-01,
          2.0781e-01,  1.3435e+00, -3.9161e-01,  1.9310e-01,  1.8522e+00,
         -2.2051e+00,  1.0410e+00,  1.1348e+00,  2.8806e-01,  1.5560e+00,
         -8.3492e-01, -1.0830e+00,  1.7924e+00, -2.1383e+00, -5.7772e-01,
         -2.4523e+00,  7.3723e-01,  2.0068e-01,  5.7979e-02,  5.9276e-01,
          7.8743e-01,  1.6781e+00,  1.4209e+00,  6.1494e-01,  2.9040e-01,
         -4.2683e-01,  1.0502e-01,  5.2594e-01, -6.1319e-03,  6.1471e-01,
          1.5476e+00,  6.5824e-01, -7.7044e-01,  1.6086e-01, -1.4530e+00,
          1.9184e-01,  1.1764e+00,  1.2015e+00,  5.2642e-01, -1.6544e-01,
         -4.7235e-01, -2.0051e+00,  2.3736e-01, -3.8971e-02]])
inputs: tensor([[-2.0213, -0.6808, -0.4568, -1.1978, -0.6782,  0.3903, -1.0661, -1.8666,
         -1.0234, -0.7542,  1.3481, -0.7110, -0.4220, -1.5325,  0.9172, -0.0960,
          0.2501, -1.6238, -0.3867, -0.7375, -0.8428, -1.3446,  0.4795,  1.3432,
         -1.5016,  0.3341, -0.2468,  1.4061,  0.9564,  0.6064,  0.6877, -0.0984,
          0.3496,  0.8420,  0.1861, -0.2851,  0.0095, -0.1582, -0.2594,  0.8601,
         -1.6484, -1.2027,  0.2852, -0.1117,  0.0232, -0.8304,  1.9247, -0.4598,
          0.7582, -0.8561,  0.1700, -1.8322,  0.3331, -0.5359, -1.1198, -0.1521,
          0.1087,  0.6104,  1.2725,  1.0870, -0.3310, -0.4512,  0.0354,  0.4051],
        [ 1.4053,  0.7892, -0.0040,  1.7946, -1.1685, -0.4530, -0.5975,  0.0168,
          1.7133,  0.6963, -0.6595, -0.3494, -0.4889,  2.6898, -0.0944, -0.3924,
         -1.1699, -0.1250,  1.3918, -0.0815,  0.1373,  1.4787, -0.3311,  0.2028,
          1.8298, -2.0866,  1.0331,  1.1288,  0.2746,  1.4745, -0.9920, -1.0111,
          1.8101, -2.1284, -0.4345, -2.3753,  0.7576,  0.2838,  0.0062,  0.4522,
          0.8157,  1.7234,  1.4519,  0.7291,  0.2849, -0.5569,  0.1459,  0.4233,
          0.0104,  0.6703,  1.5263,  0.7710, -0.5756,  0.1228, -1.4416,  0.1745,
          1.1159,  1.1395,  0.4622, -0.1773, -0.4763, -2.0074,  0.3262, -0.0747]])
candidate_state: tensor([[-0.1693, -0.1684, -0.1675,  0.3367, -0.0926, -0.1232, -0.0795,  1.1428,
         -0.1117,  0.1637,  1.0769,  0.0302,  0.5211,  0.5654, -0.1688, -0.1639,
         -0.0600,  0.4308,  0.5210, -0.1694,  0.0640, -0.1660,  0.1200, -0.1700,
          0.2002, -0.1427, -0.0253,  0.8436, -0.0768, -0.1296,  0.1858,  0.3078,
         -0.0702,  0.1492, -0.0256, -0.1474,  0.3238,  0.1368,  0.3833, -0.0133,
          0.1259, -0.0719,  0.1428, -0.0108, -0.1678, -0.1213,  0.1600,  0.5627,
          0.0068, -0.1647, -0.1461,  1.1339, -0.1184, -0.1276, -0.1072, -0.0694,
          0.3756,  0.1464, -0.1699, -0.0794,  0.2170, -0.1099,  0.2738,  0.0695],
        [ 0.7956, -0.1103, -0.1560, -0.1691,  0.1228,  0.0108,  0.5231,  0.0082,
          0.1018, -0.0110,  0.4552,  0.3858, -0.1642, -0.1691, -0.1050,  0.1050,
         -0.0818, -0.1014,  0.1498,  0.7150, -0.1177,  0.2175,  0.0262,  0.1247,
         -0.1514,  0.1569,  0.1001, -0.1691, -0.0655, -0.1580, -0.1593,  0.1223,
          0.2126, -0.0170,  0.2307, -0.1448,  1.0682,  0.3000, -0.1321,  0.0893,
          0.1187,  0.0019, -0.1239, -0.1698, -0.1285,  0.6044, -0.1248, -0.1650,
         -0.1674, -0.1397, -0.0781,  1.0305,  0.0022,  0.1297,  0.1319,  0.5350,
         -0.0730,  0.0249,  0.0952, -0.1551,  0.0920,  0.3320, -0.0298,  0.0337]])
new_state: tensor([[-1.0996e+00, -4.4935e-01, -2.9827e-01, -4.2818e-01, -4.0732e-01,
          1.3961e-01, -5.8486e-01, -3.2695e-01, -5.0991e-01, -2.3887e-01,
          1.1534e+00, -3.1274e-01,  5.8783e-02, -4.0861e-01,  2.3982e-01,
         -4.7664e-02,  5.3808e-02, -5.3904e-01,  1.1712e-01, -3.8841e-01,
         -3.5497e-01, -7.9772e-01,  4.1898e-01,  6.2734e-01, -6.3380e-01,
          1.7223e-01, -7.4689e-02,  1.1725e+00,  4.1685e-01,  2.3288e-01,
          3.9081e-01,  1.5279e-01,  9.0763e-02,  4.6657e-01,  9.4840e-02,
         -2.0442e-01,  1.3095e-01,  5.6868e-02, -2.2113e-03,  4.1902e-01,
         -7.2596e-01, -7.0384e-01,  1.8239e-01,  2.1271e-02, -5.3473e-02,
         -5.1176e-01,  9.3911e-01,  9.3774e-02,  3.5604e-01, -4.6236e-01,
          7.8179e-02, -3.1907e-01,  4.1705e-04, -3.6270e-01, -6.1139e-01,
         -9.2068e-02,  2.6756e-01,  2.9581e-01,  4.9948e-01,  5.3841e-01,
         -1.0933e-01, -3.6920e-01,  1.9182e-01,  1.9127e-01],
        [ 1.1562e+00,  2.6512e-01, -7.1918e-02,  8.4865e-01, -5.7329e-01,
         -2.6126e-01, -4.9512e-02, -2.5147e-02,  1.0004e+00,  3.9407e-01,
         -7.1903e-02, -3.2405e-02, -3.1622e-01,  1.4285e+00, -1.4431e-01,
         -1.6057e-01, -7.3317e-01, -1.8465e-02,  7.6529e-01,  1.8778e-01,
          5.7219e-02,  8.2257e-01, -1.9833e-01,  1.6147e-01,  9.2528e-01,
         -1.1124e+00,  6.0571e-01,  5.3157e-01,  1.2449e-01,  7.6303e-01,
         -5.2238e-01, -5.2542e-01,  1.0615e+00, -1.1569e+00, -2.0373e-01,
         -1.3848e+00,  8.9035e-01,  2.4664e-01, -2.9953e-02,  3.5984e-01,
          4.7804e-01,  9.0267e-01,  7.0625e-01,  2.5192e-01,  9.6584e-02,
          5.0254e-02, -1.3209e-03,  2.0628e-01, -8.0736e-02,  2.6569e-01,
          7.9551e-01,  8.3046e-01, -4.1299e-01,  1.4643e-01, -7.1980e-01,
          3.5058e-01,  5.9841e-01,  6.5716e-01,  3.2693e-01, -1.6067e-01,
         -2.1127e-01, -9.2393e-01,  1.1377e-01, -5.3449e-03]])
memory_gate: tensor([[0.5102],
        [0.5472]])
state: tensor([[-2.1059e+00, -7.5321e-01, -4.3971e-01, -1.2554e+00, -7.4770e-01,
          4.2385e-01, -1.1314e+00, -1.9166e+00, -9.4060e-01, -6.7422e-01,
          1.2361e+00, -6.8364e-01, -4.4122e-01, -1.4620e+00,  6.8171e-01,
          7.8039e-02,  1.7689e-01, -1.5880e+00, -3.1964e-01, -6.2525e-01,
         -8.0810e-01, -1.4810e+00,  7.4233e-01,  1.4897e+00, -1.5358e+00,
          5.1281e-01, -1.2815e-01,  1.5281e+00,  9.5073e-01,  6.2493e-01,
          6.1251e-01, -1.4842e-02,  2.6488e-01,  8.0983e-01,  2.2514e-01,
         -2.6607e-01, -7.7667e-02, -2.9557e-02, -4.1915e-01,  8.8658e-01,
         -1.6472e+00, -1.3873e+00,  2.2522e-01,  5.5965e-02,  7.0133e-02,
         -9.3403e-01,  1.7817e+00, -4.1339e-01,  7.3375e-01, -7.8425e-01,
          3.2069e-01, -1.8905e+00,  1.2889e-01, -6.1698e-01, -1.1567e+00,
         -1.1663e-01,  1.5069e-01,  4.5738e-01,  1.2235e+00,  1.2066e+00,
         -4.6229e-01, -6.4960e-01,  1.0320e-01,  3.2293e-01],
        [ 1.4665e+00,  5.8829e-01,  4.9435e-04,  1.7249e+00, -1.1726e+00,
         -4.9551e-01, -5.4246e-01, -5.3885e-02,  1.7741e+00,  7.4282e-01,
         -5.2571e-01, -3.9245e-01, -4.4713e-01,  2.8039e+00, -1.7818e-01,
         -3.8924e-01, -1.2940e+00,  5.2898e-02,  1.2952e+00, -2.6612e-01,
          2.0781e-01,  1.3435e+00, -3.9161e-01,  1.9310e-01,  1.8522e+00,
         -2.2051e+00,  1.0410e+00,  1.1348e+00,  2.8806e-01,  1.5560e+00,
         -8.3492e-01, -1.0830e+00,  1.7924e+00, -2.1383e+00, -5.7772e-01,
         -2.4523e+00,  7.3723e-01,  2.0068e-01,  5.7979e-02,  5.9276e-01,
          7.8743e-01,  1.6781e+00,  1.4209e+00,  6.1494e-01,  2.9040e-01,
         -4.2683e-01,  1.0502e-01,  5.2594e-01, -6.1319e-03,  6.1471e-01,
          1.5476e+00,  6.5824e-01, -7.7044e-01,  1.6086e-01, -1.4530e+00,
          1.9184e-01,  1.1764e+00,  1.2015e+00,  5.2642e-01, -1.6544e-01,
         -4.7235e-01, -2.0051e+00,  2.3736e-01, -3.8971e-02]])
inputs: tensor([[ 1.4147,  0.7941, -0.3019,  1.1591, -0.8171,  0.3681, -1.1780, -1.0753,
          0.5802, -1.3573, -1.2853, -1.1281,  0.4472, -0.2929, -0.0200, -1.0621,
          0.4157, -0.6857,  1.0383, -0.3356, -1.1199, -2.5076, -0.1796, -0.3992,
          0.5603, -0.9331,  0.0855, -1.3531, -0.1642, -0.2584,  0.5672, -0.7413,
         -0.2100, -1.1364, -0.0035,  0.5750, -1.1764,  0.0748,  0.7637,  0.8702,
         -0.0309,  3.2683,  0.8391, -2.0635, -1.1574, -0.0052, -1.4289,  0.6459,
         -1.1799, -1.5465, -0.0886, -1.2460, -1.9065, -1.7657,  0.8543, -0.1815,
         -0.2525,  1.5680, -0.3444, -1.0294,  0.1750, -0.7144,  0.2004, -0.2040],
        [ 2.9274,  0.3551, -1.8666, -0.4704,  0.6821, -1.4971,  0.7964, -2.0832,
         -0.4385, -0.2198, -1.6846, -0.0980, -1.5201, -0.3277, -0.7289, -0.0679,
         -1.3591,  0.9751,  1.1343,  0.7892,  0.2244,  0.9513, -1.5518, -0.6871,
          0.1224, -0.7134,  0.3076,  0.8522,  0.8876,  2.7633, -1.1970, -1.1056,
          1.3495,  0.0488, -1.0001,  0.0505, -0.9268,  0.9677, -0.9733,  1.3494,
         -0.9615, -0.1379, -0.0526,  0.5767, -0.0053,  0.5975, -0.7295, -1.1229,
          1.3549, -0.1560,  0.7673, -0.2398,  0.6520,  0.0451,  1.4532, -0.9022,
          0.0980,  0.9535, -2.3943, -2.5711,  0.1358, -0.2244,  1.4141, -0.5169]])
candidate_state: tensor([[-1.6949e-01, -7.4564e-05, -1.2754e-01, -1.4773e-01, -5.0739e-02,
          2.8472e-01,  2.2478e-01,  8.9648e-01,  6.6622e-01,  1.0661e-01,
          2.1466e-01, -8.3602e-02, -1.1965e-01, -4.6318e-02, -1.6544e-01,
          2.1489e-01, -2.9174e-02, -1.4604e-01,  1.1435e+00,  6.2654e-02,
          8.1042e-01,  8.3969e-01,  9.7276e-02, -1.3239e-01,  5.6257e-02,
         -1.2018e-01,  4.5088e-01,  3.7547e-01, -1.5542e-01, -1.4666e-01,
         -1.8382e-02,  1.2172e+00,  6.5109e-01, -1.5692e-01,  1.2749e-01,
          7.3373e-01, -1.6958e-01, -8.3627e-02,  4.6942e-01, -1.4454e-01,
         -1.5791e-02,  4.5595e-01,  4.2533e-01,  1.2394e-01,  3.9647e-01,
          4.6699e-01, -1.4319e-01, -1.6974e-01,  1.2264e-01, -1.3046e-01,
          3.4955e-01,  2.2499e-01,  1.8768e-01,  4.4814e-01, -1.4125e-01,
         -1.6997e-01, -1.6735e-01,  3.1576e-02,  6.5562e-01, -1.1250e-02,
         -4.9420e-02, -6.9862e-02, -1.5294e-01, -1.6673e-01],
        [ 4.3580e-01,  7.5964e-01, -1.6984e-01, -1.5536e-01,  1.4281e-01,
         -1.2565e-01, -1.1823e-01, -1.5264e-01,  2.7007e-01,  1.5038e-02,
         -4.5603e-03, -3.6648e-02, -1.5477e-01,  2.8335e-01, -1.3841e-01,
          4.8505e-01,  7.0156e-01, -1.2800e-01,  3.4172e-01, -9.9815e-02,
          3.3090e-01,  1.3383e-01,  3.5162e-02,  3.0477e-01,  1.0906e+00,
         -9.9045e-02, -7.7629e-02,  6.4612e-01,  1.9031e-01, -2.9398e-02,
          2.6808e-01,  4.1302e-02, -1.1577e-01,  3.0292e-01,  4.2145e-02,
         -1.3296e-01,  3.6910e-01,  1.0843e-01,  1.1339e-01,  8.6555e-01,
         -5.2156e-02, -9.4076e-02, -9.6086e-02, -1.6097e-01, -1.6471e-01,
         -1.4561e-01,  1.0146e-01,  1.1656e-02, -1.2831e-01, -6.5825e-02,
         -1.6765e-01,  1.9509e-02, -1.4739e-01, -1.6553e-01, -1.3010e-01,
          1.9655e-01,  1.4404e+00,  5.6051e-01, -1.0608e-01, -1.4744e-01,
         -6.0011e-03,  8.6886e-01,  1.6846e-01,  8.9836e-01]])
new_state: tensor([[-1.1575, -0.3843, -0.2868, -0.7129, -0.4063,  0.3557, -0.4672, -0.5388,
         -0.1536, -0.2918,  0.7358, -0.3897, -0.2837, -0.7686,  0.2668,  0.1451,
          0.0760, -0.8817,  0.3970, -0.2883, -0.0154, -0.3444,  0.4264,  0.6952,
         -0.7560,  0.2028,  0.1555,  0.9636,  0.4090,  0.2470,  0.3035,  0.5886,
          0.4540,  0.3363,  0.1773,  0.2236, -0.1227, -0.0560,  0.0161,  0.3816,
         -0.8482, -0.4845,  0.3232,  0.0893,  0.2300, -0.2478,  0.8389, -0.2941,
          0.4344, -0.4640,  0.3348, -0.8544,  0.1577, -0.0953, -0.6593, -0.1428,
         -0.0051,  0.2488,  0.9454,  0.6101, -0.2601, -0.3657, -0.0223,  0.0831],
        [ 0.9998,  0.6659, -0.0766,  0.8735, -0.5769, -0.3280, -0.3504, -0.0986,
          1.0930,  0.4133, -0.2897, -0.2313, -0.3147,  1.6625, -0.1602,  0.0067,
         -0.3903, -0.0290,  0.8634, -0.1908,  0.2635,  0.7957, -0.1984,  0.2437,
          1.5074, -1.2514,  0.5345,  0.9135,  0.2438,  0.8381, -0.3355, -0.5739,
          0.9283, -1.0329, -0.2970, -1.4020,  0.5705,  0.1589,  0.0831,  0.7163,
          0.4072,  0.8756,  0.7340,  0.2636,  0.0843, -0.2995,  0.1034,  0.2931,
         -0.0615,  0.3065,  0.7709,  0.3690, -0.4883,  0.0131, -0.8540,  0.1940,
          1.2959,  0.9112,  0.2400, -0.1573, -0.2612, -0.7037,  0.2062,  0.3855]])
____________ TestInformationIntegration.test_phi_metric_computation ____________

self = <test_integration.TestInformationIntegration object at 0x7222987afaf0>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_phi_metric_computation(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Create sample inputs
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Initialize parameters
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[ 3.2775e-01, -2.1988e-01, -4.9403e-01,  4.4538e-01, -2.6492e+00,
           1.5263e+00,  5.4629e-01,  6.7919...e+00,
          -9.8852e-01,  4.2363e-01,  7.6674e-01, -4.3391e-01,  2.0963e-01,
          -4.4665e-01, -2.5078e+00]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_______________ TestInformationIntegration.test_information_flow _______________

self = <test_integration.TestInformationIntegration object at 0x7222987afca0>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_information_flow(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = torch.zeros(batch_size, num_modules, input_dim, device=device)  # ensure shape matches the model
    
        # Test with and without dropout
        integration_module.train()
>       output1, _ = integration_module(inputs)

tests/unit/memory/test_integration.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0....., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_____________ TestInformationIntegration.test_entropy_calculations _____________

self = <test_integration.TestInformationIntegration object at 0x7222987afe50>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_entropy_calculations(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        # Test with different input distributions
        # Uniform distribution
        uniform_input = torch.ones(batch_size, num_modules, input_dim, device=device)
        integration_module.eval()
        with torch.no_grad():
>           _, phi_uniform = integration_module(uniform_input)

tests/unit/memory/test_integration.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1....., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
______________ TestInformationIntegration.test_memory_integration ______________

self = <test_integration.TestInformationIntegration object at 0x722298620040>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_memory_integration(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32
    
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Process through integration
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[ 0.3334, -0.8084, -1.7158,  0.5590, -0.0170, -1.1129, -0.4822,
          -0.2190, -1.4709, -1.0218,  2.1438,...        -0.4634, -1.5018, -0.2243,  1.6890, -0.8827,  1.3863, -0.8588,
          -0.2242,  0.0773,  0.2583, -0.2053]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_______________ TestStateManagement.test_state_value_estimation ________________

self = <test_consciousness_state_management.TestStateManagement object at 0x7222986222c0>
state_manager = ConsciousnessStateManager(), batch_size = 2, hidden_dim = 64

    def test_state_value_estimation(self, state_manager, batch_size, hidden_dim):
        """Test state value estimation."""
        consciousness_state = torch.randn(batch_size, hidden_dim)
        integrated_output = torch.randn(batch_size, hidden_dim)
    
        # Test value estimation consistency
        _, metrics1 = state_manager(
            consciousness_state,
            integrated_output,
            deterministic=True
        )
        _, metrics2 = state_manager(
            consciousness_state,
            integrated_output,
            deterministic=True
        )
    
        # Same input should give same value estimate
>       assert torch.allclose(metrics1['state_value'], metrics2['state_value'], rtol=1e-5)
E       assert False
E        +  where False = <built-in method allclose of type object at 0x7222d16678c0>(tensor([[0.0171],\n        [0.0305]], grad_fn=<AddmmBackward0>), tensor([[-0.3608],\n        [-0.5848]], grad_fn=<AddmmBackward0>), rtol=1e-05)
E        +    where <built-in method allclose of type object at 0x7222d16678c0> = torch.allclose

tests/unit/state/test_consciousness_state_management.py:99: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5092],
        [0.4622]], grad_fn=<SigmoidBackward0>)
state: tensor([[-0.0622, -0.1918, -1.4378,  0.1874, -0.6205, -0.3639,  1.2609,  1.1951,
         -0.2701, -2.4360,  0.0676,  2.2319,  1.2764,  0.6376,  0.0557,  0.2343,
          0.7346, -1.7968, -0.0719, -0.4068,  1.3993, -0.9615,  0.1289,  1.1071,
         -0.3142, -0.4128, -0.1297, -1.4022, -1.1950,  0.3090,  0.6780, -0.2393,
         -0.4433,  1.1312,  1.9197, -1.9778, -0.0522, -0.6942,  0.4627, -0.9476,
         -0.0219, -0.8556,  1.4407,  1.5930,  0.1951, -1.0905,  1.9482, -1.7192,
         -0.5611,  1.5930,  2.0427,  0.0123, -2.3848, -0.9913, -0.6554,  1.1428,
         -1.5381, -0.1042,  0.9014, -0.1929,  0.7283, -1.3749,  1.2432,  1.3164],
        [-1.1129,  1.3855, -0.1778, -0.1881,  0.7154, -0.0341, -1.4047, -1.0151,
         -0.2137,  0.2278, -0.3946, -1.7084,  1.0237,  0.7343,  1.1405, -0.9368,
         -0.6248,  0.3401,  1.2726, -1.1430,  0.1650, -1.2155, -0.9270, -0.5667,
          1.1748,  0.3283, -0.6333,  1.8847, -0.7160,  2.2675, -0.4215,  0.3214,
         -0.2990, -0.7911, -1.1508, -0.5328, -1.3446,  0.2703, -1.3503, -1.2416,
         -0.4018, -1.6488,  0.0146, -0.1665,  0.4496, -0.0149, -0.7112, -0.5897,
          1.6479,  0.9926,  0.3018,  0.7335,  0.5780,  1.1611, -1.1160,  0.8001,
         -1.7052, -1.0633, -1.4637, -0.1903, -2.2774,  0.7382,  0.4390, -1.3554]])
inputs: tensor([[ 0.6325,  1.2744,  1.2077, -1.0776, -0.7776,  1.0811, -0.2200,  1.3204,
          1.0682,  0.4876, -1.1566,  0.4115,  1.5285, -1.4481, -0.5963,  0.6092,
         -0.2070,  1.0156,  0.3862,  1.0046, -0.7349,  1.6752,  0.9196,  0.2060,
          0.4940, -1.0995,  1.1456, -0.1798,  0.2958, -1.7030,  1.2476, -0.0590,
          0.6724, -0.1637,  0.4016, -0.0521,  1.0900, -0.4708, -0.6467, -0.2521,
         -1.4327, -0.3512, -0.4963, -0.5081, -1.1563, -0.6900,  0.5646,  0.0663,
         -0.5244, -1.1923,  1.6826, -1.4313, -0.8258,  0.1291,  0.8951, -0.1442,
         -1.3629, -0.3980, -0.7431,  0.9175, -1.4166, -0.4552, -3.3025, -0.7484],
        [-0.6114, -0.2233,  0.3410, -0.6278, -1.3974, -1.7729,  0.3853, -0.8249,
         -0.6573, -0.8730,  0.2682,  0.2989,  0.5404, -0.9191,  2.1910, -0.1198,
         -1.5554,  1.0710,  0.0313, -1.0893, -2.2512,  2.5964,  0.8952,  0.2096,
          0.5915,  1.1121,  0.2005, -1.0203, -2.1704,  0.1380, -0.3080, -0.1208,
         -0.5648, -0.9637, -1.3078, -0.8917, -0.6129,  2.2359,  2.2141, -0.1720,
         -0.2808,  0.7654,  0.9690,  0.7815,  0.5368,  0.0456, -0.1246, -1.0264,
          0.5466,  0.9938, -1.7124,  1.1559, -1.4905, -0.3494, -2.6426, -0.9578,
          0.1560,  1.2583,  0.6672, -0.3561, -0.9900, -0.1293, -0.5332,  0.2376]])
candidate_state: tensor([[ 0.5536, -0.1610, -0.1017, -0.1558, -0.0377,  0.2797,  0.1575, -0.1446,
          0.4272, -0.1558, -0.1159,  0.5223, -0.1410,  0.4885, -0.1200, -0.0213,
         -0.0437, -0.0255,  0.0473,  0.0204, -0.0339, -0.1637, -0.0755, -0.0964,
         -0.0136, -0.1699, -0.1511, -0.1268,  0.0883,  0.3582,  0.0877,  0.3054,
         -0.1696, -0.1606, -0.0760, -0.1525, -0.0698, -0.1645, -0.0507, -0.1120,
         -0.1700,  0.0275, -0.0550, -0.0573, -0.1586, -0.0165,  0.3656, -0.1696,
          0.0199,  0.0769,  0.5286, -0.1459, -0.0204, -0.1344, -0.1538, -0.0716,
         -0.0634, -0.0733,  0.2342,  1.2559, -0.0915, -0.1474, -0.1287, -0.1700],
        [ 0.2256,  0.1417,  0.9079,  0.7134,  0.3498, -0.0844,  0.6744,  0.6754,
          0.0485, -0.0465, -0.0262,  0.7553, -0.1157,  0.0518,  0.0789,  0.4343,
         -0.1679,  0.6214,  0.0788,  1.5643, -0.1518,  0.8435,  0.3916, -0.1056,
          0.2145,  0.0623,  1.1279,  0.0793,  0.3572, -0.1402, -0.0812, -0.1448,
         -0.1499, -0.1692,  0.0246,  0.4735, -0.1199,  0.8976,  0.0408, -0.0336,
         -0.0512,  1.3619,  0.2111,  0.1487, -0.1496,  0.5236,  0.1074,  0.1413,
          1.0041, -0.1669, -0.0563, -0.1623, -0.1355, -0.0703, -0.1096,  0.6455,
          0.0829, -0.1566, -0.0031,  1.4531,  0.4858,  0.3893,  0.2557, -0.1412]],
       grad_fn=<GeluBackward0>)
new_state: tensor([[ 0.2400, -0.1767, -0.7821,  0.0190, -0.3345, -0.0480,  0.7194,  0.5376,
          0.0721, -1.3170, -0.0224,  1.3929,  0.5808,  0.5644, -0.0305,  0.1089,
          0.3526, -0.9275, -0.0134, -0.1972,  0.6959, -0.5700,  0.0286,  0.5165,
         -0.1667, -0.2936, -0.1402, -0.7763, -0.5652,  0.3332,  0.3883,  0.0280,
         -0.3090,  0.4972,  0.9403, -1.0820, -0.0608, -0.4343,  0.2107, -0.5375,
         -0.0946, -0.4222,  0.7066,  0.7831,  0.0215, -0.5634,  1.1715, -0.9587,
         -0.2760,  0.8490,  1.2996, -0.0654, -1.2244, -0.5708, -0.4092,  0.5468,
         -0.8144, -0.0890,  0.5739,  0.5181,  0.3260, -0.7725,  0.5699,  0.5870],
        [-0.3930,  0.7165,  0.4061,  0.2968,  0.5187, -0.0611, -0.2864, -0.1059,
         -0.0727,  0.0803, -0.1964, -0.3833,  0.4109,  0.3672,  0.5695, -0.1993,
         -0.3791,  0.4914,  0.6305,  0.3131, -0.0054, -0.1081, -0.2178, -0.3187,
          0.6583,  0.1853,  0.3139,  0.9137, -0.1388,  0.9725, -0.2385,  0.0706,
         -0.2188, -0.4566, -0.5186,  0.0084, -0.6859,  0.6077, -0.6021, -0.5919,
         -0.2132, -0.0295,  0.1203,  0.0030,  0.1274,  0.2747, -0.2709, -0.1966,
          1.3016,  0.3690,  0.1092,  0.2517,  0.1943,  0.4988, -0.5747,  0.7169,
         -0.7435, -0.5756, -0.6782,  0.6936, -0.7912,  0.5505,  0.3404, -0.7023]],
       grad_fn=<AddBackward0>)
memory_gate: tensor([[0.5209],
        [0.5552]], grad_fn=<SigmoidBackward0>)
state: tensor([[-0.0622, -0.1918, -1.4378,  0.1874, -0.6205, -0.3639,  1.2609,  1.1951,
         -0.2701, -2.4360,  0.0676,  2.2319,  1.2764,  0.6376,  0.0557,  0.2343,
          0.7346, -1.7968, -0.0719, -0.4068,  1.3993, -0.9615,  0.1289,  1.1071,
         -0.3142, -0.4128, -0.1297, -1.4022, -1.1950,  0.3090,  0.6780, -0.2393,
         -0.4433,  1.1312,  1.9197, -1.9778, -0.0522, -0.6942,  0.4627, -0.9476,
         -0.0219, -0.8556,  1.4407,  1.5930,  0.1951, -1.0905,  1.9482, -1.7192,
         -0.5611,  1.5930,  2.0427,  0.0123, -2.3848, -0.9913, -0.6554,  1.1428,
         -1.5381, -0.1042,  0.9014, -0.1929,  0.7283, -1.3749,  1.2432,  1.3164],
        [-1.1129,  1.3855, -0.1778, -0.1881,  0.7154, -0.0341, -1.4047, -1.0151,
         -0.2137,  0.2278, -0.3946, -1.7084,  1.0237,  0.7343,  1.1405, -0.9368,
         -0.6248,  0.3401,  1.2726, -1.1430,  0.1650, -1.2155, -0.9270, -0.5667,
          1.1748,  0.3283, -0.6333,  1.8847, -0.7160,  2.2675, -0.4215,  0.3214,
         -0.2990, -0.7911, -1.1508, -0.5328, -1.3446,  0.2703, -1.3503, -1.2416,
         -0.4018, -1.6488,  0.0146, -0.1665,  0.4496, -0.0149, -0.7112, -0.5897,
          1.6479,  0.9926,  0.3018,  0.7335,  0.5780,  1.1611, -1.1160,  0.8001,
         -1.7052, -1.0633, -1.4637, -0.1903, -2.2774,  0.7382,  0.4390, -1.3554]])
inputs: tensor([[ 0.6325,  1.2744,  1.2077, -1.0776, -0.7776,  1.0811, -0.2200,  1.3204,
          1.0682,  0.4876, -1.1566,  0.4115,  1.5285, -1.4481, -0.5963,  0.6092,
         -0.2070,  1.0156,  0.3862,  1.0046, -0.7349,  1.6752,  0.9196,  0.2060,
          0.4940, -1.0995,  1.1456, -0.1798,  0.2958, -1.7030,  1.2476, -0.0590,
          0.6724, -0.1637,  0.4016, -0.0521,  1.0900, -0.4708, -0.6467, -0.2521,
         -1.4327, -0.3512, -0.4963, -0.5081, -1.1563, -0.6900,  0.5646,  0.0663,
         -0.5244, -1.1923,  1.6826, -1.4313, -0.8258,  0.1291,  0.8951, -0.1442,
         -1.3629, -0.3980, -0.7431,  0.9175, -1.4166, -0.4552, -3.3025, -0.7484],
        [-0.6114, -0.2233,  0.3410, -0.6278, -1.3974, -1.7729,  0.3853, -0.8249,
         -0.6573, -0.8730,  0.2682,  0.2989,  0.5404, -0.9191,  2.1910, -0.1198,
         -1.5554,  1.0710,  0.0313, -1.0893, -2.2512,  2.5964,  0.8952,  0.2096,
          0.5915,  1.1121,  0.2005, -1.0203, -2.1704,  0.1380, -0.3080, -0.1208,
         -0.5648, -0.9637, -1.3078, -0.8917, -0.6129,  2.2359,  2.2141, -0.1720,
         -0.2808,  0.7654,  0.9690,  0.7815,  0.5368,  0.0456, -0.1246, -1.0264,
          0.5466,  0.9938, -1.7124,  1.1559, -1.4905, -0.3494, -2.6426, -0.9578,
          0.1560,  1.2583,  0.6672, -0.3561, -0.9900, -0.1293, -0.5332,  0.2376]])
candidate_state: tensor([[ 4.9787e-02, -1.5810e-01,  6.0196e-02, -1.2915e-01, -1.2638e-01,
         -1.1963e-01,  3.5856e-01,  5.1250e-01, -1.6951e-01,  3.2157e-01,
         -1.0158e-01, -1.6072e-01, -1.5813e-01,  7.4784e-02, -7.6317e-02,
          1.3735e+00, -7.1767e-02,  1.1161e-01,  7.0472e-01, -7.4607e-02,
          3.0177e-01,  1.1505e+00,  2.8688e-01,  4.2191e-01,  5.4924e-02,
         -1.5967e-01,  9.2861e-02, -1.6073e-01,  5.0821e-01, -6.7800e-02,
          8.1303e-02,  7.6915e-02,  9.3789e-04, -1.6082e-01,  2.1658e-01,
          1.8479e-01,  9.4787e-01,  7.2598e-03, -1.6876e-01, -4.8434e-02,
          3.5881e-01, -2.1739e-02, -1.6967e-01, -4.9092e-02,  4.1576e-03,
          4.4188e-01,  1.7457e-01,  1.3544e-01,  1.0836e+00,  4.2707e-01,
         -5.5523e-02, -1.6829e-01,  2.8957e-01,  1.2549e-02, -1.5985e-02,
         -6.0851e-02,  1.4876e-01, -7.6324e-02,  1.3381e-02, -1.6833e-01,
         -1.3888e-01, -1.3498e-01,  3.1355e-01, -1.3838e-01],
        [ 1.1629e+00,  3.8127e-01, -1.6899e-01, -8.4506e-02,  4.9022e-01,
         -1.6823e-01, -9.7630e-02, -1.6028e-01,  8.0054e-01,  2.3000e-01,
         -1.1280e-01, -9.8785e-02,  1.5411e-02, -1.1767e-01,  2.6165e-01,
         -1.3602e-01, -4.7642e-02,  3.5163e-01,  5.6783e-02, -1.6156e-01,
         -1.0651e-01, -1.0026e-01,  2.2199e-01,  2.9057e-01, -2.6007e-02,
          5.2507e-01,  2.0621e-01,  2.4474e-01, -1.5884e-01,  9.2828e-02,
         -8.6335e-02,  1.0127e-01, -5.1030e-02, -1.6105e-01, -1.1356e-01,
         -1.4829e-01,  4.1352e-01, -3.9941e-02,  4.5148e-01,  2.2313e-01,
         -8.1732e-02, -1.6384e-01, -7.4690e-02, -5.5488e-02,  2.6872e-01,
         -1.6972e-01, -1.2908e-01, -1.3110e-01,  5.9185e-01, -5.2258e-02,
         -1.4335e-02, -7.2857e-02,  4.0357e-01,  5.8504e-02,  3.3478e-01,
          3.6341e-02, -1.6850e-01, -9.8657e-02, -1.1073e-01, -1.5799e-02,
         -1.6891e-01, -1.5851e-01, -1.6922e-01, -1.4275e-01]],
       grad_fn=<GeluBackward0>)
new_state: tensor([[-0.0086, -0.1756, -0.7201,  0.0357, -0.3838, -0.2469,  0.8286,  0.8680,
         -0.2219, -1.1148, -0.0135,  1.0856,  0.5891,  0.3679, -0.0076,  0.7801,
          0.3483, -0.8825,  0.3002, -0.2477,  0.8735,  0.0503,  0.2046,  0.7788,
         -0.1373, -0.2915, -0.0231, -0.8074, -0.3790,  0.1285,  0.3921, -0.0878,
         -0.2304,  0.5122,  1.1037, -0.9417,  0.4269, -0.3582,  0.1602, -0.5168,
          0.1605, -0.4561,  0.6692,  0.8063,  0.1036, -0.3563,  1.0984, -0.8306,
          0.2269,  1.0344,  1.0374, -0.0742, -1.1035, -0.5104, -0.3491,  0.5661,
         -0.7299, -0.0908,  0.4759, -0.1811,  0.3128, -0.7809,  0.7978,  0.6194],
        [-0.1008,  0.9389, -0.1739, -0.1420,  0.6152, -0.0937, -0.8234, -0.6349,
          0.2374,  0.2288, -0.2692, -0.9925,  0.5752,  0.3554,  0.7497, -0.5806,
         -0.3681,  0.3452,  0.7319, -0.7065,  0.0442, -0.7195, -0.4160, -0.1854,
          0.6407,  0.4158, -0.2600,  1.1553, -0.4682,  1.3003, -0.2724,  0.2235,
         -0.1887, -0.5109, -0.6895, -0.3618, -0.5627,  0.1323, -0.5489, -0.5901,
         -0.2594, -0.9883, -0.0251, -0.1171,  0.3692, -0.0838, -0.4523, -0.3858,
          1.1782,  0.5279,  0.1612,  0.3749,  0.5004,  0.6707, -0.4708,  0.4604,
         -1.0217, -0.6343, -0.8620, -0.1127, -1.3396,  0.3394,  0.1685, -0.8161]],
       grad_fn=<AddBackward0>)
=============================== warnings summary ===============================
../../.local/lib/python3.10/site-packages/torch/__init__.py:1144
  /home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
    _C._set_default_tensor_type(t)

tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_consistency
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_state_updates
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_rl_optimization
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_energy_efficiency
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_state_value_estimation
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_adaptive_gating
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    state = torch.tensor(state, dtype=torch.float32)

tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_consistency
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_state_updates
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_rl_optimization
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_energy_efficiency
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_state_value_estimation
tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_adaptive_gating
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    inputs = torch.tensor(inputs, dtype=torch.float32)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_environment.py::EnvironmentTests::test_core_imports - Runti...
FAILED tests/test_environment.py::EnvironmentTests::test_framework_versions
FAILED tests/test_environment.py::EnvironmentTests::test_hardware_detection
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_scaled_dot_product
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_attention_mask
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_consciousness_broadcasting
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_global_workspace_integration
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_phi_metric_computation
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_information_flow
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_entropy_calculations
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_memory_integration
FAILED tests/unit/state/test_consciousness_state_management.py::TestStateManagement::test_state_value_estimation
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_initialization
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_config
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_state_initialization
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
ERROR tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cross_modal_attention
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_modality_specific_processing
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_integration_stability
ERROR tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cognitive_integration
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_gru_state_updates
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_sequence_processing
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_context_aware_gating
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_information_integration
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_retention
ERROR tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_state_updates
ERROR tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_reset_gate
ERROR tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_sequence_processing
ERROR tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_memory_retention
============ 14 failed, 12 passed, 19 warnings, 19 errors in 0.71s =============
