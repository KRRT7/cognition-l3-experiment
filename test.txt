============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.4, pluggy-1.5.0
rootdir: /home/kasinadhsarma/experiment/cognition-l3-experiment
plugins: cov-6.0.0, anyio-4.7.0
collected 47 items

tests/test_consciousness.py .F..FF                                       [ 12%]
tests/test_environment.py FFF..                                          [ 23%]
tests/unit/attention/test_attention.py FFFF                              [ 31%]
tests/unit/attention/test_attention_mechanisms.py ....                   [ 40%]
tests/unit/integration/test_cognitive_integration.py FFFF                [ 48%]
tests/unit/integration/test_state_management.py FFF.                     [ 57%]
tests/unit/memory/test_integration.py FFFF                               [ 65%]
tests/unit/memory/test_memory.py .FF.FF.                                 [ 80%]
tests/unit/memory/test_memory_components.py FFFF                         [ 89%]
tests/unit/state/test_consciousness_state_management.py FF..F            [100%]

=================================== FAILURES ===================================
________________ TestConsciousnessModel.test_model_forward_pass ________________

self = <test_consciousness.TestConsciousnessModel object at 0x74d4e1eda350>
model = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise...MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
)
sample_input = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def test_model_forward_pass(self, model, sample_input, deterministic):
        """Test forward pass through consciousness model."""
        # Initialize model
        input_shape = (model.hidden_dim,)
        model.eval() if deterministic else model.train()
    
        # Run forward pass
        with torch.no_grad() if deterministic else torch.enable_grad():
>           new_state, metrics = model(sample_input, deterministic=deterministic)

tests/test_consciousness.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/consciousness_model.py:118: in forward
    consciousness_state, attention_maps = self.cognitive_integration(
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...
  )
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
________________ TestConsciousnessModel.test_model_state_update ________________

self = <test_consciousness.TestConsciousnessModel object at 0x74d4e1edaa10>
model = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise...MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
)
sample_input = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def test_model_state_update(self, model, sample_input, deterministic):
        """Test updating the model state."""
        input_shape = (model.hidden_dim,)
        model.eval() if deterministic else model.train()
        with torch.no_grad() if deterministic else torch.enable_grad():
            state = torch.zeros(sample_input['attention'].shape[0], model.hidden_dim)
>           new_state, metrics = model(sample_input, state=state, deterministic=deterministic)

tests/test_consciousness.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/consciousness_model.py:118: in forward
    consciousness_state, attention_maps = self.cognitive_integration(
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...
  )
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
_____________ TestConsciousnessModel.test_model_attention_weights ______________

self = <test_consciousness.TestConsciousnessModel object at 0x74d4e1edac80>
model = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise...MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
)
sample_input = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def test_model_attention_weights(self, model, sample_input, deterministic):
        """Test attention weights in the model."""
        input_shape = (model.hidden_dim,)
        model.eval() if deterministic else model.train()
        with torch.no_grad() if deterministic else torch.enable_grad():
            state = torch.zeros(sample_input['attention'].shape[0], model.hidden_dim)
>           _, metrics = model(sample_input, state=state, deterministic=deterministic)

tests/test_consciousness.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/consciousness_model.py:118: in forward
    consciousness_state, attention_maps = self.cognitive_integration(
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...
  )
  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
______________________ EnvironmentTests.test_core_imports ______________________

self = <test_environment.EnvironmentTests testMethod=test_core_imports>

    def test_core_imports(self):
        """Test all core framework imports"""
        try:
            import torch
>           import torchvision

tests/test_environment.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164: in <module>
    def meta_nms(dets, scores, iou_threshold):
../../.local/lib/python3.10/site-packages/torch/library.py:795: in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
../../.local/lib/python3.10/site-packages/torch/library.py:184: in _register_fake
    handle = entry.fake_impl.register(func_to_register, source)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch._library.fake_impl.FakeImplHolder object at 0x74d4e1ba2a10>
func = <function meta_nms at 0x74d4e1ca8dc0>
source = '/home/kasinadhsarma/.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164'

    def register(self, func: Callable, source: str) -> RegistrationHandle:
        """Register an fake impl.
    
        Returns a RegistrationHandle that one can use to de-register this
        fake impl.
        """
        if self.kernel is not None:
            raise RuntimeError(
                f"register_fake(...): the operator {self.qualname} "
                f"already has an fake impl registered at "
                f"{self.kernel.source}."
            )
>       if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
E       RuntimeError: operator torchvision::nms does not exist

../../.local/lib/python3.10/site-packages/torch/_library/fake_impl.py:31: RuntimeError

During handling of the above exception, another exception occurred:

self = <test_environment.EnvironmentTests testMethod=test_core_imports>

    def test_core_imports(self):
        """Test all core framework imports"""
        try:
            import torch
            import torchvision
            import torchaudio
            _ = torch.__version__
            _ = torchvision.__version__
            _ = torchaudio.__version__
            self.assertTrue(True, "All core imports successful")
        except ImportError as e:
            self.fail(f"Failed to import core frameworks: {str(e)}")
        except RuntimeError as e:
>           self.fail(f"Runtime error during import: {str(e)}")
E           AssertionError: Runtime error during import: operator torchvision::nms does not exist

tests/test_environment.py:28: AssertionError
___________________ EnvironmentTests.test_framework_versions ___________________

self = <test_environment.EnvironmentTests testMethod=test_framework_versions>

    def test_framework_versions(self):
        """Verify framework versions"""
        import torch
>       import torchvision

tests/test_environment.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26: in <module>
    def meta_roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function meta_roi_align at 0x74d4e1caaf80>

    def wrapper(fn):
>       if torchvision.extension._has_ops():
E       AttributeError: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)

../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18: AttributeError
___________________ EnvironmentTests.test_hardware_detection ___________________

self = <test_environment.EnvironmentTests testMethod=test_hardware_detection>

    def test_hardware_detection(self):
        """Test hardware detection and configuration"""
        import torch
    
        # Check if CUDA is available
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            print(f"CUDA devices: {torch.cuda.device_count()} available")
        else:
            print("CUDA is not available")
>       self.assertTrue(cuda_available, "CUDA is not available")
E       AssertionError: False is not true : CUDA is not available

tests/test_environment.py:40: AssertionError
----------------------------- Captured stdout call -----------------------------
CUDA is not available
_______________ TestAttentionMechanisms.test_scaled_dot_product ________________

self = <test_attention.TestAttentionMechanisms object at 0x74d4e1efc700>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_scaled_dot_product(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test scaled dot-product attention computation."""
        # Create inputs
>       inputs_q = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim)

tests/unit/attention/test_attention.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_attention.TestAttentionMechanisms object at 0x74d4e1efc700>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
_________________ TestAttentionMechanisms.test_attention_mask __________________

self = <test_attention.TestAttentionMechanisms object at 0x74d4e1efc910>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_attention_mask(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test attention mask handling."""
        # Create inputs and mask
>       inputs_q = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim)

tests/unit/attention/test_attention.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_attention.TestAttentionMechanisms object at 0x74d4e1efc910>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
___________ TestAttentionMechanisms.test_consciousness_broadcasting ____________

self = <test_attention.TestAttentionMechanisms object at 0x74d4e1efcb20>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_consciousness_broadcasting(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test consciousness-aware broadcasting."""
>       inputs_q = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim)

tests/unit/attention/test_attention.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_attention.TestAttentionMechanisms object at 0x74d4e1efcb20>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
__________ TestAttentionMechanisms.test_global_workspace_integration ___________

self = <test_attention.TestAttentionMechanisms object at 0x74d4e1efcd00>
batch_size = 2, seq_length = 8, hidden_dim = 128, num_heads = 4

    def test_global_workspace_integration(self, batch_size, seq_length, hidden_dim, num_heads):
        """Test global workspace integration."""
        workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=0.1
        )
    
>       inputs = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim)

tests/unit/attention/test_attention.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_attention.TestAttentionMechanisms object at 0x74d4e1efcd00>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
__________ TestCognitiveProcessIntegration.test_cross_modal_attention __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x74d4e1efe860>
device = device(type='cpu')
integration_module = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_cross_modal_attention(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Create multi-modal inputs
        inputs = {
            'visual': torch.randn(batch_size, seq_length, input_dim, device=device),
            'textual': torch.randn(batch_size, seq_length, input_dim, device=device),
            'numerical': torch.randn(batch_size, seq_length, input_dim, device=device)
        }
    
        # Initialize parameters
        input_shape = (64,)
        integration_module.eval()
        with torch.no_grad():
>           consciousness_state, attention_maps = integration_module(inputs, deterministic=True)

tests/unit/integration/test_cognitive_integration.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'numerical': tensor([[[-4.8143e-01, -3.6127e-01,  5.0406e-01,  2.2154e+00,  1.8368e+00,
           6.1404e-01,  7.033...+00,
           3.8571e-01,  9.0806e-01,  1.0072e+00,  2.0605e-01, -4.5488e-01,
          -8.7693e-01,  8.4897e-02]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
______ TestCognitiveProcessIntegration.test_modality_specific_processing _______

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x74d4e1efdc90>
device = device(type='cpu')
integration_module = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_modality_specific_processing(self, device, integration_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Test with single modality
        single_input = {
            'visual': torch.randn(batch_size, seq_length, input_dim, device=device)
        }
        input_shape = (64,)
        integration_module.eval()
        with torch.no_grad():
>           consciousness_state1, _ = integration_module(single_input, deterministic=True)

tests/unit/integration/test_cognitive_integration.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'visual': tensor([[[ 5.7287e-01, -3.7417e-01,  1.5273e+00,  6.9645e-02,  1.1185e+00,
           5.1637e-01,  3.4546e-...-01,
           6.3792e-01, -1.2331e+00, -4.0110e-01,  3.9479e-01, -9.2258e-01,
           9.3135e-02,  4.9786e-01]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
__________ TestCognitiveProcessIntegration.test_integration_stability __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x74d4e1efd660>
device = device(type='cpu')
integration_module = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_integration_stability(self, device, integration_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        inputs = {
            'modality1': torch.randn(batch_size, seq_length, input_dim, device=device),
            'modality2': torch.randn(batch_size, seq_length, input_dim, device=device)
        }
    
        integration_module.eval()
        states = []
        with torch.no_grad():
            for _ in range(5):
>               state, _ = integration_module(inputs, deterministic=True)

tests/unit/integration/test_cognitive_integration.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'modality1': tensor([[[ 7.8656e-01,  4.3046e-01,  1.3711e-01, -1.3496e-01,  5.7280e-03,
           8.0930e-01, -5.396...        1.0359,  1.2281, -1.1428,  2.0762, -1.8058, -0.7151, -1.1927,
          -0.4456,  0.0848, -0.4560,  0.4052]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
__________ TestCognitiveProcessIntegration.test_cognitive_integration __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x74d4e1efc7f0>
device = device(type='cpu')
integration_module = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_cognitive_integration(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Create multi-modal inputs
        inputs = {
            'visual': torch.randn(batch_size, seq_length, input_dim, device=device),
            'textual': torch.randn(batch_size, seq_length, input_dim, device=device),
            'numerical': torch.randn(batch_size, seq_length, input_dim, device=device)
        }
    
        # Initialize parameters
        input_shape = (64,)
        integration_module.eval()
        with torch.no_grad():
>           consciousness_state, attention_maps = integration_module(inputs, deterministic=True)

tests/unit/integration/test_cognitive_integration.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'numerical': tensor([[[-2.0359e-01, -6.8227e-01,  1.0594e-01,  2.0380e-01,  3.2327e-01,
           9.9448e-01, -1.156...-01,
          -8.3444e-01,  1.1105e+00,  1.0162e+00,  1.4355e+00,  1.1088e+00,
           1.6581e-01,  9.9820e-01]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
_______________ TestConsciousnessStateManager.test_state_updates _______________

self = <test_state_management.TestConsciousnessStateManager object at 0x74d4e1eff280>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_state_updates(self, device, state_manager):
        # Test dimensions
        batch_size = 2
        hidden_dim = 64
    
        # Create sample state and inputs
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        # Initialize parameters
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test output shapes
        assert new_state.shape == state.shape
        assert 'memory_gate' in metrics
        assert 'energy_cost' in metrics
        assert 'state_value' in metrics
    
        # Test memory gate properties
>       assert metrics['memory_gate'].shape == (batch_size, hidden_dim)  # Updated shape
E       assert torch.Size([2, 1]) == (2, 64)
E         
E         At index 1 diff: 1 != 64
E         Use -v to get more diff

tests/unit/integration/test_state_management.py:44: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4657],
        [0.5080]])
state: tensor([[-1.3830, -1.4503, -1.8523,  1.5543,  0.7169, -0.5660,  1.2047,  0.6064,
          2.4917, -0.2379, -0.5402, -1.2173, -0.8302, -0.0149, -0.6758, -0.6141,
         -0.1487,  1.9537, -0.2105, -1.4271, -0.6158, -1.3398, -0.0069, -0.1715,
         -0.5670, -0.4308, -0.4233, -2.3178, -0.1384, -1.1886,  0.1009,  0.0200,
          0.7741,  0.8302,  0.1800,  0.0131, -1.1728, -0.7405, -0.5688, -1.7329,
          1.7581, -0.9426,  1.4846,  1.8746,  0.8086,  1.0405, -0.6775,  0.9629,
          0.2739,  0.6613,  0.3005, -0.5702,  0.1045, -0.6495, -0.5848,  0.7814,
         -0.0124,  1.2672,  0.3202, -0.4994, -0.2387, -0.6891,  0.1425, -0.9904],
        [ 1.0055, -0.3669, -0.1946, -0.4449,  0.8112, -0.0239, -0.4611,  0.6905,
         -2.1334, -0.8039,  2.5826,  0.4373, -0.6853,  1.8735, -0.4983, -0.7967,
          0.3671, -0.3611, -0.4447,  0.6582,  0.1812,  1.2462,  1.0336, -0.9022,
         -0.9556, -0.5635, -1.3092, -0.3412, -1.1429, -0.6851,  0.5860,  1.6440,
          0.4365,  0.4282, -0.5223, -1.0323,  1.1668,  1.5545,  1.3651, -0.2119,
          1.0643, -0.4303, -0.3554, -0.0811, -1.8799, -0.2709,  0.9479, -0.0344,
          0.8825, -0.7963, -0.1274,  1.7884,  1.4648,  2.2908,  0.7431,  0.3915,
         -0.5675, -1.0030, -0.0106, -0.6673,  0.5592,  1.2419,  1.8643,  1.1935]])
inputs: tensor([[ 0.5735, -0.1483, -0.2319,  1.1960, -0.4282,  0.1324, -0.6036, -0.0424,
         -0.6245,  1.1936,  1.0151,  0.1933, -2.2407,  1.3686,  0.2470, -0.5357,
          1.2343,  0.6840,  0.6605, -0.5586, -0.2869, -1.0841, -0.1762,  1.8438,
         -0.3306, -0.1151,  1.0079,  0.2821,  0.3033,  0.3987,  0.2760, -0.0568,
          1.6998, -0.0737,  0.6899,  0.5232, -0.8354, -0.8408, -3.0869,  1.3713,
          2.1598, -1.2853, -1.0123,  0.8712,  1.4654,  0.8098, -0.3557,  1.4343,
          0.9254, -0.3240, -0.2193,  0.0078,  1.0323,  0.6698, -1.1607, -0.5325,
         -0.1298, -2.5016, -1.4439, -0.0626, -1.1790, -0.9620, -1.0800, -0.2393],
        [-1.0747, -0.5275, -0.3907,  0.1954, -0.5732,  0.0993,  1.6708, -0.6900,
          0.1998, -1.4074, -0.4741, -1.4273, -0.6085, -0.4433,  0.7978, -0.3686,
          0.0569,  1.8666,  0.7107, -0.9635, -0.0757, -1.8338, -0.2824, -2.1353,
          0.3958, -0.9002, -0.0528, -0.3297, -0.2407, -0.9539, -0.2353, -0.4375,
         -1.0652,  2.1980,  0.4424,  2.1575, -1.0424,  0.8011, -1.0083,  0.5962,
          0.6507, -0.9030, -0.2622, -1.7842,  0.0363, -0.7881, -0.1889, -1.2449,
         -1.2264, -0.3370,  0.8512,  1.4472, -0.3314, -0.2259,  0.7974,  1.4978,
          0.5270, -1.7554, -0.8008,  0.2534,  2.6883, -0.8134,  2.0702, -0.1130]])
candidate_state: tensor([[-1.0706e-01,  5.3298e-02,  4.0605e-02,  3.0775e-01, -1.9088e-02,
         -1.6048e-01,  3.5643e-01,  9.3934e-01,  5.6449e-01, -3.5029e-02,
          3.1427e-01, -3.5541e-02,  5.7630e-01, -4.6935e-02, -9.3311e-02,
          6.7344e-01, -1.6950e-01, -2.5738e-03,  5.5097e-02, -1.5302e-01,
         -1.4457e-01, -1.3134e-01,  1.1719e+00,  4.3050e-01,  2.3022e-01,
         -2.8813e-02, -1.6807e-01, -1.3676e-01,  2.2121e-01,  2.1269e-01,
          6.3970e-01,  2.0172e-01, -1.3260e-01,  3.9148e-01,  1.9678e-02,
         -1.5893e-01,  7.2205e-01,  3.5182e-02,  5.7917e-01, -2.2574e-02,
         -1.3620e-02,  9.6195e-01, -1.2800e-01,  1.0408e-01,  1.2561e-01,
          5.0379e-02, -1.2508e-01, -1.1096e-01, -7.8608e-02, -1.6926e-01,
         -1.5911e-01, -1.5676e-01,  5.6579e-01, -1.6643e-01, -1.6019e-01,
          1.0594e-01, -3.7902e-02,  3.1556e-04,  7.6959e-01, -1.6542e-02,
         -6.4955e-02, -1.6428e-01,  8.2631e-01, -1.5471e-01],
        [-1.6800e-01, -4.5470e-02, -1.4197e-01,  3.3336e-01, -7.5096e-02,
          1.1963e+00,  1.7605e-01, -1.6652e-01, -6.2093e-02, -2.8170e-02,
          2.1297e-01,  1.4797e-01, -1.4765e-01, -8.0226e-02,  7.3747e-02,
          9.3227e-02,  8.8209e-02, -1.4256e-01,  1.2638e-01,  1.9190e-01,
         -9.7905e-02, -1.2226e-01,  9.1613e-01, -1.2820e-01, -1.5167e-01,
         -8.8945e-02, -1.2798e-01, -1.5483e-01,  4.2404e-01, -5.1631e-02,
          3.5767e-01, -1.6474e-01,  3.3847e-01, -1.6606e-01, -1.6670e-01,
         -7.4496e-02,  2.9380e-02, -8.6482e-02,  3.8251e-01, -1.5861e-01,
          5.5262e-01,  5.9915e-02,  2.3210e-01,  2.2554e-01,  1.0364e-02,
         -9.1359e-02, -8.4434e-02,  5.8762e-01, -2.2107e-02, -7.6245e-02,
          8.8358e-01,  4.2512e-01, -6.6585e-02, -1.2565e-01, -1.6554e-01,
         -1.4960e-01,  5.6122e-02,  3.2942e-01,  8.0127e-02, -1.6808e-01,
         -1.0752e-01, -1.3765e-02,  2.0825e-01, -1.9032e-02]])
new_state: tensor([[-0.7012, -0.6469, -0.8409,  0.8882,  0.3236, -0.3493,  0.7515,  0.7843,
          1.4619, -0.1295, -0.0836, -0.5859, -0.0787, -0.0320, -0.3646,  0.0738,
         -0.1598,  0.9084, -0.0686, -0.7463, -0.3640, -0.6941,  0.6230,  0.1502,
         -0.1410, -0.2160, -0.2869, -1.1524,  0.0538, -0.4399,  0.3888,  0.1171,
          0.2896,  0.5958,  0.0943, -0.0788, -0.1603, -0.3260,  0.0446, -0.8190,
          0.8114,  0.0750,  0.6229,  0.9286,  0.4437,  0.5115, -0.3823,  0.3891,
          0.0856,  0.2175,  0.0549, -0.3493,  0.3510, -0.3914, -0.3579,  0.4205,
         -0.0260,  0.5903,  0.5603, -0.2414, -0.1459, -0.4087,  0.5079, -0.5439],
        [ 0.4282, -0.2087, -0.1687, -0.0620,  0.3751,  0.5764, -0.1476,  0.2689,
         -1.1144, -0.4222,  1.4168,  0.2950, -0.4208,  0.9123, -0.2168, -0.3588,
          0.2299, -0.2536, -0.1637,  0.4288,  0.0439,  0.5729,  0.9758, -0.5214,
         -0.5601, -0.3300, -0.7281, -0.2495, -0.3720, -0.3735,  0.4737,  0.7541,
          0.3883,  0.1358, -0.3473, -0.5611,  0.6072,  0.7472,  0.8817, -0.1857,
          0.8126, -0.1891, -0.0663,  0.0697, -0.9499, -0.1826,  0.4400,  0.2716,
          0.4374, -0.4420,  0.3700,  1.1177,  0.7114,  1.1020,  0.2960,  0.1253,
         -0.2607, -0.3475,  0.0340, -0.4217,  0.2312,  0.6241,  1.0495,  0.5970]])
______________ TestConsciousnessStateManager.test_rl_optimization ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x74d4e1eff430>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_rl_optimization(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test RL loss computation
        reward = torch.ones(batch_size, 1, device=device)  # Mock reward
        value_loss, td_error = state_manager.get_rl_loss(
            state_value=metrics['state_value'],
            reward=reward,
            next_state_value=metrics['state_value']
        )
    
        # Test loss properties
        assert torch.is_tensor(value_loss)
        assert value_loss.item() >= 0.0
>       assert td_error.shape == (batch_size, 1)  # changed to match actual output
E       assert torch.Size([2, 2, 1]) == (2, 1)
E         
E         At index 1 diff: 2 != 1
E         Left contains one more item: 1
E         Use -v to get more diff

tests/unit/integration/test_state_management.py:77: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5357],
        [0.4613]])
state: tensor([[ 3.6911e-01, -1.8412e-03,  1.4699e-01, -1.7657e-01,  1.2646e+00,
         -3.7014e-02, -1.0295e+00, -1.5618e-01, -7.3206e-01,  2.9104e-01,
          7.2113e-01, -1.5336e-01, -9.6577e-03, -1.1756e-01,  3.9622e-01,
         -1.3399e+00,  1.9677e+00,  4.4244e-01, -8.0135e-01,  1.7419e-01,
          8.0537e-01, -2.2278e-01,  7.7474e-01, -1.9460e-01, -8.6974e-01,
         -3.5643e-02,  1.0047e+00,  1.0106e+00,  9.3074e-01,  6.0790e-01,
          7.8991e-01, -1.1764e+00, -8.8421e-01, -8.2159e-01,  5.8896e-01,
         -3.7130e-01,  7.0522e-02, -1.0719e+00, -1.2164e+00,  9.6203e-01,
          6.3349e-01, -6.0285e-01, -1.7631e-01,  9.2233e-01,  5.1799e-01,
          1.5612e-01, -5.4758e-02,  8.6722e-01, -3.9065e-01,  6.3418e-01,
         -1.7975e+00,  3.0095e-01,  1.4295e-01,  5.7880e-01, -1.6747e-01,
          2.0141e+00,  1.5079e-01, -1.6951e+00, -1.0783e+00,  3.7336e-01,
          3.7040e-01, -8.7432e-02,  8.2192e-01, -1.7552e+00],
        [ 4.8451e-02,  1.3113e+00,  1.2562e+00, -2.4380e-02,  6.8010e-02,
          4.1542e-01, -2.8631e-01, -1.8241e-01,  1.0661e+00, -1.4677e+00,
          5.3350e-01,  2.8839e-01, -1.5183e+00, -9.9473e-02,  1.3078e+00,
          4.9345e-01, -7.9035e-01, -9.8397e-01, -1.3644e-01, -9.0006e-01,
         -4.1993e-01, -5.2016e-01, -3.8504e-01, -1.5320e+00,  4.9437e-01,
          5.9220e-01, -4.1462e-01, -2.4861e-01,  1.2185e+00, -1.5099e+00,
          2.3433e-02, -1.1270e+00,  1.9322e+00,  4.8484e-01,  1.4557e-01,
         -7.3704e-01, -1.8227e-01, -6.2906e-01, -1.1212e+00,  6.2512e-01,
         -1.2712e-01,  4.4678e-01, -5.8975e-01, -7.5787e-02,  6.1199e-01,
         -1.0575e+00, -1.1743e+00,  9.9564e-01,  7.9370e-01,  5.2235e-02,
          4.3481e-01,  3.8716e-01, -1.9810e+00, -1.2002e+00,  1.5487e+00,
          1.2570e-01, -9.3917e-01,  4.5987e-01,  5.4651e-01,  1.0698e+00,
          1.7452e+00,  1.2179e+00,  6.3043e-02,  6.1842e-01]])
inputs: tensor([[-0.2060, -0.4903,  0.5724, -1.2396,  0.7279, -2.0026, -0.2494,  0.4112,
         -2.2560, -0.2143, -2.0242, -0.4152, -1.0785,  1.2142,  0.0241,  0.2971,
         -0.6318, -1.5536,  0.1989,  1.5715, -0.6290, -1.0957,  0.4006,  0.0246,
         -1.3953,  0.2746,  0.0398, -0.0134,  1.3806, -0.3006,  0.1140,  0.9557,
          2.1349,  0.8713,  1.3440, -0.8591, -0.8528, -0.4494,  0.8941,  0.8698,
          0.3727, -1.5515, -1.4584, -0.5517, -1.5602, -1.3332, -0.2026, -0.5787,
         -0.5219, -1.7372, -0.0728, -0.5127, -0.4169, -0.9109,  0.0543,  0.2248,
          0.8943, -0.1340,  1.0695,  1.4636, -0.1108, -0.6941, -0.3510,  0.6936],
        [-0.1238, -1.9233,  1.4517, -1.0276, -1.5326,  1.3998,  0.4279,  0.3589,
          0.4269,  0.1325,  0.8956,  1.2430, -1.7281, -0.3107, -0.0561, -0.8095,
          0.1669,  0.6086,  2.0730,  2.2649,  2.2754, -0.5778, -1.3718,  0.3889,
          1.2318, -1.4921,  0.7571,  0.8565, -0.9389, -1.9040, -1.1099,  0.2932,
         -1.4101, -1.6628,  1.1714, -0.6268,  0.6320, -0.2776,  2.2390, -1.3801,
          0.1991,  0.8598, -0.3836,  0.4461, -0.0673,  0.3683,  0.1595, -0.3828,
         -0.3933, -0.4357, -0.9459, -0.0647,  2.0562, -0.6657,  0.4050,  0.8658,
         -0.6201,  0.0813, -1.2535,  0.0316, -1.4369, -0.9236,  0.2465, -0.5628]])
candidate_state: tensor([[ 0.9670, -0.1303,  0.0659, -0.1042, -0.1614,  0.0791, -0.1453,  0.0102,
         -0.1692, -0.0280,  0.0541, -0.1588, -0.1597,  0.2567, -0.0889,  0.4080,
         -0.0302, -0.0682,  0.0262, -0.1696,  0.1564, -0.1653,  0.4396,  0.2794,
         -0.0038,  0.9700,  0.0871, -0.1633,  0.1437,  0.0152, -0.1022, -0.1699,
          0.0467, -0.0650, -0.0979, -0.1093,  0.1487,  1.0216, -0.0191,  0.5354,
         -0.0729, -0.1277,  0.2034,  0.0404,  0.8234,  0.8098, -0.1538, -0.1680,
         -0.1450, -0.1135,  0.0813, -0.1592, -0.1577, -0.1548,  0.0135,  0.0098,
         -0.0929, -0.1694,  0.9560, -0.0535,  0.4352, -0.1038, -0.1584, -0.1697],
        [ 0.3970,  1.2602, -0.1214,  0.1959,  0.1826, -0.1073, -0.0697, -0.1698,
          0.9607,  0.6368,  0.0760, -0.1244,  0.1543, -0.0880, -0.1296, -0.0310,
         -0.1684,  0.6546, -0.0910,  0.0898, -0.1342, -0.1665, -0.1672, -0.0628,
         -0.1404,  0.4228, -0.0648,  0.7308,  1.3705,  0.1265,  0.6072, -0.1177,
         -0.1698, -0.1117, -0.0281, -0.1654,  0.0935,  0.0288, -0.1638,  0.6862,
          0.4368, -0.0658, -0.1681,  0.2961,  1.1920,  1.5827,  0.0327,  0.1469,
         -0.0888, -0.0492, -0.1338, -0.0771, -0.0745,  0.3443, -0.1459, -0.0202,
         -0.1361,  0.1076,  0.1426,  0.0654, -0.1646, -0.0165,  0.1554,  1.1255]])
new_state: tensor([[ 6.4672e-01, -6.1493e-02,  1.0933e-01, -1.4298e-01,  6.0252e-01,
          1.6882e-02, -6.1896e-01, -7.8912e-02, -4.7072e-01,  1.4292e-01,
          4.1142e-01, -1.5587e-01, -7.9328e-02,  5.6200e-02,  1.7098e-01,
         -5.2838e-01,  1.0401e+00,  2.0535e-01, -4.1712e-01,  1.4556e-02,
          5.0406e-01, -1.9608e-01,  6.1913e-01,  2.5473e-02, -4.6769e-01,
          4.3128e-01,  5.7867e-01,  4.6556e-01,  5.6534e-01,  3.3270e-01,
          3.7572e-01, -7.0913e-01, -4.5202e-01, -4.7030e-01,  2.7007e-01,
         -2.4967e-01,  1.0683e-01, -9.9958e-02, -6.6051e-01,  7.6395e-01,
          3.0550e-01, -3.8223e-01, -2.5742e-05,  5.1288e-01,  6.5979e-01,
          4.5961e-01, -1.0074e-01,  3.8660e-01, -2.7659e-01,  2.8703e-01,
         -9.2521e-01,  8.7305e-02,  3.3524e-03,  2.3820e-01, -8.3453e-02,
          1.0835e+00,  3.7631e-02, -9.8671e-01, -1.3383e-01,  1.7517e-01,
          4.0049e-01, -9.5021e-02,  3.6677e-01, -1.0191e+00],
        [ 2.3618e-01,  1.2838e+00,  5.1412e-01,  9.4269e-02,  1.2974e-01,
          1.3382e-01, -1.6964e-01, -1.7563e-01,  1.0094e+00, -3.3404e-01,
          2.8704e-01,  6.6018e-02, -6.1732e-01, -9.3274e-02,  5.3348e-01,
          2.1095e-01, -4.5531e-01, -1.0128e-01, -1.1195e-01, -3.6686e-01,
         -2.6603e-01, -3.2967e-01, -2.6769e-01, -7.4055e-01,  1.5245e-01,
          5.0097e-01, -2.2618e-01,  2.7900e-01,  1.3004e+00, -6.2842e-01,
          3.3789e-01, -5.8329e-01,  7.9990e-01,  1.6351e-01,  5.1993e-02,
         -4.2909e-01, -3.3731e-02, -2.7467e-01, -6.0546e-01,  6.5802e-01,
          1.7666e-01,  1.7065e-01, -3.6259e-01,  1.2453e-01,  9.2444e-01,
          3.6472e-01, -5.2412e-01,  5.3846e-01,  3.1829e-01, -2.3886e-03,
          1.2850e-01,  1.3707e-01, -9.5401e-01, -3.6821e-01,  6.3580e-01,
          4.7126e-02, -5.0659e-01,  2.7011e-01,  3.2895e-01,  5.2875e-01,
          7.1642e-01,  5.5294e-01,  1.1277e-01,  8.9158e-01]])
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x74d4e1eff5e0>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_adaptive_gating(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            # Test adaptation to different input patterns
            # Case 1: Similar input to current state
            similar_input = state + torch.randn_like(state) * 0.1
            _, metrics1 = state_manager(state, similar_input, threshold=0.5, deterministic=True)
    
            # Case 2: Very different input
            different_input = torch.randn(batch_size, hidden_dim, device=device)
            _, metrics2 = state_manager(state, different_input, threshold=0.5, deterministic=True)
    
        # Memory gate should be more open (lower values) for different inputs
>       assert torch.mean(metrics1['memory_gate']) > torch.mean(metrics2['memory_gate'])
E       assert tensor(0.4373) > tensor(0.5014)
E        +  where tensor(0.4373) = <built-in method mean of type object at 0x74d5198678c0>(tensor([[0.4466],\n        [0.4281]]))
E        +    where <built-in method mean of type object at 0x74d5198678c0> = torch.mean
E        +  and   tensor(0.5014) = <built-in method mean of type object at 0x74d5198678c0>(tensor([[0.4656],\n        [0.5372]]))
E        +    where <built-in method mean of type object at 0x74d5198678c0> = torch.mean

tests/unit/integration/test_state_management.py:97: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4466],
        [0.4281]])
state: tensor([[-0.0046, -0.8307, -0.4420,  1.6286, -1.3511, -1.0486,  0.3171, -0.0421,
         -0.0460, -0.4760,  0.6386,  0.2148,  0.4966, -0.7065, -0.5173,  2.5037,
         -0.4005, -0.2967, -1.7716, -0.4197, -0.1839,  1.5103,  0.5361,  1.1292,
         -0.6580, -2.1182, -0.6929,  0.2603,  0.3279,  0.1000, -0.0166,  1.0406,
         -0.8146,  0.1125, -0.1910, -0.8793,  0.3679,  1.3233,  1.7359,  0.2043,
         -0.0387, -1.6342,  0.1319, -1.4819, -0.5558,  0.1816,  2.0678, -0.8482,
         -0.1171, -0.6863, -1.0614, -0.0904, -0.0454, -0.9330, -0.6501, -0.5303,
          0.8793,  0.9402, -1.0973, -0.4582,  0.7953,  0.7276,  0.9952, -0.9362],
        [ 1.9020,  0.5882,  2.0966, -0.4684,  0.1020, -0.0260, -0.0510, -1.0481,
          0.2548,  1.2712,  0.5860, -1.4407,  0.5132, -0.8692,  1.2142,  0.3937,
          0.1454,  1.7867,  1.1014,  0.0790,  0.0630, -0.1338,  0.0255,  1.3037,
          0.6399,  1.6430, -0.4002,  1.8989,  0.4411, -0.7200,  1.1549, -0.4476,
         -1.1762,  1.2539, -1.3005, -0.6501, -0.7786, -1.4240, -1.5052, -0.1017,
          0.4205,  2.1241,  0.2676,  0.4274,  0.0886, -0.5294,  0.4932, -1.7230,
         -0.7828,  1.8020, -0.1850,  0.8029, -0.6560, -0.1796,  1.6317, -0.0920,
          0.4898,  1.4418,  1.3841,  0.1555,  0.3335, -1.4094, -1.3076,  0.2628]])
inputs: tensor([[ 0.0139, -0.7467, -0.5182,  1.6578, -1.2441, -1.0564,  0.2663, -0.0940,
         -0.3106, -0.4683,  0.4373,  0.3795,  0.4307, -0.7133, -0.3787,  2.4524,
         -0.4949, -0.3160, -1.9112, -0.3239, -0.1711,  1.5353,  0.3608,  1.2323,
         -0.4935, -2.2462, -0.6495,  0.3371,  0.4450, -0.0171, -0.0572,  1.0275,
         -0.7988,  0.1636, -0.1626, -0.8686,  0.4098,  1.3565,  1.8892,  0.1901,
          0.0215, -1.6184,  0.1152, -1.4944, -0.4439,  0.2921,  1.8608, -0.9706,
          0.0500, -0.6840, -1.1719, -0.1524,  0.0084, -0.8521, -0.7759, -0.6608,
          0.7753,  0.9210, -1.2827, -0.4154,  0.9710,  0.9263,  0.9170, -0.9813],
        [ 1.8814,  0.5537,  2.0190, -0.5048,  0.2724, -0.0439, -0.0432, -0.9826,
          0.2580,  1.2394,  0.4776, -1.3289,  0.4926, -0.7032,  1.1680,  0.5689,
          0.1753,  1.8319,  1.0598, -0.0901,  0.1007, -0.1608,  0.0676,  1.3086,
          0.7153,  1.8526, -0.4195,  1.7745,  0.3901, -0.6032,  1.3100, -0.5422,
         -1.0610,  1.3252, -1.2068, -0.5200, -0.7132, -1.3482, -1.4328, -0.0473,
          0.3687,  2.2255,  0.4362,  0.4455,  0.0848, -0.8619,  0.4211, -1.8364,
         -0.7558,  1.9049, -0.1052,  0.7378, -0.5592, -0.0964,  1.8618, -0.0510,
          0.5490,  1.4784,  1.2788,  0.2961,  0.2810, -1.4237, -1.3234,  0.3155]])
candidate_state: tensor([[ 0.4029,  0.1944, -0.1377, -0.1698, -0.1473,  0.0557, -0.1644, -0.1049,
          0.1811,  0.5301, -0.1294, -0.0422,  0.7996,  0.8061,  0.0359, -0.0072,
         -0.1240, -0.1622, -0.0743, -0.0750, -0.1217,  0.0493, -0.1496, -0.1142,
          0.2742, -0.0903, -0.0284, -0.1534, -0.0756, -0.1655,  0.1130,  0.2478,
         -0.1659, -0.1695,  0.2597,  0.8941, -0.1677,  0.0122, -0.1672, -0.1664,
         -0.0058,  0.0050,  0.2988, -0.0950,  0.6576,  0.6607,  0.5480,  0.3884,
          0.7303,  0.5469,  0.7923,  1.3071, -0.0998, -0.1695,  0.6111, -0.1486,
         -0.1285, -0.1633,  0.3133,  0.2730,  0.1365,  0.0429, -0.0059, -0.0343],
        [-0.1050,  0.0825,  0.4715,  0.7253, -0.0660,  0.1193,  0.6707, -0.1558,
          0.1305,  0.5340,  0.0816,  0.4146,  0.3206, -0.1579, -0.1549,  0.1159,
          0.0847, -0.0107,  0.3722,  0.2381,  0.0138, -0.1494, -0.1673,  0.0582,
          0.3145,  0.3544,  0.0816,  1.2668, -0.1253, -0.1044,  0.3541,  0.5060,
          0.2151, -0.1698, -0.0833, -0.0954,  0.4040,  0.4767, -0.0359, -0.0848,
          0.2362,  0.4116,  0.0804, -0.1592, -0.1618, -0.1689, -0.1497, -0.0789,
         -0.1695,  0.3132, -0.1693, -0.0931, -0.1022,  0.5413, -0.0628, -0.1632,
          0.4291,  0.4188, -0.1036,  0.6248,  0.6972, -0.1600,  0.2417, -0.0601]])
new_state: tensor([[ 2.2088e-01, -2.6344e-01, -2.7361e-01,  6.3346e-01, -6.8493e-01,
         -4.3752e-01,  5.0644e-02, -7.6894e-02,  7.9649e-02,  8.0745e-02,
          2.1360e-01,  7.2594e-02,  6.6430e-01,  1.3051e-01, -2.1116e-01,
          1.1143e+00, -2.4749e-01, -2.2229e-01, -8.3240e-01, -2.2894e-01,
         -1.4946e-01,  7.0186e-01,  1.5664e-01,  4.4111e-01, -1.4215e-01,
         -9.9603e-01, -3.2518e-01,  3.1338e-02,  1.0458e-01, -4.6885e-02,
          5.5091e-02,  6.0188e-01, -4.5560e-01, -4.3560e-02,  5.8382e-02,
          1.0207e-01,  7.1503e-02,  5.9778e-01,  6.8276e-01, -8.2999e-04,
         -2.0488e-02, -7.2712e-01,  2.2427e-01, -7.1445e-01,  1.1569e-01,
          4.4674e-01,  1.2268e+00, -1.6388e-01,  3.5181e-01, -3.8735e-03,
         -3.5565e-02,  6.8289e-01, -7.5530e-02, -5.1046e-01,  4.7787e-02,
         -3.1909e-01,  3.2161e-01,  3.2955e-01, -3.1670e-01, -5.3529e-02,
          4.3075e-01,  3.4870e-01,  4.4121e-01, -4.3714e-01],
        [ 7.5414e-01,  2.9897e-01,  1.1671e+00,  2.1437e-01,  5.8854e-03,
          5.7126e-02,  3.6177e-01, -5.3777e-01,  1.8374e-01,  8.4955e-01,
          2.9753e-01, -3.7957e-01,  4.0305e-01, -4.6235e-01,  4.3115e-01,
          2.3479e-01,  1.1070e-01,  7.5865e-01,  6.8434e-01,  1.7002e-01,
          3.4882e-02, -1.4273e-01, -8.4757e-02,  5.9134e-01,  4.5377e-01,
          9.0598e-01, -1.2465e-01,  1.5374e+00,  1.1713e-01, -3.6790e-01,
          6.9688e-01,  9.7807e-02, -3.8044e-01,  4.3961e-01, -6.0436e-01,
         -3.3282e-01, -1.0222e-01, -3.3693e-01, -6.6484e-01, -9.2022e-02,
          3.1509e-01,  1.1447e+00,  1.6055e-01,  9.1919e-02, -5.4617e-02,
         -3.2324e-01,  1.2549e-01, -7.8267e-01, -4.3199e-01,  9.5051e-01,
         -1.7601e-01,  2.9043e-01, -3.3926e-01,  2.3273e-01,  6.6255e-01,
         -1.3272e-01,  4.5508e-01,  8.5667e-01,  5.3319e-01,  4.2392e-01,
          5.4153e-01, -6.9484e-01, -4.2149e-01,  7.8142e-02]])
memory_gate: tensor([[0.4656],
        [0.5372]])
state: tensor([[-0.0046, -0.8307, -0.4420,  1.6286, -1.3511, -1.0486,  0.3171, -0.0421,
         -0.0460, -0.4760,  0.6386,  0.2148,  0.4966, -0.7065, -0.5173,  2.5037,
         -0.4005, -0.2967, -1.7716, -0.4197, -0.1839,  1.5103,  0.5361,  1.1292,
         -0.6580, -2.1182, -0.6929,  0.2603,  0.3279,  0.1000, -0.0166,  1.0406,
         -0.8146,  0.1125, -0.1910, -0.8793,  0.3679,  1.3233,  1.7359,  0.2043,
         -0.0387, -1.6342,  0.1319, -1.4819, -0.5558,  0.1816,  2.0678, -0.8482,
         -0.1171, -0.6863, -1.0614, -0.0904, -0.0454, -0.9330, -0.6501, -0.5303,
          0.8793,  0.9402, -1.0973, -0.4582,  0.7953,  0.7276,  0.9952, -0.9362],
        [ 1.9020,  0.5882,  2.0966, -0.4684,  0.1020, -0.0260, -0.0510, -1.0481,
          0.2548,  1.2712,  0.5860, -1.4407,  0.5132, -0.8692,  1.2142,  0.3937,
          0.1454,  1.7867,  1.1014,  0.0790,  0.0630, -0.1338,  0.0255,  1.3037,
          0.6399,  1.6430, -0.4002,  1.8989,  0.4411, -0.7200,  1.1549, -0.4476,
         -1.1762,  1.2539, -1.3005, -0.6501, -0.7786, -1.4240, -1.5052, -0.1017,
          0.4205,  2.1241,  0.2676,  0.4274,  0.0886, -0.5294,  0.4932, -1.7230,
         -0.7828,  1.8020, -0.1850,  0.8029, -0.6560, -0.1796,  1.6317, -0.0920,
          0.4898,  1.4418,  1.3841,  0.1555,  0.3335, -1.4094, -1.3076,  0.2628]])
inputs: tensor([[ 0.8101,  1.1118,  0.0929,  2.5097,  1.2032, -0.9313,  0.0712, -1.6273,
         -0.4703, -0.4227, -0.6994,  0.3104, -1.0098,  1.1185,  0.0889, -0.0982,
         -0.4451, -0.0466, -0.5547,  0.4480,  0.1768,  0.0271, -1.1602,  1.3825,
         -0.3119, -0.9072, -1.9088, -0.6600,  0.8917,  1.1360, -1.3226, -0.8691,
          0.3350, -0.8679, -0.2191, -1.0433, -0.7759, -0.7498,  0.5988, -1.4551,
         -0.9603, -0.4080, -0.7319, -0.4247, -0.7292,  1.8667,  1.3041, -0.6049,
          0.1289,  1.0635,  1.5887, -0.9616, -0.9265,  1.5034, -0.2190, -0.5261,
          0.3080,  1.3573, -0.0911,  1.2504,  0.8178, -0.2000,  0.2959,  0.6777],
        [-0.6291, -0.8177, -0.7813, -0.5240, -0.0590,  1.0499,  0.0613, -1.3480,
         -0.5692,  0.5707, -0.0639,  0.5769, -0.2687, -0.0635,  0.6327, -0.0065,
         -0.2599, -0.5758,  1.1915,  1.6432,  0.7809, -1.0926, -0.4473,  0.0910,
          0.3665, -0.3892,  0.4264,  0.7535,  0.1262,  1.5046, -1.0795,  0.8766,
          1.4403, -2.0051, -0.0719, -1.2880,  0.2854,  0.8868, -0.2229,  0.6119,
          0.2970,  0.4199,  0.7268, -0.8892,  0.5516,  0.8800, -0.1767, -1.8151,
         -0.0997,  1.1206,  1.1284, -0.4092,  1.0431, -1.1110, -0.7656,  1.4586,
          0.0048, -0.3542,  0.1132,  0.7832,  0.7211, -2.3475,  0.8166, -1.2317]])
candidate_state: tensor([[-0.1441,  0.6503,  0.0176,  0.2199, -0.1122,  0.5478, -0.0864, -0.0790,
          0.0020,  0.5445,  0.6659, -0.1679, -0.1664, -0.1559, -0.1651, -0.1557,
         -0.1623, -0.0499,  0.0273, -0.1601, -0.1382,  0.0346, -0.1329, -0.0052,
          0.1798, -0.1592,  0.3018,  0.7785, -0.1291,  0.1082, -0.1077, -0.0817,
          0.4434,  0.2059, -0.0484,  0.2122,  0.2728, -0.1398,  0.3917, -0.0339,
         -0.1499,  0.4616, -0.1691,  0.2522, -0.1699, -0.1657, -0.0417,  0.5549,
          0.4730,  0.1361,  0.0137,  0.4743,  0.6555,  0.1271,  0.2098,  0.7653,
          1.2670,  0.8013,  0.7114, -0.1659,  0.1712,  0.0719, -0.1655,  0.2484],
        [-0.1693, -0.1138, -0.1182,  0.1259,  0.2794, -0.1628,  1.3205, -0.1400,
          0.7927,  0.0147, -0.0595, -0.1302,  0.0993,  0.5638, -0.0696, -0.0146,
          0.0890, -0.1120,  0.3665,  0.1124,  0.4054, -0.0250, -0.1576, -0.1028,
          0.1854,  1.3183, -0.0550,  0.0764,  0.1569, -0.0420,  0.0508,  0.4474,
         -0.1386,  0.1647,  0.1571, -0.1515, -0.1598,  0.1967,  0.2148, -0.1575,
         -0.1698,  0.0231,  0.0657,  0.2989, -0.1578, -0.1630,  0.5265,  0.3505,
          0.2492, -0.0303,  0.1271, -0.0772,  0.1341,  0.1985, -0.1474,  0.2495,
         -0.1149,  0.1982,  0.2584,  0.4105, -0.1234,  0.1227, -0.0407,  0.2203]])
new_state: tensor([[-0.0791, -0.0393, -0.1964,  0.8758, -0.6890, -0.1955,  0.1015, -0.0618,
         -0.0204,  0.0694,  0.6532,  0.0103,  0.1423, -0.4123, -0.3291,  1.0825,
         -0.2732, -0.1648, -0.8103, -0.2810, -0.1595,  0.7217,  0.1785,  0.5230,
         -0.2103, -1.0713, -0.1614,  0.5372,  0.0837,  0.1044, -0.0653,  0.4408,
         -0.1423,  0.1624, -0.1148, -0.2960,  0.3171,  0.5414,  1.0176,  0.0770,
         -0.0981, -0.5142, -0.0289, -0.5552, -0.3496, -0.0040,  0.9405, -0.0984,
          0.1982, -0.2468, -0.4868,  0.2114,  0.3292, -0.3665, -0.1906,  0.1621,
          1.0865,  0.8660, -0.1307, -0.3020,  0.4618,  0.3772,  0.3749, -0.3032],
        [ 0.9435,  0.2633,  1.0717, -0.1934,  0.1841, -0.0893,  0.5837, -0.6279,
          0.5037,  0.6898,  0.2873, -0.8343,  0.3216, -0.2061,  0.6201,  0.2047,
          0.1193,  0.9080,  0.7613,  0.0945,  0.2215, -0.0834, -0.0592,  0.6529,
          0.4296,  1.4927, -0.2405,  1.0555,  0.3096, -0.4062,  0.6439, -0.0334,
         -0.6961,  0.7499, -0.6260, -0.4194, -0.4923, -0.6740, -0.7093, -0.1275,
          0.1474,  1.1519,  0.1742,  0.3680, -0.0254, -0.3598,  0.5086, -0.7635,
         -0.3052,  0.9541, -0.0406,  0.3956, -0.2904, -0.0046,  0.8084,  0.0660,
          0.2100,  0.8663,  0.8632,  0.2735,  0.1221, -0.7005, -0.7213,  0.2432]])
____________ TestInformationIntegration.test_phi_metric_computation ____________

self = <test_integration.TestInformationIntegration object at 0x74d4e1d90400>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_phi_metric_computation(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        num_modules = 4
        input_dim = 32  # Updated to match expected shapes
    
        # Create sample inputs
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Initialize parameters
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[-4.4224e-01, -2.0076e+00,  7.0499e-02,  3.7851e-01, -2.7778e-01,
          -4.6669e-01, -1.0876e+00, -2.9068...e+00,
           1.0701e-01, -1.8491e-01, -2.2077e-01,  1.1823e+00, -7.0056e-01,
          -2.6341e-02,  1.1052e+00]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_______________ TestInformationIntegration.test_information_flow _______________

self = <test_integration.TestInformationIntegration object at 0x74d4e1d905b0>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_information_flow(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32  # Updated to match expected shapes
    
        inputs = torch.zeros(batch_size, num_modules, input_dim, device=device)  # ensure shape matches the model
    
        # Test with and without dropout
        integration_module.train()
>       output1, _ = integration_module(inputs, deterministic=False)

tests/unit/memory/test_integration.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0....., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_____________ TestInformationIntegration.test_entropy_calculations _____________

self = <test_integration.TestInformationIntegration object at 0x74d4e1d90760>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_entropy_calculations(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32  # Updated to match expected shapes
    
        # Test with different input distributions
        # Uniform distribution
        uniform_input = torch.ones(batch_size, num_modules, input_dim, device=device)
        integration_module.eval()
        with torch.no_grad():
>           _, phi_uniform = integration_module(uniform_input)

tests/unit/memory/test_integration.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1....., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
______________ TestInformationIntegration.test_memory_integration ______________

self = <test_integration.TestInformationIntegration object at 0x74d4e1d90910>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_memory_integration(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32  # Updated to match expected shapes
    
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Process through integration
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[ 1.1594, -0.4872, -0.8200,  1.6364,  0.8068, -1.3143, -1.2821,
          -1.1104, -0.4075,  0.5045, -0.0069,...        -0.3542, -1.1802, -1.2843,  0.3597,  0.6060,  0.7145, -0.6332,
           0.8154, -1.0031,  0.0118, -0.5938]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_____________ TestMemoryComponents.test_memory_sequence_processing _____________

self = <test_memory.TestMemoryComponents object at 0x74d4e1d918a0>
working_memory = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
device = device(type='cpu'), batch_size = 2, seq_length = 8, hidden_dim = 64

    def test_memory_sequence_processing(self, working_memory, device, batch_size, seq_length, hidden_dim):
        """Test working memory sequence processing."""
        # Test with different sequence lengths
        for test_length in [4, 8, 16]:
>           inputs = self.create_inputs(self.seed, batch_size, test_length, hidden_dim).to(device)

tests/unit/memory/test_memory.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_memory.TestMemoryComponents object at 0x74d4e1d918a0>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
________________ TestMemoryComponents.test_context_aware_gating ________________

self = <test_memory.TestMemoryComponents object at 0x74d4e1d91a80>
working_memory = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
device = device(type='cpu'), batch_size = 2, seq_length = 8, hidden_dim = 64

    def test_context_aware_gating(self, working_memory, device, batch_size, seq_length, hidden_dim):
        """Test context-aware gating mechanisms."""
        # Create two different input sequences with controlled differences
>       base_inputs = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim).to(device)

tests/unit/memory/test_memory.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_memory.TestMemoryComponents object at 0x74d4e1d91a80>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
__________________ TestMemoryComponents.test_memory_retention __________________

self = <test_memory.TestMemoryComponents object at 0x74d4e1d91e40>
working_memory = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
device = device(type='cpu'), batch_size = 2, seq_length = 8, hidden_dim = 64

    def test_memory_retention(self, working_memory, device, batch_size, seq_length, hidden_dim):
        """Test memory retention over sequences."""
        # Create a sequence with a distinctive pattern
        pattern = torch.ones(batch_size, 1, hidden_dim, device=device)
        inputs = torch.cat([
            pattern,
>           self.create_inputs(self.seed, batch_size, seq_length-2, hidden_dim).to(device),
            pattern
        ], dim=1)

tests/unit/memory/test_memory.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_memory.TestMemoryComponents object at 0x74d4e1d91e40>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
___________________ TestMemoryComponents.test_working_memory ___________________

self = <test_memory.TestMemoryComponents object at 0x74d4e1d92020>
working_memory = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
device = device(type='cpu'), batch_size = 2, seq_length = 8, hidden_dim = 64

    def test_working_memory(self, working_memory, device, batch_size, seq_length, hidden_dim):
        """Test WorkingMemory component."""
        inputs = torch.randn(batch_size, seq_length, hidden_dim, device=device)
        initial_state = torch.zeros(batch_size, hidden_dim, device=device)
    
        output, final_state = working_memory(inputs, initial_state, deterministic=True)
    
        # Verify shapes
        self.assert_output_shape(output, (batch_size, seq_length, hidden_dim))
>       self.assert_output_shape(final_state, (batch_size, hidden_dim))

tests/unit/memory/test_memory.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <test_memory.TestMemoryComponents object at 0x74d4e1d92020>
output = (tensor([[ 0.0098, -0.2070,  0.0819, -0.1142,  0.0330, -0.2198, -0.0295,  0.2070,
         -0.1438, -0.0873,  0.4372, ...500,
         -0.4479,  0.3391,  0.1905, -0.3798,  0.0564,  0.2251,  0.4181, -0.4739]],
       grad_fn=<AddBackward0>))
expected_shape = (2, 64)

    def assert_output_shape(self, output, expected_shape):
        """Assert output has expected shape."""
>       assert output.shape == expected_shape, f"Expected shape {expected_shape}, got {output.shape}"
E       AttributeError: 'tuple' object has no attribute 'shape'

tests/unit/test_base.py:47: AttributeError
______________________ TestGRUCell.test_gru_state_updates ______________________

self = <test_memory_components.TestGRUCell object at 0x74d4e1d92a40>
gru_cell = GRUCell(
  (update): Linear(in_features=128, out_features=64, bias=True)
  (reset): Linear(in_features=128, out_features=64, bias=True)
  (candidate): Linear(in_features=128, out_features=64, bias=True)
)

    def test_gru_state_updates(self, gru_cell):
        # Test dimensions
        batch_size = 2
        input_dim = 32
        hidden_dim = 64
    
        # Create sample inputs
        x = torch.randn(batch_size, input_dim)
        h = torch.randn(batch_size, hidden_dim)
    
        # Initialize parameters
>       gru_cell.reset_parameters()

tests/unit/memory/test_memory_components.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GRUCell(
  (update): Linear(in_features=128, out_features=64, bias=True)
  (reset): Linear(in_features=128, out_features=64, bias=True)
  (candidate): Linear(in_features=128, out_features=64, bias=True)
)
name = 'reset_parameters'

    def __getattr__(self, name: str) -> Any:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'GRUCell' object has no attribute 'reset_parameters'. Did you mean: 'get_parameter'?

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931: AttributeError
_______________________ TestGRUCell.test_gru_reset_gate ________________________

self = <test_memory_components.TestGRUCell object at 0x74d4e1d92bf0>
gru_cell = GRUCell(
  (update): Linear(in_features=128, out_features=64, bias=True)
  (reset): Linear(in_features=128, out_features=64, bias=True)
  (candidate): Linear(in_features=128, out_features=64, bias=True)
)

    def test_gru_reset_gate(self, gru_cell):
        batch_size = 2
        input_dim = 32
        hidden_dim = 64
    
        x = torch.randn(batch_size, input_dim)
        h = torch.randn(batch_size, hidden_dim)
    
>       gru_cell.reset_parameters()

tests/unit/memory/test_memory_components.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GRUCell(
  (update): Linear(in_features=128, out_features=64, bias=True)
  (reset): Linear(in_features=128, out_features=64, bias=True)
  (candidate): Linear(in_features=128, out_features=64, bias=True)
)
name = 'reset_parameters'

    def __getattr__(self, name: str) -> Any:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'GRUCell' object has no attribute 'reset_parameters'. Did you mean: 'get_parameter'?

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931: AttributeError
__________________ TestWorkingMemory.test_sequence_processing __________________

self = <test_memory_components.TestWorkingMemory object at 0x74d4e1d92ef0>
memory_module = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_sequence_processing(self, memory_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
        hidden_dim = 64
    
        # Create sample sequence
        inputs = torch.randn(batch_size, seq_length, input_dim)
    
        # Initialize parameters
>       memory_module.reset_parameters()

tests/unit/memory/test_memory_components.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
name = 'reset_parameters'

    def __getattr__(self, name: str) -> Any:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'WorkingMemory' object has no attribute 'reset_parameters'. Did you mean: 'get_parameter'?

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931: AttributeError
___________________ TestWorkingMemory.test_memory_retention ____________________

self = <test_memory_components.TestWorkingMemory object at 0x74d4e1d930a0>
memory_module = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_memory_retention(self, memory_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        inputs = torch.randn(batch_size, seq_length, input_dim)
    
        # Test with different initial states
        initial_state = torch.randn(batch_size, 64)
    
>       outputs1, final_state1 = memory_module(inputs, initial_state=initial_state, deterministic=True)

tests/unit/memory/test_memory_components.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:83: in forward
    h, c = self.lstm(x, (h, c))
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(64, 64)
input = tensor([[-0.9802, -2.5509,  1.0513, -0.9286, -0.1738, -2.1272,  0.0070, -1.0037,
          0.5632, -0.5390, -0.3301,  ...  1.1889,  0.8048, -1.6138,  1.3717,
          0.5917, -0.6198,  0.6973,  1.1738,  1.3272,  0.9345,  0.1625,  0.0501]])
hx = (tensor([[ 2.5769,  0.0093, -0.2642, -0.2135, -0.3166, -0.5919, -1.4534, -1.9073,
          1.5983,  0.5750,  0.9150, ...0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))

    def forward(
        self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None
    ) -> Tuple[Tensor, Tensor]:
        if input.dim() not in (1, 2):
            raise ValueError(
                f"LSTMCell: Expected input to be 1D or 2D, got {input.dim()}D instead"
            )
        if hx is not None:
            for idx, value in enumerate(hx):
                if value.dim() not in (1, 2):
                    raise ValueError(
                        f"LSTMCell: Expected hx[{idx}] to be 1D or 2D, got {value.dim()}D instead"
                    )
        is_batched = input.dim() == 2
        if not is_batched:
            input = input.unsqueeze(0)
    
        if hx is None:
            zeros = torch.zeros(
                input.size(0), self.hidden_size, dtype=input.dtype, device=input.device
            )
            hx = (zeros, zeros)
        else:
            hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx
    
>       ret = _VF.lstm_cell(
            input,
            hx,
            self.weight_ih,
            self.weight_hh,
            self.bias_ih,
            self.bias_hh,
        )
E       RuntimeError: input has inconsistent input_size: got 32 expected 64

../../.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1705: RuntimeError
_______________ TestConsciousnessStateManager.test_state_updates _______________

self = <test_consciousness_state_management.TestConsciousnessStateManager object at 0x74d4e1d93640>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_state_updates(self, device, state_manager):
        # Test dimensions
        batch_size = 2
        hidden_dim = 64
    
        # Create sample state and inputs
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        # Initialize parameters
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test output shapes
        assert new_state.shape == state.shape
        assert 'memory_gate' in metrics
        assert 'energy_cost' in metrics
        assert 'state_value' in metrics
    
        # Test memory gate properties
>       assert metrics['memory_gate'].shape == (batch_size, hidden_dim)  # Updated shape
E       assert torch.Size([2, 1]) == (2, 64)
E         
E         At index 1 diff: 1 != 64
E         Use -v to get more diff

tests/unit/state/test_consciousness_state_management.py:44: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5637],
        [0.5205]])
state: tensor([[ 6.5665e-01, -1.5228e-01, -2.0093e-01, -9.3677e-01, -2.4498e-01,
         -1.4227e-01,  3.4789e-01, -1.0471e+00,  1.2661e+00,  2.1532e+00,
          1.2944e+00,  3.0895e-01,  4.3496e-01, -4.5348e-01, -3.8704e-01,
         -9.6354e-01,  4.9757e-01,  1.0891e-01, -1.1301e+00,  2.0946e+00,
         -1.4446e+00,  1.6232e-01, -7.1584e-01, -9.4224e-01,  3.5863e-01,
          1.2962e-01, -1.1760e+00, -1.4091e+00, -9.5867e-02,  2.0788e-01,
         -9.0591e-01, -1.6123e+00,  7.6874e-02, -4.4603e-01, -1.3883e+00,
          7.2259e-01, -3.1743e-01,  3.7810e-01,  2.1524e-01,  8.3736e-02,
         -9.7122e-01,  1.1406e+00,  3.2447e-01,  2.1624e+00,  1.2865e-01,
          4.8753e-01,  3.3932e-01, -7.5564e-01, -2.7567e+00, -1.4047e+00,
         -1.5775e+00,  1.0474e+00, -5.2101e-01,  2.8735e-01,  4.2427e-01,
          5.5305e-01, -2.0090e-01, -9.8143e-01, -1.3084e+00, -1.4602e+00,
         -2.2155e-01,  1.5302e+00, -1.7005e-01,  3.2771e-01],
        [-4.0162e-01,  4.3458e-01, -1.5100e+00, -1.0083e+00,  5.1561e-01,
          1.4309e-01, -1.3317e+00, -1.2274e-01, -3.7759e-01,  1.2223e+00,
         -2.0404e-01,  9.5041e-01,  7.6808e-01, -1.7640e+00,  4.4184e-01,
         -5.9860e-01, -1.3934e-01,  9.9269e-01,  3.7330e-01,  1.2940e+00,
         -9.5943e-01, -1.9997e+00,  6.2478e-01, -1.1029e+00,  2.1852e+00,
         -5.5836e-02,  1.0432e+00,  5.8157e-01, -9.1628e-02,  8.8692e-01,
         -2.9720e-02,  6.3342e-01,  1.3780e-01,  9.4910e-02,  1.8494e+00,
         -7.5284e-01,  4.2030e-01,  1.0026e+00, -4.9365e-01, -5.2605e-02,
          1.9917e-01,  6.5652e-01,  1.5258e-03, -3.6600e-01,  6.7940e-01,
         -1.7813e+00, -6.9951e-01,  4.9702e-03,  1.1589e+00, -4.5067e-01,
         -1.2742e+00,  6.2946e-01,  4.8273e-01,  1.1663e+00, -1.3433e+00,
          3.8340e-01, -6.3305e-01,  3.2807e-01, -8.8513e-01,  5.2178e-01,
          4.6166e-01, -1.6035e-01,  4.6263e-01,  4.6942e-02]])
inputs: tensor([[ 1.8401e+00,  4.7266e-02, -3.5690e-01, -3.5338e-01,  7.0858e-01,
         -7.0400e-01,  1.8408e-01,  4.0104e-01,  9.0827e-02,  1.3638e+00,
         -6.4228e-01, -1.5410e+00, -1.2894e+00,  2.6241e+00,  1.5077e-01,
         -3.1153e-01,  1.1522e+00,  1.6552e+00,  2.5389e-02,  3.1288e-01,
         -1.4605e-01,  3.1821e-01, -1.0674e+00,  4.6885e-01, -4.7929e-01,
          8.4287e-01, -1.6952e-02,  5.5523e-01,  8.9376e-01, -1.3294e+00,
          1.1928e+00,  9.6145e-01, -3.6188e-01, -1.2793e+00, -2.0456e-01,
          2.5081e-02,  6.8284e-01,  5.0952e-01, -3.5464e-01, -1.4633e+00,
          1.0786e+00, -2.3886e+00,  5.9362e-01, -3.9318e-01, -1.5324e+00,
         -6.5247e-01,  4.5102e-01, -2.2184e-01,  3.9737e-01, -5.9796e-01,
          3.6556e-01,  2.5369e+00,  9.9740e-01,  1.4088e-01, -1.9335e+00,
          5.3602e-01, -8.1784e-02,  2.4092e+00, -7.3990e-03, -1.5312e+00,
         -1.4893e-01, -5.4581e-01, -7.7376e-01,  4.3086e-01],
        [-1.8223e-01, -3.7319e-01, -1.0984e+00,  6.3161e-01,  5.1522e-01,
          3.1002e-01,  1.0003e+00,  6.9534e-01, -3.4574e-01,  1.2280e-01,
          3.0399e-01,  1.3305e-01,  1.0860e+00, -7.3905e-01, -2.5040e-01,
         -1.4277e+00, -1.9477e-01,  4.5058e-01, -6.5393e-01, -2.8608e-02,
          1.9164e+00,  6.7650e-01, -4.9410e-01, -4.3937e-01, -6.7226e-01,
         -8.5618e-01, -4.1976e-01, -2.3037e+00, -2.4469e-03,  1.8568e+00,
          1.2552e+00,  2.9163e-02, -7.2441e-01,  5.4152e-01,  1.3277e-01,
          6.5169e-01, -1.3720e+00, -1.5072e+00,  7.2424e-01, -8.9811e-01,
          5.0621e-01, -3.7838e-01,  3.3022e-01,  8.8783e-01, -7.4641e-01,
          9.1653e-01,  1.1723e+00, -1.0407e+00,  1.9256e-01, -5.7676e-01,
         -1.2574e+00, -4.1160e-01, -1.3235e+00,  9.0469e-01,  5.8404e-01,
         -1.5139e-02,  1.1392e+00, -5.3670e-01, -1.5221e-01,  9.1694e-01,
          8.1467e-01,  8.2936e-01,  2.4871e-01, -4.6818e-01]])
candidate_state: tensor([[-0.1593, -0.1698, -0.1330, -0.1632,  0.3922, -0.1402, -0.0884, -0.1460,
         -0.0449,  0.0844,  0.6001, -0.0847, -0.1378,  1.2785,  0.1228, -0.0528,
          0.1456, -0.0393,  0.0507,  0.0319, -0.1696, -0.1675,  0.6443, -0.1558,
          0.4846, -0.0680,  0.0100, -0.1373,  0.6775, -0.1632, -0.0571,  0.3180,
          0.2344,  0.2824, -0.1511, -0.1037,  0.0949,  0.1951,  0.0194, -0.0811,
          0.9423, -0.1650, -0.1700,  0.1729, -0.0303,  0.1578,  0.5448,  1.0180,
          1.5787, -0.1188, -0.1045,  0.2292, -0.1604, -0.1671, -0.1420,  0.1210,
         -0.0996,  0.0460, -0.1560, -0.1654,  0.3172,  0.3539, -0.0818,  0.1329],
        [-0.1671,  0.1006, -0.0054,  0.3574,  0.1090, -0.0982, -0.1431,  0.7585,
         -0.1230, -0.1652,  0.5398,  0.2135,  0.0815, -0.1415,  0.4712,  0.1204,
         -0.0609,  0.7595,  0.5854,  0.1861, -0.1662, -0.1661, -0.1623,  0.8561,
          0.1370, -0.1056,  0.0661,  0.2555, -0.0554,  0.1096, -0.1612,  0.2629,
         -0.1492, -0.1670, -0.1545,  0.1944,  0.1267, -0.1255,  0.1469,  0.0094,
         -0.1164, -0.1436, -0.0122,  1.0077,  0.2955,  0.0082,  0.0175, -0.0719,
         -0.0942, -0.1700, -0.1684, -0.1105,  0.3418, -0.1696,  0.0178,  0.1720,
         -0.1692,  0.3445, -0.0325, -0.1003,  0.0631,  0.2025,  0.1033, -0.1603]])
new_state: tensor([[ 3.0065e-01, -1.5994e-01, -1.7129e-01, -5.9928e-01,  3.3030e-02,
         -1.4135e-01,  1.5756e-01, -6.5397e-01,  6.9412e-01,  1.2506e+00,
          9.9149e-01,  1.3720e-01,  1.8509e-01,  3.0218e-01, -1.6458e-01,
         -5.6620e-01,  3.4399e-01,  4.4241e-02, -6.1489e-01,  1.1947e+00,
         -8.8830e-01,  1.8430e-02, -1.2241e-01, -5.9911e-01,  4.1361e-01,
          4.3404e-02, -6.5853e-01, -8.5420e-01,  2.4157e-01,  4.5966e-02,
         -5.3557e-01, -7.7008e-01,  1.4560e-01, -1.2823e-01, -8.4849e-01,
          3.6207e-01, -1.3751e-01,  2.9826e-01,  1.2980e-01,  1.1822e-02,
         -1.3635e-01,  5.7094e-01,  1.0876e-01,  1.2944e+00,  5.9313e-02,
          3.4367e-01,  4.2896e-01,  1.8212e-02, -8.6522e-01, -8.4371e-01,
         -9.3482e-01,  6.9044e-01, -3.6366e-01,  8.9085e-02,  1.7719e-01,
          3.6454e-01, -1.5670e-01, -5.3318e-01, -8.0564e-01, -8.9532e-01,
          1.3512e-02,  1.0170e+00, -1.3153e-01,  2.4273e-01],
        [-2.8919e-01,  2.7447e-01, -7.8860e-01, -3.5351e-01,  3.2064e-01,
          2.7389e-02, -7.6179e-01,  2.9975e-01, -2.5552e-01,  5.5705e-01,
          1.5259e-01,  5.9709e-01,  4.3889e-01, -9.8611e-01,  4.5591e-01,
         -2.5386e-01, -1.0171e-01,  8.8091e-01,  4.7501e-01,  7.6285e-01,
         -5.7914e-01, -1.1206e+00,  2.4742e-01, -1.6366e-01,  1.2032e+00,
         -7.9686e-02,  5.7472e-01,  4.2523e-01, -7.4252e-02,  5.1423e-01,
         -9.2753e-02,  4.5579e-01,  1.7972e-04, -3.0670e-02,  8.8862e-01,
         -2.9870e-01,  2.7951e-01,  4.6172e-01, -1.8654e-01, -2.2880e-02,
          4.7877e-02,  2.7290e-01, -5.0561e-03,  2.9262e-01,  4.9533e-01,
         -9.2331e-01, -3.5575e-01, -3.1864e-02,  5.5810e-01, -3.1609e-01,
         -7.4404e-01,  2.7467e-01,  4.1515e-01,  5.2582e-01, -6.9069e-01,
          2.8203e-01, -4.1068e-01,  3.3594e-01, -4.7633e-01,  2.2352e-01,
          2.7057e-01,  1.3602e-02,  2.9034e-01, -5.2430e-02]])
______________ TestConsciousnessStateManager.test_rl_optimization ______________

self = <test_consciousness_state_management.TestConsciousnessStateManager object at 0x74d4e1d937f0>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_rl_optimization(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test RL loss computation
        reward = torch.ones(batch_size, 1, device=device)  # Mock reward
        value_loss, td_error = state_manager.get_rl_loss(
            state_value=metrics['state_value'],
            reward=reward,
            next_state_value=metrics['state_value']
        )
    
        # Test loss properties
        assert torch.is_tensor(value_loss)
        assert value_loss.item() >= 0.0
>       assert td_error.shape == (batch_size, 1)  # changed to match actual output
E       assert torch.Size([2, 2, 1]) == (2, 1)
E         
E         At index 1 diff: 2 != 1
E         Left contains one more item: 1
E         Use -v to get more diff

tests/unit/state/test_consciousness_state_management.py:77: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5689],
        [0.5773]])
state: tensor([[-0.8651,  0.1913, -0.0645, -0.8713,  0.8679, -0.4472,  0.0373,  0.5370,
         -0.6810, -2.0182,  0.9289,  0.2956, -0.0326,  0.2511, -0.2576, -1.7458,
          0.1569,  0.3154,  2.2505,  0.6932, -0.5283, -0.9864,  1.6864, -1.1497,
         -0.1290,  0.2553,  0.8380,  1.9550,  0.5308, -0.3694,  0.0425, -0.0084,
         -0.2790,  0.6615, -0.6750,  0.6444,  0.4084, -1.5827, -1.8837,  1.2499,
         -0.3463, -0.1256, -0.8956,  0.7741,  0.4321,  0.0651, -0.9160, -0.0812,
          0.2652,  0.8787, -0.0612,  1.3209,  0.4303, -0.7373,  2.0433,  3.0962,
          0.5593,  0.1466, -0.0933, -1.1014, -1.4859,  0.2024, -0.3897,  0.6157],
        [-3.0168,  2.3136, -0.3806,  0.2590,  0.0584,  1.6136,  1.7489,  0.4145,
          1.2160, -1.2218,  0.4253, -0.7097, -0.1573, -0.1616,  0.7932, -1.3236,
         -1.9277,  0.9064,  0.1850, -0.0331,  1.7243, -1.1046, -0.1074,  1.9060,
          0.2643, -0.5370,  0.2055,  0.9203, -0.5956, -1.5115, -0.8483, -0.2554,
          1.1679, -0.0857,  1.5259,  0.9223, -1.9911,  0.2451, -1.4332,  1.7816,
         -1.2721, -0.3487,  0.2230,  3.4685, -1.3482, -0.8909, -1.1812,  1.8120,
         -0.6480, -2.1399,  1.6707, -1.4702,  0.3821, -0.2261,  0.4000,  1.3051,
         -0.1963, -0.8810,  2.2989,  1.1835, -0.1628,  1.5732, -0.5142,  0.6686]])
inputs: tensor([[-1.5623,  1.0835,  0.2832,  2.1419, -0.4139,  0.6154,  0.2956, -0.9230,
          0.7107,  1.4511, -1.6755, -1.4961, -1.2123, -0.8074, -0.4882,  0.4931,
         -0.5989,  0.4135, -1.5378, -0.7442, -0.4680, -1.4017, -0.5087, -0.6802,
          0.0890,  1.1821,  0.9811, -1.2883, -2.1560,  0.3297,  0.0073,  1.3234,
          1.7996, -0.5895,  0.1076,  0.5471, -0.1427,  1.2879,  0.0487,  1.1587,
          0.3419, -0.0158,  0.2253,  0.5263, -0.9731,  0.5673,  0.5273, -1.4535,
         -0.7676, -0.2333, -0.6358,  0.0450,  0.0751, -0.0303,  1.5297,  0.0112,
         -0.7199, -0.4536,  0.2624,  0.9440,  0.2350,  1.6140, -0.3795,  0.3268],
        [-0.6091,  1.9324, -0.9755,  1.1966, -0.5263,  0.5682,  0.1579, -1.9945,
         -0.0391, -1.0154, -1.0960, -1.8876, -0.3930, -1.5992,  0.5002, -0.7950,
          1.2349, -1.0601, -0.2269,  2.6848,  1.2389,  0.6370,  1.6119,  0.2389,
          0.6865, -1.6743,  0.5444, -0.8917, -1.6053,  1.0440, -0.8913,  0.6154,
         -1.4667,  0.8872, -0.1768,  1.2787, -0.0596,  0.2800,  1.7361, -0.2370,
         -0.5011, -0.3721,  0.0923,  2.1334, -0.0535, -0.5073, -1.9337,  0.1136,
          0.5876, -0.3716, -0.6980, -0.7876, -0.1465, -1.3959, -0.0750,  1.1492,
          0.5471, -0.1991, -1.7313,  1.5657,  0.0914, -1.0803,  1.1009, -0.4079]])
candidate_state: tensor([[-7.1845e-02, -9.1215e-02,  1.3143e-02,  3.9916e-01, -1.6985e-01,
          7.4923e-01,  8.6547e-01,  2.5507e-01,  3.9535e-01, -1.2298e-01,
          2.5344e-01,  1.0914e+00, -6.6870e-02, -1.5907e-01,  2.1158e-01,
          6.9935e-01, -1.3565e-01,  1.0853e-01,  1.7436e-02, -9.5615e-02,
          1.1349e-01,  3.2220e-01,  1.4688e-01, -7.9378e-02, -1.6592e-01,
         -1.5490e-01,  1.5471e-01, -1.3460e-01,  3.5220e-01, -9.5081e-02,
          6.8580e-03, -2.6760e-02,  1.2754e+00,  3.1376e-01, -3.2632e-02,
         -1.2465e-03,  2.2213e-01,  8.2984e-01, -1.4443e-01,  8.7383e-03,
         -6.0103e-02,  1.2926e-01, -1.6993e-01, -6.0617e-02, -4.1811e-02,
         -1.5484e-01, -8.1304e-02,  1.4809e-01,  1.3071e-01, -9.5276e-02,
         -1.4271e-01, -5.1574e-02,  2.7614e-01,  9.6272e-02,  1.3704e-01,
         -1.2629e-02,  2.3144e-01,  1.1187e-01, -7.9365e-02, -1.2208e-01,
         -6.6970e-02,  8.8518e-01, -1.6013e-01,  2.8585e-02],
        [ 9.6251e-01,  1.7107e-01,  6.1253e-01, -8.1045e-02, -8.5124e-02,
          9.9069e-02, -1.4558e-01,  7.5954e-01,  1.9535e-01, -1.4851e-01,
         -1.0126e-01,  6.6609e-01,  4.5583e-01,  1.8112e-03,  1.2469e+00,
         -1.3871e-01,  2.6572e-01,  5.4242e-01, -1.0444e-01,  3.7136e-02,
         -1.5152e-01, -1.4822e-01,  1.2675e-02, -1.1000e-01,  1.2832e-01,
         -1.4161e-01, -1.6888e-01,  4.3755e-01, -9.7378e-02, -1.6572e-01,
          7.4450e-01, -8.1778e-02, -1.6651e-01, -1.6095e-01, -1.6986e-01,
          5.5626e-01,  2.1462e-02,  3.6891e-01, -1.4796e-01,  2.7152e-01,
         -1.6309e-01,  1.1605e+00, -8.8681e-02,  1.7153e-01,  1.5701e-02,
          7.4811e-02, -1.0002e-01,  1.8022e-01, -5.3979e-02,  8.0313e-01,
         -1.4762e-01,  2.5789e-01, -2.2100e-02, -1.3167e-01,  6.3364e-01,
          9.2079e-01,  1.1454e-01, -1.6330e-01,  1.0924e-01,  1.4537e+00,
          1.6380e-01,  1.5746e-01, -1.4073e-01,  3.9249e-01]])
new_state: tensor([[-0.5231,  0.0695, -0.0311, -0.3236,  0.4206,  0.0686,  0.3943,  0.4155,
         -0.2170, -1.2012,  0.6377,  0.6386, -0.0474,  0.0743, -0.0553, -0.6918,
          0.0308,  0.2262,  1.2879,  0.3531, -0.2517, -0.4223,  1.0227, -0.6883,
         -0.1449,  0.0785,  0.5434,  1.0542,  0.4538, -0.2511,  0.0272, -0.0163,
          0.3911,  0.5116, -0.3981,  0.3661,  0.3281, -0.5427, -1.1339,  0.7149,
         -0.2229, -0.0157, -0.5828,  0.4143,  0.2278, -0.0297, -0.5562,  0.0176,
          0.2072,  0.4589, -0.0963,  0.7293,  0.3639, -0.3779,  1.2215,  1.7561,
          0.4180,  0.1316, -0.0873, -0.6793, -0.8742,  0.4967, -0.2908,  0.3626],
        [-1.3346,  1.4079,  0.0392,  0.1152, -0.0023,  0.9734,  0.9481,  0.5603,
          0.7845, -0.7681,  0.2027, -0.1281,  0.1019, -0.0925,  0.9849, -0.8227,
         -1.0005,  0.7525,  0.0626, -0.0034,  0.9314, -0.7003, -0.0566,  1.0538,
          0.2068, -0.3699,  0.0472,  0.7162, -0.3850, -0.9426, -0.1750, -0.1820,
          0.6038, -0.1175,  0.8091,  0.7676, -1.1403,  0.2974, -0.8899,  1.1432,
         -0.8033,  0.2892,  0.0912,  2.0748, -0.7717, -0.4827, -0.7242,  1.1222,
         -0.3969, -0.8958,  0.9021, -0.7397,  0.2113, -0.1862,  0.4987,  1.1427,
         -0.0649, -0.5776,  1.3733,  1.2977, -0.0248,  0.9747, -0.3563,  0.5519]])
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_consciousness_state_management.TestConsciousnessStateManager object at 0x74d4e1d93d00>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_adaptive_gating(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            # Test adaptation to different input patterns
            # Case 1: Similar input to current state
            similar_input = state + torch.randn_like(state) * 0.1
            _, metrics1 = state_manager(state, similar_input, threshold=0.5, deterministic=True)
    
            # Case 2: Very different input
            different_input = torch.randn(batch_size, hidden_dim, device=device)
            _, metrics2 = state_manager(state, different_input, threshold=0.5, deterministic=True)
    
        # Memory gate should be more open (lower values) for different inputs
        assert torch.mean(metrics1['memory_gate']) > torch.mean(metrics2['memory_gate'])
    
        # Energy cost should be higher for more different inputs
>       assert metrics2['energy_cost'].item() > metrics1['energy_cost'].item()
E       assert 0.4617244601249695 > 0.4718417823314667
E        +  where 0.4617244601249695 = <built-in method item of Tensor object at 0x74d4e1dfa7a0>()
E        +    where <built-in method item of Tensor object at 0x74d4e1dfa7a0> = tensor(0.4617).item
E        +  and   0.4718417823314667 = <built-in method item of Tensor object at 0x74d4e1dfae80>()
E        +    where <built-in method item of Tensor object at 0x74d4e1dfae80> = tensor(0.4718).item

tests/unit/state/test_consciousness_state_management.py:129: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4974],
        [0.4664]])
state: tensor([[ 1.2129,  0.4031, -0.6157,  0.2066, -0.2025, -1.6789,  0.9699,  1.4816,
          2.0621, -0.9441, -0.4318,  0.5959, -0.7015,  0.5681, -1.4581, -1.9609,
          0.7310, -1.7484, -0.2394,  0.3727, -0.2331,  0.2182, -0.7997,  0.8814,
         -0.1919, -1.6838, -0.2992,  0.5613, -0.9185,  1.0294,  0.9571,  1.8441,
         -1.6228,  0.0778, -0.6283, -1.6632, -0.4163,  0.3649,  0.4745,  0.7554,
         -0.2729, -0.4720, -0.1071,  1.0832,  0.1559, -1.8100, -1.2345,  0.8710,
         -0.4908,  1.0711, -0.2213, -0.2756, -0.0693,  1.0605,  0.4986,  0.0891,
          0.0738, -0.5950, -1.4960, -0.0589,  1.2500, -2.7415,  1.1903,  0.7783],
        [ 0.9897, -0.3846,  0.0251, -0.1531,  1.2091, -1.0570,  0.1531,  0.2342,
          0.2452,  0.4838, -1.0563,  0.3670, -2.2843,  0.0566, -0.0880,  2.0686,
         -0.7447,  1.7747, -1.8691, -0.4469, -0.2299, -0.7236,  2.7994,  1.1296,
          1.4817, -0.2299,  0.0500, -0.5810,  0.5789,  0.7213, -0.4431,  0.8665,
          0.7054,  0.3319, -0.6459,  0.2307, -0.5591, -0.2159,  1.2354, -0.1257,
          0.1552,  1.6297,  1.1887, -0.9477,  0.8706, -2.1452,  0.3704, -2.2866,
         -1.0785, -0.2734,  0.4142, -0.5656, -0.4220,  1.1968,  0.8583,  1.4575,
          3.7075,  0.1530,  0.8736, -0.1210, -1.3485,  0.6142, -0.2549, -0.0930]])
inputs: tensor([[ 1.0441,  0.4955, -0.5637,  0.2668, -0.1374, -1.6950,  0.8768,  1.5347,
          2.2410, -0.9583, -0.4277,  0.3157, -0.6075,  0.5866, -1.3014, -2.1534,
          0.7448, -1.5344, -0.2637,  0.2669, -0.2644,  0.2310, -0.9177,  0.9558,
         -0.2920, -1.7242, -0.3125,  0.4522, -0.9010,  0.9509,  0.9012,  1.7839,
         -1.6297,  0.1041, -0.7167, -1.8368, -0.3581,  0.4314,  0.3677,  0.7390,
         -0.3507, -0.5248, -0.3036,  1.1705,  0.0578, -1.6456, -1.2507,  0.8503,
         -0.6040,  1.0370, -0.2802, -0.4215,  0.0691,  1.0408,  0.5343,  0.1956,
          0.2021, -0.6255, -1.5397, -0.0956,  1.3934, -2.6513,  1.1885,  0.7421],
        [ 0.9562, -0.4665,  0.1342, -0.1861,  1.1580, -1.1211,  0.2740, -0.0257,
          0.1327,  0.6968, -1.0863,  0.4981, -2.2829,  0.0154, -0.1509,  2.1804,
         -0.6384,  1.8063, -1.9636, -0.3649, -0.2444, -0.6697,  2.8894,  1.1517,
          1.3997, -0.1773, -0.0764, -0.4029,  0.7334,  0.6970, -0.5488,  0.7312,
          0.7262,  0.3275, -0.7336,  0.1718, -0.6177, -0.2809,  1.1885, -0.1300,
          0.3371,  1.6937,  1.2543, -0.8310,  0.7339, -2.2347,  0.3300, -2.2274,
         -1.2775, -0.2667,  0.2267, -0.6806, -0.4419,  1.0858,  0.6707,  1.5792,
          3.6976,  0.2188,  1.0513, -0.0687, -1.2674,  0.6933, -0.1675, -0.1238]])
candidate_state: tensor([[-0.1590,  0.6417,  0.4514, -0.0067, -0.1037,  0.4301,  0.0781,  0.3605,
          1.0938,  0.1472, -0.0419, -0.1411, -0.1218,  0.3846, -0.0910, -0.0183,
          0.0812, -0.1700, -0.0562,  0.0583, -0.1338,  0.4375, -0.1265, -0.0093,
          0.3824, -0.1598,  0.9801, -0.0266, -0.1243,  0.1192, -0.1670, -0.1534,
          0.4643, -0.1685,  0.0544,  0.4374,  0.2765,  0.2999, -0.1037,  0.0101,
         -0.0393,  0.6460,  1.7166, -0.1696, -0.1319,  0.4691,  0.2875, -0.0738,
          0.0262,  0.1996, -0.1290,  0.1071,  0.2151, -0.1699, -0.1254, -0.0519,
          0.3944, -0.0300,  0.1337,  0.6975,  0.3009,  0.7896, -0.0185, -0.0904],
        [ 0.3354, -0.0612, -0.1100, -0.1697, -0.0513,  0.3800,  0.0179,  0.3734,
         -0.1284,  0.3336,  0.1526,  0.1786, -0.1075, -0.1334, -0.0255, -0.0608,
          0.2008,  0.4644,  0.0213,  0.2855, -0.0079, -0.0380, -0.0812, -0.1588,
         -0.0687,  1.2409,  0.2792, -0.1697, -0.1627, -0.0707,  0.4760, -0.1009,
         -0.1233,  0.5841, -0.0433, -0.1105,  0.2971,  0.3720, -0.1544,  0.2236,
         -0.1660,  0.0425, -0.1697, -0.1670, -0.1686,  0.4091,  0.0755, -0.1503,
          0.2026,  1.5644,  0.3894, -0.1290, -0.1691,  0.3581,  0.0516,  0.0185,
          1.3578, -0.1652, -0.0299, -0.1451,  0.9987, -0.1681,  1.1776,  0.1063]])
new_state: tensor([[ 0.5234,  0.5231, -0.0794,  0.0994, -0.1528, -0.6190,  0.5217,  0.9182,
          1.5755, -0.3956, -0.2358,  0.2255, -0.4102,  0.4759, -0.7710, -0.9846,
          0.4044, -0.9551, -0.1473,  0.2147, -0.1832,  0.3285, -0.4613,  0.4338,
          0.0968, -0.9179,  0.3438,  0.2658, -0.5193,  0.5719,  0.3921,  0.8402,
         -0.5739, -0.0460, -0.2852, -0.6075, -0.0681,  0.3322,  0.1839,  0.3808,
         -0.1555,  0.0899,  0.8094,  0.4536,  0.0113, -0.6646, -0.4696,  0.3962,
         -0.2309,  0.6331, -0.1749, -0.0833,  0.0736,  0.4421,  0.1850,  0.0182,
          0.2349, -0.3110, -0.6769,  0.3212,  0.7730, -0.9668,  0.5828,  0.3417],
        [ 0.6406, -0.2120, -0.0470, -0.1619,  0.5365, -0.2901,  0.0810,  0.3085,
          0.0458,  0.4036, -0.4112,  0.2665, -1.1227, -0.0448, -0.0547,  0.9323,
         -0.2401,  1.0755, -0.8603, -0.0560, -0.1114, -0.3577,  1.2622,  0.4421,
          0.6544,  0.5550,  0.1723, -0.3615,  0.1831,  0.2987,  0.0474,  0.3502,
          0.2631,  0.4665, -0.3243,  0.0486, -0.1022,  0.0978,  0.4937,  0.0607,
         -0.0162,  0.7827,  0.4638, -0.5311,  0.3160, -0.7822,  0.2130, -1.1466,
         -0.3949,  0.7073,  0.4010, -0.3326, -0.2870,  0.7492,  0.4278,  0.6896,
          2.4536, -0.0168,  0.3914, -0.1339, -0.0960,  0.1967,  0.5095,  0.0134]])
memory_gate: tensor([[0.4876],
        [0.4413]])
state: tensor([[ 1.2129,  0.4031, -0.6157,  0.2066, -0.2025, -1.6789,  0.9699,  1.4816,
          2.0621, -0.9441, -0.4318,  0.5959, -0.7015,  0.5681, -1.4581, -1.9609,
          0.7310, -1.7484, -0.2394,  0.3727, -0.2331,  0.2182, -0.7997,  0.8814,
         -0.1919, -1.6838, -0.2992,  0.5613, -0.9185,  1.0294,  0.9571,  1.8441,
         -1.6228,  0.0778, -0.6283, -1.6632, -0.4163,  0.3649,  0.4745,  0.7554,
         -0.2729, -0.4720, -0.1071,  1.0832,  0.1559, -1.8100, -1.2345,  0.8710,
         -0.4908,  1.0711, -0.2213, -0.2756, -0.0693,  1.0605,  0.4986,  0.0891,
          0.0738, -0.5950, -1.4960, -0.0589,  1.2500, -2.7415,  1.1903,  0.7783],
        [ 0.9897, -0.3846,  0.0251, -0.1531,  1.2091, -1.0570,  0.1531,  0.2342,
          0.2452,  0.4838, -1.0563,  0.3670, -2.2843,  0.0566, -0.0880,  2.0686,
         -0.7447,  1.7747, -1.8691, -0.4469, -0.2299, -0.7236,  2.7994,  1.1296,
          1.4817, -0.2299,  0.0500, -0.5810,  0.5789,  0.7213, -0.4431,  0.8665,
          0.7054,  0.3319, -0.6459,  0.2307, -0.5591, -0.2159,  1.2354, -0.1257,
          0.1552,  1.6297,  1.1887, -0.9477,  0.8706, -2.1452,  0.3704, -2.2866,
         -1.0785, -0.2734,  0.4142, -0.5656, -0.4220,  1.1968,  0.8583,  1.4575,
          3.7075,  0.1530,  0.8736, -0.1210, -1.3485,  0.6142, -0.2549, -0.0930]])
inputs: tensor([[-2.3872, -0.8464, -0.2666,  0.1670, -3.1275, -0.9195, -1.5355, -2.2661,
         -1.5055,  0.3335, -0.6709,  0.2338, -0.1101, -2.5771,  0.9827, -0.1595,
          0.2059, -1.1418, -0.9833, -1.1274,  1.9636,  0.5962, -3.0393, -0.9386,
          0.6509, -0.5274, -0.6506, -0.0378, -0.2502, -0.3308, -1.3095,  0.6860,
         -0.7108, -1.6129,  0.1166, -0.6883, -0.5421,  0.4645, -0.5057,  0.3769,
          0.9206,  1.9810, -0.1476,  1.1632,  0.1184, -0.1305,  0.1315, -1.7424,
          0.4584, -1.2811, -0.7317, -1.6019, -0.8354, -1.1399, -0.2660,  0.9884,
          0.5785,  1.1119, -0.4254,  1.7774, -0.8208,  2.4929,  0.5673,  0.3579],
        [-2.1316,  0.6077,  0.3801,  0.6372,  1.1363, -2.1141, -1.4309, -0.3112,
          1.2800,  2.0065,  0.1235, -1.1237, -0.4501, -0.5040,  1.4071, -0.5665,
          0.5142,  2.1725,  1.2120,  0.5296, -0.1668, -1.0583, -0.4101,  0.3373,
         -1.0764,  0.9985, -1.0909,  0.4312, -0.0570, -0.4255,  0.3420, -0.8472,
          0.0987,  1.3003, -0.0262, -0.4258, -0.6001,  0.1270, -1.7949, -0.3650,
          1.0777, -0.7975, -0.0266, -0.5281, -0.9308, -0.0957,  0.2691, -0.6268,
         -0.4830, -0.5803, -1.8473,  1.3362,  0.5132, -0.4421,  0.4847, -1.7187,
         -2.5195,  0.2438, -0.4005,  0.8220,  1.3431, -0.8439, -0.1762,  0.2891]])
candidate_state: tensor([[-0.1626, -0.1517, -0.0027, -0.1473,  0.0168, -0.1081, -0.0447,  0.8189,
         -0.0772, -0.1650, -0.0793,  1.0014,  0.0370, -0.0687,  0.0842,  0.6851,
          0.3496, -0.0580,  0.6123,  1.0990,  0.9406,  0.7264,  0.1340, -0.1647,
         -0.0615, -0.0830,  0.3089, -0.0726, -0.1192, -0.0650, -0.0277,  0.5840,
         -0.0775, -0.0684,  0.0293, -0.1301,  1.1095,  0.1035,  0.3338, -0.1364,
         -0.0737,  0.0099, -0.1089,  0.2809, -0.1447, -0.1192, -0.1540, -0.1698,
         -0.1542, -0.0371,  1.5455,  0.0616, -0.1659, -0.0729,  0.5791, -0.1619,
         -0.1288, -0.1572, -0.1150, -0.1666,  0.6939,  0.1197,  1.2358,  0.1259],
        [ 0.1607,  0.5820, -0.0487, -0.1205, -0.1677,  0.6594, -0.0831,  0.2811,
         -0.1518,  0.9252, -0.1279, -0.1686,  0.1142, -0.1686,  0.3490,  0.0095,
          0.1884,  0.3672, -0.1687, -0.0110,  0.0236,  0.0737, -0.0410, -0.1687,
         -0.1343,  0.9679,  0.2171,  0.2333, -0.1553, -0.1518, -0.1674,  0.1367,
          0.6359, -0.1692, -0.1293, -0.1699, -0.1559, -0.1309, -0.0685,  0.2403,
          1.0870, -0.0939, -0.1498, -0.1088,  0.1696, -0.1658,  0.0892,  0.1686,
          0.6205,  0.8729, -0.0095, -0.0999,  0.0090, -0.1476, -0.1575,  0.1634,
          0.4864, -0.0158,  0.8171, -0.1685, -0.1005,  0.3914,  0.7604, -0.1646]])
new_state: tensor([[ 5.0805e-01,  1.1884e-01, -3.0162e-01,  2.5244e-02, -9.0123e-02,
         -8.7401e-01,  4.4999e-01,  1.1420e+00,  9.6588e-01, -5.4487e-01,
         -2.5118e-01,  8.0369e-01, -3.2312e-01,  2.4182e-01, -6.6781e-01,
         -6.0505e-01,  5.3557e-01, -8.8219e-01,  1.9703e-01,  7.4487e-01,
          3.6836e-01,  4.7865e-01, -3.2123e-01,  3.4538e-01, -1.2507e-01,
         -8.6354e-01,  1.2419e-02,  2.3649e-01, -5.0894e-01,  4.6858e-01,
          4.5244e-01,  1.1984e+00, -8.3098e-01,  2.9141e-03, -2.9131e-01,
         -8.7760e-01,  3.6560e-01,  2.3096e-01,  4.0240e-01,  2.9840e-01,
         -1.7083e-01, -2.2507e-01, -1.0804e-01,  6.7208e-01,  1.8623e-03,
         -9.4360e-01, -6.8084e-01,  3.3771e-01, -3.1833e-01,  5.0326e-01,
          6.8406e-01, -1.0279e-01, -1.1877e-01,  4.7970e-01,  5.3984e-01,
         -3.9499e-02, -3.0010e-02, -3.7066e-01, -7.8835e-01, -1.1408e-01,
          9.6506e-01, -1.2754e+00,  1.2136e+00,  4.4400e-01],
        [ 5.2649e-01,  1.5550e-01, -1.6131e-02, -1.3488e-01,  4.3983e-01,
         -9.7947e-02,  2.1138e-02,  2.6039e-01,  2.3368e-02,  7.3042e-01,
         -5.3756e-01,  6.7732e-02, -9.4417e-01, -6.9186e-02,  1.5616e-01,
          9.1808e-01, -2.2334e-01,  9.8823e-01, -9.1897e-01, -2.0332e-01,
         -8.8245e-02, -2.7806e-01,  1.2123e+00,  4.0418e-01,  5.7880e-01,
          4.3935e-01,  1.4334e-01, -1.2602e-01,  1.6867e-01,  2.3343e-01,
         -2.8904e-01,  4.5873e-01,  6.6654e-01,  5.1874e-02, -3.5725e-01,
          6.8609e-03, -3.3381e-01, -1.6843e-01,  5.0684e-01,  7.8820e-02,
          6.7583e-01,  6.6662e-01,  4.4082e-01, -4.7900e-01,  4.7893e-01,
         -1.0392e+00,  2.1325e-01, -9.1477e-01, -1.2921e-01,  3.6706e-01,
          1.7744e-01, -3.0539e-01, -1.8116e-01,  4.4559e-01,  2.9074e-01,
          7.3441e-01,  1.9078e+00,  5.8658e-02,  8.4203e-01, -1.4752e-01,
         -6.5119e-01,  4.8972e-01,  3.1239e-01, -1.3303e-01]])
=============================== warnings summary ===============================
../../.local/lib/python3.10/site-packages/torch/__init__.py:1144
  /home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
    _C._set_default_tensor_type(t)

tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_model.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    inputs = {k: torch.tensor(v, dtype=torch.float32) for k, v in inputs.items()}

tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_model.py:119: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    {k: torch.tensor(v, dtype=torch.float32) for k, v in inputs.items()},

tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_model.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    state = torch.tensor(state, dtype=torch.float32)

tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_consistency
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_energy_efficiency
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_value_estimation
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    state = torch.tensor(state, dtype=torch.float32)

tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_consistency
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_energy_efficiency
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_value_estimation
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    inputs = torch.tensor(inputs, dtype=torch.float32)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
FAILED tests/test_environment.py::EnvironmentTests::test_core_imports - Asser...
FAILED tests/test_environment.py::EnvironmentTests::test_framework_versions
FAILED tests/test_environment.py::EnvironmentTests::test_hardware_detection
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_scaled_dot_product
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_attention_mask
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_consciousness_broadcasting
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_global_workspace_integration
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cross_modal_attention
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_modality_specific_processing
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_integration_stability
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cognitive_integration
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_rl_optimization
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_phi_metric_computation
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_information_flow
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_entropy_calculations
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_memory_integration
FAILED tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_sequence_processing
FAILED tests/unit/memory/test_memory.py::TestMemoryComponents::test_context_aware_gating
FAILED tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_retention
FAILED tests/unit/memory/test_memory.py::TestMemoryComponents::test_working_memory
FAILED tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_state_updates
FAILED tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_reset_gate
FAILED tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_sequence_processing
FAILED tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_memory_retention
FAILED tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_updates
FAILED tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_rl_optimization
FAILED tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
================== 32 failed, 15 passed, 27 warnings in 1.34s ==================
