============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.4, pluggy-1.5.0
rootdir: /home/kasinadhsarma/experiment/cognition-l3-experiment
plugins: cov-6.0.0, anyio-4.7.0
collected 45 items

tests/test_consciousness.py .F..FF                                       [ 13%]
tests/test_environment.py FFF..                                          [ 24%]
tests/unit/attention/test_attention.py FFFF                              [ 33%]
tests/unit/attention/test_attention_mechanisms.py ....                   [ 42%]
tests/unit/integration/test_cognitive_integration.py FFFF                [ 51%]
tests/unit/integration/test_state_management.py ..F.                     [ 60%]
tests/unit/memory/test_integration.py FFFF                               [ 68%]
tests/unit/memory/test_memory.py .EEEE                                   [ 80%]
tests/unit/memory/test_memory_components.py FFFF                         [ 88%]
tests/unit/state/test_consciousness_state_management.py FF..F            [100%]

==================================== ERRORS ====================================
____ ERROR at setup of TestMemoryComponents.test_memory_sequence_processing ____

self = <test_memory.TestMemoryComponents object at 0x7b4a4aeadc60>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
        return WorkingMemory(
            input_dim=hidden_dim,
            hidden_dim=hidden_dim,
            dropout_rate=0.1
>       ).to(self.device)

tests/unit/memory/test_memory.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
args = (<bound method TestMemoryComponents.device of <test_memory.TestMemoryComponents object at 0x7b4a4aeadc60>>,)
kwargs = {}

    def to(self, *args, **kwargs):
        r"""Move and/or cast the parameters and buffers.
    
        This can be called as
    
        .. function:: to(device=None, dtype=None, non_blocking=False)
           :noindex:
    
        .. function:: to(dtype, non_blocking=False)
           :noindex:
    
        .. function:: to(tensor, non_blocking=False)
           :noindex:
    
        .. function:: to(memory_format=torch.channels_last)
           :noindex:
    
        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
        floating point or complex :attr:`dtype`\ s. In addition, this method will
        only cast the floating point or complex parameters and buffers to :attr:`dtype`
        (if given). The integral parameters and buffers will be moved
        :attr:`device`, if that is given, but with dtypes unchanged. When
        :attr:`non_blocking` is set, it tries to convert/move asynchronously
        with respect to the host if possible, e.g., moving CPU Tensors with
        pinned memory to CUDA devices.
    
        See below for examples.
    
        .. note::
            This method modifies the module in-place.
    
        Args:
            device (:class:`torch.device`): the desired device of the parameters
                and buffers in this module
            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
                the parameters and buffers in this module
            tensor (torch.Tensor): Tensor whose dtype and device are the desired
                dtype and device for all parameters and buffers in this module
            memory_format (:class:`torch.memory_format`): the desired memory
                format for 4D parameters and buffers in this module (keyword
                only argument)
    
        Returns:
            Module: self
    
        Examples::
    
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> linear = nn.Linear(2, 2)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]])
            >>> linear.to(torch.double)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]], dtype=torch.float64)
            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)
            >>> gpu1 = torch.device("cuda:1")
            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
            >>> cpu = torch.device("cpu")
            >>> linear.to(cpu)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16)
    
            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.3741+0.j,  0.2382+0.j],
                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
            tensor([[0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
    
        """
>       device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
            *args, **kwargs
        )
E       TypeError: to() received an invalid combination of arguments - got (method), but expected one of:
E        * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1299: TypeError
_______ ERROR at setup of TestMemoryComponents.test_context_aware_gating _______

self = <test_memory.TestMemoryComponents object at 0x7b4a4aeade40>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
        return WorkingMemory(
            input_dim=hidden_dim,
            hidden_dim=hidden_dim,
            dropout_rate=0.1
>       ).to(self.device)

tests/unit/memory/test_memory.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
args = (<bound method TestMemoryComponents.device of <test_memory.TestMemoryComponents object at 0x7b4a4aeade40>>,)
kwargs = {}

    def to(self, *args, **kwargs):
        r"""Move and/or cast the parameters and buffers.
    
        This can be called as
    
        .. function:: to(device=None, dtype=None, non_blocking=False)
           :noindex:
    
        .. function:: to(dtype, non_blocking=False)
           :noindex:
    
        .. function:: to(tensor, non_blocking=False)
           :noindex:
    
        .. function:: to(memory_format=torch.channels_last)
           :noindex:
    
        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
        floating point or complex :attr:`dtype`\ s. In addition, this method will
        only cast the floating point or complex parameters and buffers to :attr:`dtype`
        (if given). The integral parameters and buffers will be moved
        :attr:`device`, if that is given, but with dtypes unchanged. When
        :attr:`non_blocking` is set, it tries to convert/move asynchronously
        with respect to the host if possible, e.g., moving CPU Tensors with
        pinned memory to CUDA devices.
    
        See below for examples.
    
        .. note::
            This method modifies the module in-place.
    
        Args:
            device (:class:`torch.device`): the desired device of the parameters
                and buffers in this module
            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
                the parameters and buffers in this module
            tensor (torch.Tensor): Tensor whose dtype and device are the desired
                dtype and device for all parameters and buffers in this module
            memory_format (:class:`torch.memory_format`): the desired memory
                format for 4D parameters and buffers in this module (keyword
                only argument)
    
        Returns:
            Module: self
    
        Examples::
    
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> linear = nn.Linear(2, 2)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]])
            >>> linear.to(torch.double)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]], dtype=torch.float64)
            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)
            >>> gpu1 = torch.device("cuda:1")
            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
            >>> cpu = torch.device("cpu")
            >>> linear.to(cpu)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16)
    
            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.3741+0.j,  0.2382+0.j],
                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
            tensor([[0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
    
        """
>       device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
            *args, **kwargs
        )
E       TypeError: to() received an invalid combination of arguments - got (method), but expected one of:
E        * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1299: TypeError
_____ ERROR at setup of TestMemoryComponents.test_information_integration ______

self = <test_memory.TestMemoryComponents object at 0x7b4a4aeae020>
hidden_dim = 64

    @pytest.fixture
    def info_integration(self, hidden_dim):
        """Create information integration module for testing."""
        return InformationIntegration(
            hidden_dim=hidden_dim,
            num_modules=4,
            dropout_rate=0.1
>       ).to(self.device)

tests/unit/memory/test_memory.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)
args = (<bound method TestMemoryComponents.device of <test_memory.TestMemoryComponents object at 0x7b4a4aeae020>>,)
kwargs = {}

    def to(self, *args, **kwargs):
        r"""Move and/or cast the parameters and buffers.
    
        This can be called as
    
        .. function:: to(device=None, dtype=None, non_blocking=False)
           :noindex:
    
        .. function:: to(dtype, non_blocking=False)
           :noindex:
    
        .. function:: to(tensor, non_blocking=False)
           :noindex:
    
        .. function:: to(memory_format=torch.channels_last)
           :noindex:
    
        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
        floating point or complex :attr:`dtype`\ s. In addition, this method will
        only cast the floating point or complex parameters and buffers to :attr:`dtype`
        (if given). The integral parameters and buffers will be moved
        :attr:`device`, if that is given, but with dtypes unchanged. When
        :attr:`non_blocking` is set, it tries to convert/move asynchronously
        with respect to the host if possible, e.g., moving CPU Tensors with
        pinned memory to CUDA devices.
    
        See below for examples.
    
        .. note::
            This method modifies the module in-place.
    
        Args:
            device (:class:`torch.device`): the desired device of the parameters
                and buffers in this module
            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
                the parameters and buffers in this module
            tensor (torch.Tensor): Tensor whose dtype and device are the desired
                dtype and device for all parameters and buffers in this module
            memory_format (:class:`torch.memory_format`): the desired memory
                format for 4D parameters and buffers in this module (keyword
                only argument)
    
        Returns:
            Module: self
    
        Examples::
    
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> linear = nn.Linear(2, 2)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]])
            >>> linear.to(torch.double)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]], dtype=torch.float64)
            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)
            >>> gpu1 = torch.device("cuda:1")
            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
            >>> cpu = torch.device("cpu")
            >>> linear.to(cpu)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16)
    
            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.3741+0.j,  0.2382+0.j],
                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
            tensor([[0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
    
        """
>       device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
            *args, **kwargs
        )
E       TypeError: to() received an invalid combination of arguments - got (method), but expected one of:
E        * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1299: TypeError
_________ ERROR at setup of TestMemoryComponents.test_memory_retention _________

self = <test_memory.TestMemoryComponents object at 0x7b4a4aeae200>
hidden_dim = 64

    @pytest.fixture
    def working_memory(self, hidden_dim):
        """Create working memory module for testing."""
        return WorkingMemory(
            input_dim=hidden_dim,
            hidden_dim=hidden_dim,
            dropout_rate=0.1
>       ).to(self.device)

tests/unit/memory/test_memory.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
args = (<bound method TestMemoryComponents.device of <test_memory.TestMemoryComponents object at 0x7b4a4aeae200>>,)
kwargs = {}

    def to(self, *args, **kwargs):
        r"""Move and/or cast the parameters and buffers.
    
        This can be called as
    
        .. function:: to(device=None, dtype=None, non_blocking=False)
           :noindex:
    
        .. function:: to(dtype, non_blocking=False)
           :noindex:
    
        .. function:: to(tensor, non_blocking=False)
           :noindex:
    
        .. function:: to(memory_format=torch.channels_last)
           :noindex:
    
        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
        floating point or complex :attr:`dtype`\ s. In addition, this method will
        only cast the floating point or complex parameters and buffers to :attr:`dtype`
        (if given). The integral parameters and buffers will be moved
        :attr:`device`, if that is given, but with dtypes unchanged. When
        :attr:`non_blocking` is set, it tries to convert/move asynchronously
        with respect to the host if possible, e.g., moving CPU Tensors with
        pinned memory to CUDA devices.
    
        See below for examples.
    
        .. note::
            This method modifies the module in-place.
    
        Args:
            device (:class:`torch.device`): the desired device of the parameters
                and buffers in this module
            dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
                the parameters and buffers in this module
            tensor (torch.Tensor): Tensor whose dtype and device are the desired
                dtype and device for all parameters and buffers in this module
            memory_format (:class:`torch.memory_format`): the desired memory
                format for 4D parameters and buffers in this module (keyword
                only argument)
    
        Returns:
            Module: self
    
        Examples::
    
            >>> # xdoctest: +IGNORE_WANT("non-deterministic")
            >>> linear = nn.Linear(2, 2)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]])
            >>> linear.to(torch.double)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]], dtype=torch.float64)
            >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)
            >>> gpu1 = torch.device("cuda:1")
            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
            >>> cpu = torch.device("cpu")
            >>> linear.to(cpu)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16)
    
            >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.3741+0.j,  0.2382+0.j],
                    [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
            >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
            tensor([[0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j],
                    [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
    
        """
>       device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
            *args, **kwargs
        )
E       TypeError: to() received an invalid combination of arguments - got (method), but expected one of:
E        * (torch.device device = None, torch.dtype dtype = None, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (torch.dtype dtype, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)
E        * (Tensor tensor, bool non_blocking = False, bool copy = False, *, torch.memory_format memory_format = None)

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1299: TypeError
=================================== FAILURES ===================================
________________ TestConsciousnessModel.test_model_forward_pass ________________

self = <test_consciousness.TestConsciousnessModel object at 0x7b4a4ae026b0>
model = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise...MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
)
sample_input = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def test_model_forward_pass(self, model, sample_input, deterministic):
        """Test forward pass through consciousness model."""
        # Initialize model
        input_shape = (model.hidden_dim,)
        model.eval() if deterministic else model.train()
    
        # Run forward pass
        with torch.no_grad() if deterministic else torch.enable_grad():
>           new_state, metrics = model(sample_input, deterministic=deterministic)

tests/test_consciousness.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/consciousness_model.py:97: in forward
    workspace_output, attention_weights = self.global_workspace(
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GlobalWorkspace(
  (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (layer_norm2): LayerNorm((12...place=False)
    (3): Linear(in_features=512, out_features=128, bias=True)
    (4): Dropout(p=0.1, inplace=False)
  )
)
args = (tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.4364,  ...,  0....,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]]),)
kwargs = {'deterministic': True}

    def _call_impl(self, *args, **kwargs):
        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
        # If we don't have any hooks, we want to skip the rest of the logic in
        # this function, and just call forward.
        if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
                or _global_backward_pre_hooks or _global_backward_hooks
                or _global_forward_hooks or _global_forward_pre_hooks):
>           return forward_call(*args, **kwargs)
E           TypeError: GlobalWorkspace.forward() got an unexpected keyword argument 'deterministic'

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: TypeError
________________ TestConsciousnessModel.test_model_state_update ________________

self = <test_consciousness.TestConsciousnessModel object at 0x7b4a4ae02d70>
model = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise...MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
)
sample_input = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def test_model_state_update(self, model, sample_input, deterministic):
        """Test updating the model state."""
        input_shape = (model.hidden_dim,)
        model.eval() if deterministic else model.train()
        with torch.no_grad() if deterministic else torch.enable_grad():
            state = torch.zeros(sample_input['attention'].shape[0], model.hidden_dim)
>           new_state, metrics = model(sample_input, state=state, deterministic=deterministic)

tests/test_consciousness.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/consciousness_model.py:97: in forward
    workspace_output, attention_weights = self.global_workspace(
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GlobalWorkspace(
  (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (layer_norm2): LayerNorm((12...place=False)
    (3): Linear(in_features=512, out_features=128, bias=True)
    (4): Dropout(p=0.1, inplace=False)
  )
)
args = (tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.4364,  ...,  0....,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]]),)
kwargs = {'deterministic': True}

    def _call_impl(self, *args, **kwargs):
        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
        # If we don't have any hooks, we want to skip the rest of the logic in
        # this function, and just call forward.
        if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
                or _global_backward_pre_hooks or _global_backward_hooks
                or _global_forward_hooks or _global_forward_pre_hooks):
>           return forward_call(*args, **kwargs)
E           TypeError: GlobalWorkspace.forward() got an unexpected keyword argument 'deterministic'

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: TypeError
_____________ TestConsciousnessModel.test_model_attention_weights ______________

self = <test_consciousness.TestConsciousnessModel object at 0x7b4a4ae02fe0>
model = ConsciousnessModel(
  (global_workspace): GlobalWorkspace(
    (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise...MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
  )
)
sample_input = {'attention': tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.43...6,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]])}
deterministic = True

    def test_model_attention_weights(self, model, sample_input, deterministic):
        """Test attention weights in the model."""
        input_shape = (model.hidden_dim,)
        model.eval() if deterministic else model.train()
        with torch.no_grad() if deterministic else torch.enable_grad():
            state = torch.zeros(sample_input['attention'].shape[0], model.hidden_dim)
>           _, metrics = model(sample_input, state=state, deterministic=deterministic)

tests/test_consciousness.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/consciousness_model.py:97: in forward
    workspace_output, attention_weights = self.global_workspace(
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GlobalWorkspace(
  (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  (layer_norm2): LayerNorm((12...place=False)
    (3): Linear(in_features=512, out_features=128, bias=True)
    (4): Dropout(p=0.1, inplace=False)
  )
)
args = (tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  0.3399,  0.7200,  0.4114],
         [ 1.9312,  1.0119, -1.4364,  ...,  0....,  0.2921,  ...,  0.6600, -0.1619,  0.8861],
         [ 0.5484,  0.4577,  0.9677,  ..., -0.0334, -0.8276, -0.3524]]]),)
kwargs = {'deterministic': True}

    def _call_impl(self, *args, **kwargs):
        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
        # If we don't have any hooks, we want to skip the rest of the logic in
        # this function, and just call forward.
        if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
                or _global_backward_pre_hooks or _global_backward_hooks
                or _global_forward_hooks or _global_forward_pre_hooks):
>           return forward_call(*args, **kwargs)
E           TypeError: GlobalWorkspace.forward() got an unexpected keyword argument 'deterministic'

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: TypeError
______________________ EnvironmentTests.test_core_imports ______________________

self = <test_environment.EnvironmentTests testMethod=test_core_imports>

    def test_core_imports(self):
        """Test all core framework imports"""
        try:
            import torch
>           import torchvision

tests/test_environment.py:19: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164: in <module>
    def meta_nms(dets, scores, iou_threshold):
../../.local/lib/python3.10/site-packages/torch/library.py:795: in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
../../.local/lib/python3.10/site-packages/torch/library.py:184: in _register_fake
    handle = entry.fake_impl.register(func_to_register, source)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <torch._library.fake_impl.FakeImplHolder object at 0x7b4a4acd1de0>
func = <function meta_nms at 0x7b4a4adcc550>
source = '/home/kasinadhsarma/.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:164'

    def register(self, func: Callable, source: str) -> RegistrationHandle:
        """Register an fake impl.
    
        Returns a RegistrationHandle that one can use to de-register this
        fake impl.
        """
        if self.kernel is not None:
            raise RuntimeError(
                f"register_fake(...): the operator {self.qualname} "
                f"already has an fake impl registered at "
                f"{self.kernel.source}."
            )
>       if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
E       RuntimeError: operator torchvision::nms does not exist

../../.local/lib/python3.10/site-packages/torch/_library/fake_impl.py:31: RuntimeError
___________________ EnvironmentTests.test_framework_versions ___________________

self = <test_environment.EnvironmentTests testMethod=test_framework_versions>

    def test_framework_versions(self):
        """Verify framework versions"""
        import torch
>       import torchvision

tests/test_environment.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torchvision/__init__.py:10: in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:26: in <module>
    def meta_roi_align(input, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, aligned):
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function meta_roi_align at 0x7b4a4adce8c0>

    def wrapper(fn):
>       if torchvision.extension._has_ops():
E       AttributeError: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)

../../.local/lib/python3.10/site-packages/torchvision/_meta_registrations.py:18: AttributeError
___________________ EnvironmentTests.test_hardware_detection ___________________

self = <test_environment.EnvironmentTests testMethod=test_hardware_detection>

    def test_hardware_detection(self):
        """Test hardware detection and configuration"""
        import torch
    
        # Check if CUDA is available
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            print(f"CUDA devices: {torch.cuda.device_count()} available")
        else:
            print("CUDA is not available")
>       self.assertTrue(cuda_available, "CUDA is not available")
E       AssertionError: False is not true : CUDA is not available

tests/test_environment.py:38: AssertionError
----------------------------- Captured stdout call -----------------------------
CUDA is not available
_______________ TestAttentionMechanisms.test_scaled_dot_product ________________

self = <test_attention.TestAttentionMechanisms object at 0x7b4a4ae20a90>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_scaled_dot_product(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test scaled dot-product attention computation."""
        # Create inputs
>       inputs_q = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim)

tests/unit/attention/test_attention.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_attention.TestAttentionMechanisms object at 0x7b4a4ae20a90>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
_________________ TestAttentionMechanisms.test_attention_mask __________________

self = <test_attention.TestAttentionMechanisms object at 0x7b4a4ae20ca0>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_attention_mask(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test attention mask handling."""
        # Create inputs and mask
>       inputs_q = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim)

tests/unit/attention/test_attention.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_attention.TestAttentionMechanisms object at 0x7b4a4ae20ca0>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
___________ TestAttentionMechanisms.test_consciousness_broadcasting ____________

self = <test_attention.TestAttentionMechanisms object at 0x7b4a4ae20eb0>
attention_module = ConsciousnessAttention(
  (query): Linear(in_features=128, out_features=128, bias=True)
  (key): Linear(in_features=12...res=128, bias=True)
  (attn_dropout): Dropout(p=0.1, inplace=False)
  (output_dropout): Dropout(p=0.1, inplace=False)
)
batch_size = 2, seq_length = 8, hidden_dim = 128

    def test_consciousness_broadcasting(self, attention_module, batch_size, seq_length, hidden_dim):
        """Test consciousness-aware broadcasting."""
>       inputs_q = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim)

tests/unit/attention/test_attention.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_attention.TestAttentionMechanisms object at 0x7b4a4ae20eb0>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
__________ TestAttentionMechanisms.test_global_workspace_integration ___________

self = <test_attention.TestAttentionMechanisms object at 0x7b4a4ae21090>
batch_size = 2, seq_length = 8, hidden_dim = 128, num_heads = 4

    def test_global_workspace_integration(self, batch_size, seq_length, hidden_dim, num_heads):
        """Test global workspace integration."""
        workspace = GlobalWorkspace(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            head_dim=hidden_dim // num_heads,
            dropout_rate=0.1
        )
    
>       inputs = self.create_inputs(self.seed, batch_size, seq_length, hidden_dim)

tests/unit/attention/test_attention.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/unit/test_base.py:42: in create_inputs
    torch.manual_seed(seed)  # Use seed value directly
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

seed = <bound method ConsciousnessTestBase.seed of <test_attention.TestAttentionMechanisms object at 0x7b4a4ae21090>>

    def manual_seed(seed) -> torch._C.Generator:
        r"""Sets the seed for generating random numbers on all devices. Returns a
        `torch.Generator` object.
    
        Args:
            seed (int): The desired seed. Value must be within the inclusive range
                `[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError
                is raised. Negative inputs are remapped to positive values with the formula
                `0xffff_ffff_ffff_ffff + seed`.
        """
>       seed = int(seed)
E       TypeError: int() argument must be a string, a bytes-like object or a real number, not 'method'

../../.local/lib/python3.10/site-packages/torch/random.py:42: TypeError
__________ TestCognitiveProcessIntegration.test_cross_modal_attention __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7b4a4ae22c20>
device = device(type='cpu')
integration_module = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_cross_modal_attention(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Create multi-modal inputs
        inputs = {
            'visual': torch.randn(batch_size, seq_length, input_dim, device=device),
            'textual': torch.randn(batch_size, seq_length, input_dim, device=device),
            'numerical': torch.randn(batch_size, seq_length, input_dim, device=device)
        }
    
        # Initialize parameters
        input_shape = (64,)
        integration_module.eval()
        with torch.no_grad():
>           consciousness_state, attention_maps = integration_module(inputs, deterministic=True)

tests/unit/integration/test_cognitive_integration.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'numerical': tensor([[[-4.8143e-01, -3.6127e-01,  5.0406e-01,  2.2154e+00,  1.8368e+00,
           6.1404e-01,  7.033...+00,
           3.8571e-01,  9.0806e-01,  1.0072e+00,  2.0605e-01, -4.5488e-01,
          -8.7693e-01,  8.4897e-02]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
______ TestCognitiveProcessIntegration.test_modality_specific_processing _______

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7b4a4ae22050>
device = device(type='cpu')
integration_module = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_modality_specific_processing(self, device, integration_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Test with single modality
        single_input = {
            'visual': torch.randn(batch_size, seq_length, input_dim, device=device)
        }
        input_shape = (64,)
        integration_module.eval()
        with torch.no_grad():
>           consciousness_state1, _ = integration_module(single_input, deterministic=True)

tests/unit/integration/test_cognitive_integration.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'visual': tensor([[[ 5.7287e-01, -3.7417e-01,  1.5273e+00,  6.9645e-02,  1.1185e+00,
           5.1637e-01,  3.4546e-...-01,
           6.3792e-01, -1.2331e+00, -4.0110e-01,  3.9479e-01, -9.2258e-01,
           9.3135e-02,  4.9786e-01]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
__________ TestCognitiveProcessIntegration.test_integration_stability __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7b4a4ae21a20>
device = device(type='cpu')
integration_module = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_integration_stability(self, device, integration_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        inputs = {
            'modality1': torch.randn(batch_size, seq_length, input_dim, device=device),
            'modality2': torch.randn(batch_size, seq_length, input_dim, device=device)
        }
    
        integration_module.eval()
        states = []
        with torch.no_grad():
            for _ in range(5):
>               state, _ = integration_module(inputs, deterministic=True)

tests/unit/integration/test_cognitive_integration.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'modality1': tensor([[[ 7.8656e-01,  4.3046e-01,  1.3711e-01, -1.3496e-01,  5.7280e-03,
           8.0930e-01, -5.396...        1.0359,  1.2281, -1.1428,  2.0762, -1.8058, -0.7151, -1.1927,
          -0.4456,  0.0848, -0.4560,  0.4052]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
__________ TestCognitiveProcessIntegration.test_cognitive_integration __________

self = <test_cognitive_integration.TestCognitiveProcessIntegration object at 0x7b4a4ae20ac0>
device = device(type='cpu')
integration_module = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_cognitive_integration(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        # Create multi-modal inputs
        inputs = {
            'visual': torch.randn(batch_size, seq_length, input_dim, device=device),
            'textual': torch.randn(batch_size, seq_length, input_dim, device=device),
            'numerical': torch.randn(batch_size, seq_length, input_dim, device=device)
        }
    
        # Initialize parameters
        input_shape = (64,)
        integration_module.eval()
        with torch.no_grad():
>           consciousness_state, attention_maps = integration_module(inputs, deterministic=True)

tests/unit/integration/test_cognitive_integration.py:137: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = CognitiveProcessIntegration(
  (attention): MultiheadAttention(
    (out_proj): NonDynamicallyQuantizableLinear(in_fea...)
  )
  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  (dropout): Dropout(p=0.1, inplace=False)
)
inputs = {'numerical': tensor([[[-2.0359e-01, -6.8227e-01,  1.0594e-01,  2.0380e-01,  3.2327e-01,
           9.9448e-01, -1.156...-01,
          -8.3444e-01,  1.1105e+00,  1.0162e+00,  1.4355e+00,  1.1088e+00,
           1.6581e-01,  9.9820e-01]]])}
deterministic = True

    def forward(self, inputs: Dict[str, torch.Tensor], deterministic: bool = True):
        processed_modalities = {}
        for modality, x in inputs.items():
>           x = nn.LayerNorm()(x)
E           TypeError: LayerNorm.__init__() missing 1 required positional argument: 'normalized_shape'

models/consciousness_state.py:31: TypeError
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_state_management.TestConsciousnessStateManager object at 0x7b4a4ae239a0>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_adaptive_gating(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            # Test adaptation to different input patterns
            # Case 1: Similar input to current state
            similar_input = state + torch.randn_like(state) * 0.1
            _, metrics1 = state_manager(state, similar_input, threshold=0.5, deterministic=True)
    
            # Case 2: Very different input
            different_input = torch.randn(batch_size, hidden_dim, device=device)
            _, metrics2 = state_manager(state, different_input, threshold=0.5, deterministic=True)
    
        # Memory gate should be more open (lower values) for different inputs
>       assert torch.mean(metrics1['memory_gate']) > torch.mean(metrics2['memory_gate'])
E       assert tensor(0.4373) > tensor(0.5014)
E        +  where tensor(0.4373) = <built-in method mean of type object at 0x7b4a828678c0>(tensor([[0.4466],\n        [0.4281]]))
E        +    where <built-in method mean of type object at 0x7b4a828678c0> = torch.mean
E        +  and   tensor(0.5014) = <built-in method mean of type object at 0x7b4a828678c0>(tensor([[0.4656],\n        [0.5372]]))
E        +    where <built-in method mean of type object at 0x7b4a828678c0> = torch.mean

tests/unit/integration/test_state_management.py:97: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.4466],
        [0.4281]])
state: tensor([[-0.0046, -0.8307, -0.4420,  1.6286, -1.3511, -1.0486,  0.3171, -0.0421,
         -0.0460, -0.4760,  0.6386,  0.2148,  0.4966, -0.7065, -0.5173,  2.5037,
         -0.4005, -0.2967, -1.7716, -0.4197, -0.1839,  1.5103,  0.5361,  1.1292,
         -0.6580, -2.1182, -0.6929,  0.2603,  0.3279,  0.1000, -0.0166,  1.0406,
         -0.8146,  0.1125, -0.1910, -0.8793,  0.3679,  1.3233,  1.7359,  0.2043,
         -0.0387, -1.6342,  0.1319, -1.4819, -0.5558,  0.1816,  2.0678, -0.8482,
         -0.1171, -0.6863, -1.0614, -0.0904, -0.0454, -0.9330, -0.6501, -0.5303,
          0.8793,  0.9402, -1.0973, -0.4582,  0.7953,  0.7276,  0.9952, -0.9362],
        [ 1.9020,  0.5882,  2.0966, -0.4684,  0.1020, -0.0260, -0.0510, -1.0481,
          0.2548,  1.2712,  0.5860, -1.4407,  0.5132, -0.8692,  1.2142,  0.3937,
          0.1454,  1.7867,  1.1014,  0.0790,  0.0630, -0.1338,  0.0255,  1.3037,
          0.6399,  1.6430, -0.4002,  1.8989,  0.4411, -0.7200,  1.1549, -0.4476,
         -1.1762,  1.2539, -1.3005, -0.6501, -0.7786, -1.4240, -1.5052, -0.1017,
          0.4205,  2.1241,  0.2676,  0.4274,  0.0886, -0.5294,  0.4932, -1.7230,
         -0.7828,  1.8020, -0.1850,  0.8029, -0.6560, -0.1796,  1.6317, -0.0920,
          0.4898,  1.4418,  1.3841,  0.1555,  0.3335, -1.4094, -1.3076,  0.2628]])
inputs: tensor([[ 0.0139, -0.7467, -0.5182,  1.6578, -1.2441, -1.0564,  0.2663, -0.0940,
         -0.3106, -0.4683,  0.4373,  0.3795,  0.4307, -0.7133, -0.3787,  2.4524,
         -0.4949, -0.3160, -1.9112, -0.3239, -0.1711,  1.5353,  0.3608,  1.2323,
         -0.4935, -2.2462, -0.6495,  0.3371,  0.4450, -0.0171, -0.0572,  1.0275,
         -0.7988,  0.1636, -0.1626, -0.8686,  0.4098,  1.3565,  1.8892,  0.1901,
          0.0215, -1.6184,  0.1152, -1.4944, -0.4439,  0.2921,  1.8608, -0.9706,
          0.0500, -0.6840, -1.1719, -0.1524,  0.0084, -0.8521, -0.7759, -0.6608,
          0.7753,  0.9210, -1.2827, -0.4154,  0.9710,  0.9263,  0.9170, -0.9813],
        [ 1.8814,  0.5537,  2.0190, -0.5048,  0.2724, -0.0439, -0.0432, -0.9826,
          0.2580,  1.2394,  0.4776, -1.3289,  0.4926, -0.7032,  1.1680,  0.5689,
          0.1753,  1.8319,  1.0598, -0.0901,  0.1007, -0.1608,  0.0676,  1.3086,
          0.7153,  1.8526, -0.4195,  1.7745,  0.3901, -0.6032,  1.3100, -0.5422,
         -1.0610,  1.3252, -1.2068, -0.5200, -0.7132, -1.3482, -1.4328, -0.0473,
          0.3687,  2.2255,  0.4362,  0.4455,  0.0848, -0.8619,  0.4211, -1.8364,
         -0.7558,  1.9049, -0.1052,  0.7378, -0.5592, -0.0964,  1.8618, -0.0510,
          0.5490,  1.4784,  1.2788,  0.2961,  0.2810, -1.4237, -1.3234,  0.3155]])
candidate_state: tensor([[ 0.4029,  0.1944, -0.1377, -0.1698, -0.1473,  0.0557, -0.1644, -0.1049,
          0.1811,  0.5301, -0.1294, -0.0422,  0.7996,  0.8061,  0.0359, -0.0072,
         -0.1240, -0.1622, -0.0743, -0.0750, -0.1217,  0.0493, -0.1496, -0.1142,
          0.2742, -0.0903, -0.0284, -0.1534, -0.0756, -0.1655,  0.1130,  0.2478,
         -0.1659, -0.1695,  0.2597,  0.8941, -0.1677,  0.0122, -0.1672, -0.1664,
         -0.0058,  0.0050,  0.2988, -0.0950,  0.6576,  0.6607,  0.5480,  0.3884,
          0.7303,  0.5469,  0.7923,  1.3071, -0.0998, -0.1695,  0.6111, -0.1486,
         -0.1285, -0.1633,  0.3133,  0.2730,  0.1365,  0.0429, -0.0059, -0.0343],
        [-0.1050,  0.0825,  0.4715,  0.7253, -0.0660,  0.1193,  0.6707, -0.1558,
          0.1305,  0.5340,  0.0816,  0.4146,  0.3206, -0.1579, -0.1549,  0.1159,
          0.0847, -0.0107,  0.3722,  0.2381,  0.0138, -0.1494, -0.1673,  0.0582,
          0.3145,  0.3544,  0.0816,  1.2668, -0.1253, -0.1044,  0.3541,  0.5060,
          0.2151, -0.1698, -0.0833, -0.0954,  0.4040,  0.4767, -0.0359, -0.0848,
          0.2362,  0.4116,  0.0804, -0.1592, -0.1618, -0.1689, -0.1497, -0.0789,
         -0.1695,  0.3132, -0.1693, -0.0931, -0.1022,  0.5413, -0.0628, -0.1632,
          0.4291,  0.4188, -0.1036,  0.6248,  0.6972, -0.1600,  0.2417, -0.0601]])
new_state: tensor([[ 2.2088e-01, -2.6344e-01, -2.7361e-01,  6.3346e-01, -6.8493e-01,
         -4.3752e-01,  5.0644e-02, -7.6894e-02,  7.9649e-02,  8.0745e-02,
          2.1360e-01,  7.2594e-02,  6.6430e-01,  1.3051e-01, -2.1116e-01,
          1.1143e+00, -2.4749e-01, -2.2229e-01, -8.3240e-01, -2.2894e-01,
         -1.4946e-01,  7.0186e-01,  1.5664e-01,  4.4111e-01, -1.4215e-01,
         -9.9603e-01, -3.2518e-01,  3.1338e-02,  1.0458e-01, -4.6885e-02,
          5.5091e-02,  6.0188e-01, -4.5560e-01, -4.3560e-02,  5.8382e-02,
          1.0207e-01,  7.1503e-02,  5.9778e-01,  6.8276e-01, -8.2999e-04,
         -2.0488e-02, -7.2712e-01,  2.2427e-01, -7.1445e-01,  1.1569e-01,
          4.4674e-01,  1.2268e+00, -1.6388e-01,  3.5181e-01, -3.8735e-03,
         -3.5565e-02,  6.8289e-01, -7.5530e-02, -5.1046e-01,  4.7787e-02,
         -3.1909e-01,  3.2161e-01,  3.2955e-01, -3.1670e-01, -5.3529e-02,
          4.3075e-01,  3.4870e-01,  4.4121e-01, -4.3714e-01],
        [ 7.5414e-01,  2.9897e-01,  1.1671e+00,  2.1437e-01,  5.8854e-03,
          5.7126e-02,  3.6177e-01, -5.3777e-01,  1.8374e-01,  8.4955e-01,
          2.9753e-01, -3.7957e-01,  4.0305e-01, -4.6235e-01,  4.3115e-01,
          2.3479e-01,  1.1070e-01,  7.5865e-01,  6.8434e-01,  1.7002e-01,
          3.4882e-02, -1.4273e-01, -8.4757e-02,  5.9134e-01,  4.5377e-01,
          9.0598e-01, -1.2465e-01,  1.5374e+00,  1.1713e-01, -3.6790e-01,
          6.9688e-01,  9.7807e-02, -3.8044e-01,  4.3961e-01, -6.0436e-01,
         -3.3282e-01, -1.0222e-01, -3.3693e-01, -6.6484e-01, -9.2022e-02,
          3.1509e-01,  1.1447e+00,  1.6055e-01,  9.1919e-02, -5.4617e-02,
         -3.2324e-01,  1.2549e-01, -7.8267e-01, -4.3199e-01,  9.5051e-01,
         -1.7601e-01,  2.9043e-01, -3.3926e-01,  2.3273e-01,  6.6255e-01,
         -1.3272e-01,  4.5508e-01,  8.5667e-01,  5.3319e-01,  4.2392e-01,
          5.4153e-01, -6.9484e-01, -4.2149e-01,  7.8142e-02]])
memory_gate: tensor([[0.4656],
        [0.5372]])
state: tensor([[-0.0046, -0.8307, -0.4420,  1.6286, -1.3511, -1.0486,  0.3171, -0.0421,
         -0.0460, -0.4760,  0.6386,  0.2148,  0.4966, -0.7065, -0.5173,  2.5037,
         -0.4005, -0.2967, -1.7716, -0.4197, -0.1839,  1.5103,  0.5361,  1.1292,
         -0.6580, -2.1182, -0.6929,  0.2603,  0.3279,  0.1000, -0.0166,  1.0406,
         -0.8146,  0.1125, -0.1910, -0.8793,  0.3679,  1.3233,  1.7359,  0.2043,
         -0.0387, -1.6342,  0.1319, -1.4819, -0.5558,  0.1816,  2.0678, -0.8482,
         -0.1171, -0.6863, -1.0614, -0.0904, -0.0454, -0.9330, -0.6501, -0.5303,
          0.8793,  0.9402, -1.0973, -0.4582,  0.7953,  0.7276,  0.9952, -0.9362],
        [ 1.9020,  0.5882,  2.0966, -0.4684,  0.1020, -0.0260, -0.0510, -1.0481,
          0.2548,  1.2712,  0.5860, -1.4407,  0.5132, -0.8692,  1.2142,  0.3937,
          0.1454,  1.7867,  1.1014,  0.0790,  0.0630, -0.1338,  0.0255,  1.3037,
          0.6399,  1.6430, -0.4002,  1.8989,  0.4411, -0.7200,  1.1549, -0.4476,
         -1.1762,  1.2539, -1.3005, -0.6501, -0.7786, -1.4240, -1.5052, -0.1017,
          0.4205,  2.1241,  0.2676,  0.4274,  0.0886, -0.5294,  0.4932, -1.7230,
         -0.7828,  1.8020, -0.1850,  0.8029, -0.6560, -0.1796,  1.6317, -0.0920,
          0.4898,  1.4418,  1.3841,  0.1555,  0.3335, -1.4094, -1.3076,  0.2628]])
inputs: tensor([[ 0.8101,  1.1118,  0.0929,  2.5097,  1.2032, -0.9313,  0.0712, -1.6273,
         -0.4703, -0.4227, -0.6994,  0.3104, -1.0098,  1.1185,  0.0889, -0.0982,
         -0.4451, -0.0466, -0.5547,  0.4480,  0.1768,  0.0271, -1.1602,  1.3825,
         -0.3119, -0.9072, -1.9088, -0.6600,  0.8917,  1.1360, -1.3226, -0.8691,
          0.3350, -0.8679, -0.2191, -1.0433, -0.7759, -0.7498,  0.5988, -1.4551,
         -0.9603, -0.4080, -0.7319, -0.4247, -0.7292,  1.8667,  1.3041, -0.6049,
          0.1289,  1.0635,  1.5887, -0.9616, -0.9265,  1.5034, -0.2190, -0.5261,
          0.3080,  1.3573, -0.0911,  1.2504,  0.8178, -0.2000,  0.2959,  0.6777],
        [-0.6291, -0.8177, -0.7813, -0.5240, -0.0590,  1.0499,  0.0613, -1.3480,
         -0.5692,  0.5707, -0.0639,  0.5769, -0.2687, -0.0635,  0.6327, -0.0065,
         -0.2599, -0.5758,  1.1915,  1.6432,  0.7809, -1.0926, -0.4473,  0.0910,
          0.3665, -0.3892,  0.4264,  0.7535,  0.1262,  1.5046, -1.0795,  0.8766,
          1.4403, -2.0051, -0.0719, -1.2880,  0.2854,  0.8868, -0.2229,  0.6119,
          0.2970,  0.4199,  0.7268, -0.8892,  0.5516,  0.8800, -0.1767, -1.8151,
         -0.0997,  1.1206,  1.1284, -0.4092,  1.0431, -1.1110, -0.7656,  1.4586,
          0.0048, -0.3542,  0.1132,  0.7832,  0.7211, -2.3475,  0.8166, -1.2317]])
candidate_state: tensor([[-0.1441,  0.6503,  0.0176,  0.2199, -0.1122,  0.5478, -0.0864, -0.0790,
          0.0020,  0.5445,  0.6659, -0.1679, -0.1664, -0.1559, -0.1651, -0.1557,
         -0.1623, -0.0499,  0.0273, -0.1601, -0.1382,  0.0346, -0.1329, -0.0052,
          0.1798, -0.1592,  0.3018,  0.7785, -0.1291,  0.1082, -0.1077, -0.0817,
          0.4434,  0.2059, -0.0484,  0.2122,  0.2728, -0.1398,  0.3917, -0.0339,
         -0.1499,  0.4616, -0.1691,  0.2522, -0.1699, -0.1657, -0.0417,  0.5549,
          0.4730,  0.1361,  0.0137,  0.4743,  0.6555,  0.1271,  0.2098,  0.7653,
          1.2670,  0.8013,  0.7114, -0.1659,  0.1712,  0.0719, -0.1655,  0.2484],
        [-0.1693, -0.1138, -0.1182,  0.1259,  0.2794, -0.1628,  1.3205, -0.1400,
          0.7927,  0.0147, -0.0595, -0.1302,  0.0993,  0.5638, -0.0696, -0.0146,
          0.0890, -0.1120,  0.3665,  0.1124,  0.4054, -0.0250, -0.1576, -0.1028,
          0.1854,  1.3183, -0.0550,  0.0764,  0.1569, -0.0420,  0.0508,  0.4474,
         -0.1386,  0.1647,  0.1571, -0.1515, -0.1598,  0.1967,  0.2148, -0.1575,
         -0.1698,  0.0231,  0.0657,  0.2989, -0.1578, -0.1630,  0.5265,  0.3505,
          0.2492, -0.0303,  0.1271, -0.0772,  0.1341,  0.1985, -0.1474,  0.2495,
         -0.1149,  0.1982,  0.2584,  0.4105, -0.1234,  0.1227, -0.0407,  0.2203]])
new_state: tensor([[-0.0791, -0.0393, -0.1964,  0.8758, -0.6890, -0.1955,  0.1015, -0.0618,
         -0.0204,  0.0694,  0.6532,  0.0103,  0.1423, -0.4123, -0.3291,  1.0825,
         -0.2732, -0.1648, -0.8103, -0.2810, -0.1595,  0.7217,  0.1785,  0.5230,
         -0.2103, -1.0713, -0.1614,  0.5372,  0.0837,  0.1044, -0.0653,  0.4408,
         -0.1423,  0.1624, -0.1148, -0.2960,  0.3171,  0.5414,  1.0176,  0.0770,
         -0.0981, -0.5142, -0.0289, -0.5552, -0.3496, -0.0040,  0.9405, -0.0984,
          0.1982, -0.2468, -0.4868,  0.2114,  0.3292, -0.3665, -0.1906,  0.1621,
          1.0865,  0.8660, -0.1307, -0.3020,  0.4618,  0.3772,  0.3749, -0.3032],
        [ 0.9435,  0.2633,  1.0717, -0.1934,  0.1841, -0.0893,  0.5837, -0.6279,
          0.5037,  0.6898,  0.2873, -0.8343,  0.3216, -0.2061,  0.6201,  0.2047,
          0.1193,  0.9080,  0.7613,  0.0945,  0.2215, -0.0834, -0.0592,  0.6529,
          0.4296,  1.4927, -0.2405,  1.0555,  0.3096, -0.4062,  0.6439, -0.0334,
         -0.6961,  0.7499, -0.6260, -0.4194, -0.4923, -0.6740, -0.7093, -0.1275,
          0.1474,  1.1519,  0.1742,  0.3680, -0.0254, -0.3598,  0.5086, -0.7635,
         -0.3052,  0.9541, -0.0406,  0.3956, -0.2904, -0.0046,  0.8084,  0.0660,
          0.2100,  0.8663,  0.8632,  0.2735,  0.1221, -0.7005, -0.7213,  0.2432]])
____________ TestInformationIntegration.test_phi_metric_computation ____________

self = <test_integration.TestInformationIntegration object at 0x7b4a4aeac7c0>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_phi_metric_computation(self, device, integration_module):
        # Test dimensions
        batch_size = 2
        num_modules = 4
        input_dim = 32  # Updated to match expected shapes
    
        # Create sample inputs
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Initialize parameters
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[-4.4224e-01, -2.0076e+00,  7.0499e-02,  3.7851e-01, -2.7778e-01,
          -4.6669e-01, -1.0876e+00, -2.9068...e+00,
           1.0701e-01, -1.8491e-01, -2.2077e-01,  1.1823e+00, -7.0056e-01,
          -2.6341e-02,  1.1052e+00]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_______________ TestInformationIntegration.test_information_flow _______________

self = <test_integration.TestInformationIntegration object at 0x7b4a4aeac970>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_information_flow(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32  # Updated to match expected shapes
    
        inputs = torch.zeros(batch_size, num_modules, input_dim, device=device)  # ensure shape matches the model
    
        # Test with and without dropout
        integration_module.train()
>       output1, _ = integration_module(inputs, deterministic=False)

tests/unit/memory/test_integration.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0....., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
          0., 0., 0., 0., 0., 0., 0., 0., 0.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
_____________ TestInformationIntegration.test_entropy_calculations _____________

self = <test_integration.TestInformationIntegration object at 0x7b4a4aeacb20>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_entropy_calculations(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32  # Updated to match expected shapes
    
        # Test with different input distributions
        # Uniform distribution
        uniform_input = torch.ones(batch_size, num_modules, input_dim, device=device)
        integration_module.eval()
        with torch.no_grad():
>           _, phi_uniform = integration_module(uniform_input)

tests/unit/memory/test_integration.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1....., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
______________ TestInformationIntegration.test_memory_integration ______________

self = <test_integration.TestInformationIntegration object at 0x7b4a4aeaccd0>
device = device(type='cpu')
integration_module = InformationIntegration(
  (input_projection): Linear(in_features=64, out_features=64, bias=True)
  (layer_norm): Layer...namicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_memory_integration(self, device, integration_module):
        batch_size = 2
        num_modules = 4
        input_dim = 32  # Updated to match expected shapes
    
        inputs = torch.randn(batch_size, num_modules, input_dim, device=device)
    
        # Process through integration
        integration_module.eval()
        with torch.no_grad():
>           output, phi = integration_module(inputs)

tests/unit/memory/test_integration.py:119: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:112: in forward
    inputs = self.input_projection(inputs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Linear(in_features=64, out_features=64, bias=True)
input = tensor([[[ 1.1594, -0.4872, -0.8200,  1.6364,  0.8068, -1.3143, -1.2821,
          -1.1104, -0.4075,  0.5045, -0.0069,...        -0.3542, -1.1802, -1.2843,  0.3597,  0.6060,  0.7145, -0.6332,
           0.8154, -1.0031,  0.0118, -0.5938]]])

    def forward(self, input: Tensor) -> Tensor:
>       return F.linear(input, self.weight, self.bias)
E       RuntimeError: mat1 and mat2 shapes cannot be multiplied (8x32 and 64x64)

../../.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125: RuntimeError
______________________ TestGRUCell.test_gru_state_updates ______________________

self = <test_memory_components.TestGRUCell object at 0x7b4a4aeaead0>
gru_cell = GRUCell(
  (update): Linear(in_features=128, out_features=64, bias=True)
  (reset): Linear(in_features=128, out_features=64, bias=True)
  (candidate): Linear(in_features=128, out_features=64, bias=True)
)

    def test_gru_state_updates(self, gru_cell):
        # Test dimensions
        batch_size = 2
        input_dim = 32
        hidden_dim = 64
    
        # Create sample inputs
        x = torch.randn(batch_size, input_dim)
        h = torch.randn(batch_size, hidden_dim)
    
        # Initialize parameters
>       gru_cell.reset_parameters()

tests/unit/memory/test_memory_components.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GRUCell(
  (update): Linear(in_features=128, out_features=64, bias=True)
  (reset): Linear(in_features=128, out_features=64, bias=True)
  (candidate): Linear(in_features=128, out_features=64, bias=True)
)
name = 'reset_parameters'

    def __getattr__(self, name: str) -> Any:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'GRUCell' object has no attribute 'reset_parameters'. Did you mean: 'get_parameter'?

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931: AttributeError
_______________________ TestGRUCell.test_gru_reset_gate ________________________

self = <test_memory_components.TestGRUCell object at 0x7b4a4aeaec80>
gru_cell = GRUCell(
  (update): Linear(in_features=128, out_features=64, bias=True)
  (reset): Linear(in_features=128, out_features=64, bias=True)
  (candidate): Linear(in_features=128, out_features=64, bias=True)
)

    def test_gru_reset_gate(self, gru_cell):
        batch_size = 2
        input_dim = 32
        hidden_dim = 64
    
        x = torch.randn(batch_size, input_dim)
        h = torch.randn(batch_size, hidden_dim)
    
>       gru_cell.reset_parameters()

tests/unit/memory/test_memory_components.py:53: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GRUCell(
  (update): Linear(in_features=128, out_features=64, bias=True)
  (reset): Linear(in_features=128, out_features=64, bias=True)
  (candidate): Linear(in_features=128, out_features=64, bias=True)
)
name = 'reset_parameters'

    def __getattr__(self, name: str) -> Any:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'GRUCell' object has no attribute 'reset_parameters'. Did you mean: 'get_parameter'?

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931: AttributeError
__________________ TestWorkingMemory.test_sequence_processing __________________

self = <test_memory_components.TestWorkingMemory object at 0x7b4a4aeaef80>
memory_module = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_sequence_processing(self, memory_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
        hidden_dim = 64
    
        # Create sample sequence
        inputs = torch.randn(batch_size, seq_length, input_dim)
    
        # Initialize parameters
>       memory_module.reset_parameters()

tests/unit/memory/test_memory_components.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)
name = 'reset_parameters'

    def __getattr__(self, name: str) -> Any:
        if "_parameters" in self.__dict__:
            _parameters = self.__dict__["_parameters"]
            if name in _parameters:
                return _parameters[name]
        if "_buffers" in self.__dict__:
            _buffers = self.__dict__["_buffers"]
            if name in _buffers:
                return _buffers[name]
        if "_modules" in self.__dict__:
            modules = self.__dict__["_modules"]
            if name in modules:
                return modules[name]
>       raise AttributeError(
            f"'{type(self).__name__}' object has no attribute '{name}'"
        )
E       AttributeError: 'WorkingMemory' object has no attribute 'reset_parameters'. Did you mean: 'get_parameter'?

../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1931: AttributeError
___________________ TestWorkingMemory.test_memory_retention ____________________

self = <test_memory_components.TestWorkingMemory object at 0x7b4a4aeaf130>
memory_module = WorkingMemory(
  (lstm): LSTMCell(64, 64)
  (dropout): Dropout(p=0.1, inplace=False)
)

    def test_memory_retention(self, memory_module):
        batch_size = 2
        seq_length = 8
        input_dim = 32
    
        inputs = torch.randn(batch_size, seq_length, input_dim)
    
        # Test with different initial states
        initial_state = torch.randn(batch_size, 64)
    
>       outputs1, final_state1 = memory_module(inputs, initial_state=initial_state, deterministic=True)

tests/unit/memory/test_memory_components.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
models/memory.py:83: in forward
    h, c = self.lstm(x, (h, c))
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736: in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
../../.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747: in _call_impl
    return forward_call(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LSTMCell(64, 64)
input = tensor([[ 0.8228,  0.4778, -0.2195,  1.2569,  0.2110, -1.1205,  0.0466,  1.2571,
         -0.8726, -0.0091, -0.1483, -...  1.1979,  0.3885,  0.0584,  1.6941,
          0.9218,  1.1737,  1.9621, -1.1779, -1.0148, -0.1602, -0.2617,  0.2438]])
hx = (tensor([[-0.8408,  0.3559,  0.0365,  0.6261, -0.9948,  0.4083, -0.2302, -0.6006,
         -1.4249, -0.4872, -0.4891, ...0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]))

    def forward(
        self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = None
    ) -> Tuple[Tensor, Tensor]:
        if input.dim() not in (1, 2):
            raise ValueError(
                f"LSTMCell: Expected input to be 1D or 2D, got {input.dim()}D instead"
            )
        if hx is not None:
            for idx, value in enumerate(hx):
                if value.dim() not in (1, 2):
                    raise ValueError(
                        f"LSTMCell: Expected hx[{idx}] to be 1D or 2D, got {value.dim()}D instead"
                    )
        is_batched = input.dim() == 2
        if not is_batched:
            input = input.unsqueeze(0)
    
        if hx is None:
            zeros = torch.zeros(
                input.size(0), self.hidden_size, dtype=input.dtype, device=input.device
            )
            hx = (zeros, zeros)
        else:
            hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx
    
>       ret = _VF.lstm_cell(
            input,
            hx,
            self.weight_ih,
            self.weight_hh,
            self.bias_ih,
            self.bias_hh,
        )
E       RuntimeError: input has inconsistent input_size: got 32 expected 64

../../.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1705: RuntimeError
_______________ TestConsciousnessStateManager.test_state_updates _______________

self = <test_consciousness_state_management.TestConsciousnessStateManager object at 0x7b4a4aeadf90>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_state_updates(self, device, state_manager):
        # Test dimensions
        batch_size = 2
        hidden_dim = 64
    
        # Create sample state and inputs
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        # Initialize parameters
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test output shapes
        assert new_state.shape == state.shape
        assert 'memory_gate' in metrics
        assert 'energy_cost' in metrics
        assert 'state_value' in metrics
    
        # Test memory gate properties
>       assert metrics['memory_gate'].shape == (batch_size, hidden_dim)  # Updated shape
E       assert torch.Size([2, 1]) == (2, 64)
E         
E         At index 1 diff: 1 != 64
E         Use -v to get more diff

tests/unit/state/test_consciousness_state_management.py:44: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5412],
        [0.5170]])
state: tensor([[ 0.8820, -0.7512,  0.6737, -2.2565, -1.7772, -0.1782, -0.4873, -0.1511,
         -2.2662,  1.1800,  0.9239, -0.1837, -1.5239,  0.2382,  1.2670,  0.1802,
         -2.4874, -0.6499, -0.8651, -0.9638, -0.5067,  0.1546,  0.3963, -0.7894,
         -0.1478, -0.0920, -0.9379, -0.0855, -0.2504,  1.9650, -0.2188,  0.6699,
          1.0218,  1.2517,  1.1306, -0.1146, -1.0109,  0.6345, -0.6379, -0.6539,
          2.1770,  0.3034, -0.2830,  0.3491, -0.0464,  1.1567, -0.2502,  0.4240,
         -0.4437,  0.2839,  0.0499, -1.9377, -0.3953,  0.2102,  2.4607,  0.1184,
          2.4748,  0.3154, -0.2272,  0.8170,  0.5415,  0.4056,  0.8086, -0.4328],
        [ 0.9448, -0.7397, -0.8774,  0.8758,  1.2169, -0.5378,  1.8439, -0.9371,
         -0.9611,  0.3984, -1.5090, -0.6318,  1.3794,  0.0817, -0.7661,  0.6600,
         -0.7496, -0.6153,  1.2564, -0.4554,  2.0231, -0.5384, -0.6119,  0.2901,
         -0.5602,  0.5989,  1.1150,  1.3037,  2.5314,  0.3666,  0.0834, -1.7847,
         -0.4952, -0.4795,  0.4190, -1.9200, -0.8524, -0.4395,  0.6200,  0.0522,
          0.1855, -0.6626, -0.1577,  1.2307,  2.3511,  0.0249,  0.9703, -2.7720,
          0.9532, -0.1512, -0.5343,  0.5091, -0.2342, -1.1259, -1.5034,  0.7678,
         -0.7191, -0.7906,  0.7700, -1.2683, -0.0233, -1.0126, -0.8398,  1.0070]])
inputs: tensor([[ 0.1948, -1.3239,  1.6676, -0.3421, -1.3011,  0.7207,  1.1146, -1.0774,
         -1.5337,  0.9098,  0.2896,  0.5194, -2.3901, -0.0425,  0.9515,  1.1472,
         -0.9730,  0.2578,  0.5923,  1.5675,  0.2541,  0.7129,  0.0370,  0.7166,
          0.7626,  0.5782,  0.5814, -0.5973, -1.1850,  0.1958,  1.6039, -0.6257,
         -0.3211,  0.7313, -0.7422, -0.0868,  0.6487, -0.0277, -1.0139,  1.3784,
         -0.4017, -1.1731, -0.5788,  0.3226, -0.9097,  0.4519, -0.1791,  0.4229,
         -2.2706, -0.7889, -1.8209, -2.6179, -0.4474, -0.2763, -0.9779, -0.4729,
          0.8560, -1.2766, -1.1726,  0.0630,  0.2604, -0.6412, -0.2963,  0.1510],
        [-0.5808, -0.0987, -0.9359, -1.1939,  0.9917, -0.0835, -0.2132, -0.5334,
          0.0243, -0.9013,  0.9665, -0.4049,  0.2158,  0.9369,  0.1134,  2.1238,
         -0.3237,  0.0566,  0.2899, -1.7032, -0.9657, -0.7004,  0.2255, -1.5247,
          1.7609,  0.1356,  1.5309, -0.5397,  0.8107, -0.4656, -1.1226,  0.0047,
         -1.5309, -1.0939,  0.3569, -0.5346,  0.2778,  0.5504,  0.2426,  2.5595,
          0.6370,  1.3665, -1.9968,  0.1720, -0.5796, -0.0052,  0.9699, -0.3565,
          0.4616, -0.1758,  0.8227, -0.3531,  1.0586, -0.3473,  1.3160,  2.0770,
         -0.1590,  1.0057, -0.1636,  0.5453, -0.7962, -0.8051,  0.5816, -0.3534]])
candidate_state: tensor([[-0.1553, -0.1621, -0.0797, -0.0539,  0.2687,  0.2451,  0.0051, -0.0501,
         -0.1277,  0.1359, -0.1640,  0.0685,  0.7885, -0.0224, -0.1700, -0.1061,
         -0.1680,  0.0059,  0.4551,  0.3507,  0.0274,  0.3834, -0.1424, -0.1540,
          0.4526,  0.3002,  0.5560, -0.1317,  0.3783, -0.1557, -0.1429,  0.6180,
          0.3798,  0.3071,  0.0159, -0.1700,  0.9002, -0.0872, -0.1337, -0.1370,
          0.3476,  0.4501, -0.1171, -0.1495, -0.1360,  1.8006, -0.0858,  0.9336,
          0.4015,  0.1339,  0.4185, -0.0818, -0.0100,  0.3985,  0.5455, -0.1064,
         -0.1371, -0.1163,  0.2909, -0.1539, -0.1277, -0.0871,  0.0127, -0.0367],
        [ 0.3312, -0.1583,  0.0951, -0.1055,  0.1239, -0.1615, -0.1664, -0.1275,
          1.3537,  0.0798, -0.0885,  0.1018,  0.3026, -0.1688, -0.1694, -0.1622,
          0.0789,  0.0026, -0.0543,  0.2084,  0.5630, -0.1613,  0.1892, -0.0073,
         -0.1699, -0.0704, -0.1593, -0.0923,  0.1002, -0.0640,  0.3762, -0.1675,
         -0.1283, -0.1254,  0.0297,  0.0714, -0.1592,  0.0142, -0.0952,  0.0558,
          0.1797,  0.3788, -0.1494,  0.3453, -0.1096, -0.1472, -0.1692,  0.1846,
          0.3279,  0.0324, -0.1556,  0.0965,  0.1407,  0.3393,  0.0306, -0.0868,
          0.5760, -0.1447, -0.1609,  0.3956, -0.1699, -0.1675, -0.1694, -0.1338]])
new_state: tensor([[ 0.4061, -0.4810,  0.3280, -1.2459, -0.8385,  0.0161, -0.2614, -0.1048,
         -1.2850,  0.7010,  0.4248, -0.0680, -0.4629,  0.1186,  0.6077,  0.0489,
         -1.4232, -0.3490, -0.2594, -0.3607, -0.2617,  0.2596,  0.1491, -0.4979,
          0.1277,  0.0879, -0.2525, -0.1067,  0.0381,  0.9920, -0.1840,  0.6461,
          0.7272,  0.8183,  0.6192, -0.1400, -0.1341,  0.3034, -0.4066, -0.4168,
          1.3377,  0.3707, -0.2069,  0.1203, -0.0875,  1.4521, -0.1747,  0.6578,
         -0.0559,  0.2151,  0.2190, -1.0862, -0.2185,  0.2966,  1.5820,  0.0153,
          1.2765,  0.1173,  0.0105,  0.3715,  0.2344,  0.1795,  0.4434, -0.2511],
        [ 0.6484, -0.4589, -0.4077,  0.4018,  0.6890, -0.3560,  0.8730, -0.5461,
          0.1569,  0.2445, -0.8229, -0.2775,  0.8593, -0.0393, -0.4779,  0.2629,
         -0.3494, -0.3169,  0.6234, -0.1348,  1.3179, -0.3563, -0.2250,  0.1465,
         -0.3717,  0.2757,  0.4996,  0.6295,  1.3572,  0.1586,  0.2248, -1.0036,
         -0.3180, -0.3085,  0.2310, -0.9582, -0.5176, -0.2204,  0.2746,  0.0539,
          0.1827, -0.1597, -0.1537,  0.8031,  1.1626, -0.0582,  0.4200, -1.3441,
          0.6512, -0.0625, -0.3514,  0.3098, -0.0532, -0.4182, -0.7625,  0.3550,
         -0.0936, -0.4786,  0.3204, -0.4647, -0.0941, -0.6045, -0.5160,  0.4560]])
______________ TestConsciousnessStateManager.test_rl_optimization ______________

self = <test_consciousness_state_management.TestConsciousnessStateManager object at 0x7b4a4aead9f0>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_rl_optimization(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
        inputs = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            new_state, metrics = state_manager(state, inputs, threshold=0.5, deterministic=True)
    
        # Test RL loss computation
        reward = torch.ones(batch_size, 1, device=device)  # Mock reward
        value_loss, td_error = state_manager.get_rl_loss(
            state_value=metrics['state_value'],
            reward=reward,
            next_state_value=metrics['state_value']
        )
    
        # Test loss properties
        assert torch.is_tensor(value_loss)
        assert value_loss.item() >= 0.0
>       assert td_error.shape == (batch_size, 1)  # changed to match actual output
E       assert torch.Size([2, 2, 1]) == (2, 1)
E         
E         At index 1 diff: 2 != 1
E         Left contains one more item: 1
E         Use -v to get more diff

tests/unit/state/test_consciousness_state_management.py:77: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5462],
        [0.4592]])
state: tensor([[-0.0215, -0.2080,  0.0664,  0.1954, -0.2166, -0.6670, -0.1647,  0.8755,
          1.4463, -0.2060,  0.9041,  0.5017,  1.2995,  1.4059, -0.1122,  0.4749,
          0.0219, -0.8775, -0.8715, -0.0937, -1.4074,  0.2708,  0.0475,  0.0833,
          0.2527, -0.5745,  2.0306,  0.1610, -2.1403,  1.5936,  1.5685, -1.3849,
          1.2188, -0.3131,  0.3581, -0.1170, -0.3196, -1.0305, -1.5965, -1.0688,
          0.9409, -0.2300,  0.2976,  0.6790,  0.5668,  1.1681,  0.0153, -0.2132,
         -0.4667, -1.2876, -0.0721, -0.4406,  0.4029,  0.3752,  0.2712, -2.2890,
         -2.2416, -0.6094,  1.3367,  0.3362, -0.2169, -1.6600,  0.8313, -0.7743],
        [-0.4288, -1.0654, -0.1357, -1.0145, -0.8882, -0.6013, -0.3724, -1.1474,
         -1.1120, -2.0784,  1.0906, -2.2554, -0.2268, -0.7192, -1.2673, -0.1355,
          0.2638,  0.2265, -0.3228, -0.7810, -0.1246, -0.4671, -2.2133, -0.9329,
         -0.1660,  0.0689, -0.0612, -1.4402, -0.5815,  0.1381, -0.6190,  0.0543,
          0.1941, -0.6193, -0.2559, -1.1757,  0.5475,  0.5596,  0.1200, -0.7053,
          0.2003, -0.6909,  0.0458, -0.6171, -0.3177, -1.5047, -1.3826, -0.4256,
          0.8921, -0.9193, -0.9078,  0.2017, -1.9540, -1.1114,  1.7960, -1.2983,
          0.9973, -0.0068,  0.0578, -0.3034, -0.3879, -1.7129,  0.1869,  1.9621]])
inputs: tensor([[ 0.7523,  2.6620, -0.3555,  0.7638, -0.3027,  0.9788, -1.9908,  0.3391,
          1.4979, -0.6132,  1.0638, -1.6855,  0.4368, -0.5107, -0.5266,  2.0736,
          0.5547,  1.9405,  0.0494, -0.4679, -0.9783,  0.3979,  0.5574, -0.0319,
          0.8016, -1.4584,  0.6052, -1.2437, -1.1315,  0.1944, -1.4731, -0.0727,
         -1.1874,  1.2561, -1.1548,  1.2036, -0.1809, -0.0750,  0.0230,  0.8032,
          0.7380,  1.8496,  0.4837, -1.5867,  0.1092, -1.8601,  1.0840,  1.0974,
         -0.7771, -1.2941,  1.9953,  0.1119, -0.9055, -2.3305, -0.0821, -0.9397,
          0.6400,  2.8995,  2.2591,  0.7272,  0.4535,  0.0455, -0.9516,  0.1513],
        [-0.0240,  1.3383, -0.4531,  0.0491,  0.9277,  0.3967,  1.1144, -1.0312,
         -0.6109, -1.7256, -0.1511,  0.8318,  1.3060,  0.0076, -0.1055, -1.1893,
          0.4162,  0.5758, -0.3197, -0.0292, -0.3551, -1.9816,  2.0662, -2.0654,
          2.2116,  1.8103, -0.1501,  1.7537, -1.3544, -0.8756,  0.4902, -0.8912,
          0.4791, -0.7841,  0.8482, -0.6068,  0.7265,  1.6602,  0.0955, -1.5102,
          1.4641, -0.1426, -1.1310, -0.7143,  0.3114, -0.6710, -1.1766,  0.0058,
         -1.4802,  1.3942,  0.9471,  0.7597, -0.2500,  0.0186, -0.7708,  0.1165,
         -0.2390,  1.7303,  0.2569,  1.6242, -0.1922, -0.8932,  0.3570,  0.6594]])
candidate_state: tensor([[-0.1488,  0.5099,  0.3296,  0.5263,  0.1305, -0.1505,  0.8713,  0.0375,
         -0.1099,  0.7244,  0.8453, -0.1518, -0.1285, -0.1192,  0.8361, -0.1623,
          0.4195, -0.1592,  0.6800, -0.1696,  0.0315, -0.1668,  0.4759,  0.6916,
         -0.1028, -0.1390, -0.1208,  0.0050,  1.0333,  1.1151, -0.1152, -0.0967,
          0.8912, -0.1543,  0.1798, -0.1680,  0.4579, -0.1591,  1.4220,  0.2381,
          1.0167, -0.0852,  0.0639, -0.1538,  0.5371,  0.3114, -0.1636,  0.1756,
         -0.1596,  0.2287,  0.1233, -0.1335, -0.1089,  0.8543, -0.1557,  1.5388,
         -0.1632, -0.0414, -0.1596, -0.0528, -0.0988, -0.0527,  0.4454, -0.1669],
        [-0.1699, -0.0944,  0.7841,  0.1666, -0.0395,  0.9619,  0.2882, -0.0425,
          0.3661, -0.0592,  0.0745,  0.5798,  0.1708, -0.1653,  0.6857,  0.1239,
         -0.1658, -0.1699, -0.1338, -0.1316, -0.1035, -0.0811, -0.0420,  0.1198,
         -0.0345,  0.0696,  0.0263,  0.6482,  1.6582,  0.1678,  0.0300, -0.1282,
         -0.1322, -0.1540, -0.0553, -0.1393, -0.1045, -0.1696,  0.3409,  0.3694,
          0.0204, -0.1651, -0.1528,  0.4094, -0.1415, -0.1426,  0.0911,  0.5620,
         -0.0106, -0.1610,  0.4916,  0.1205,  0.7164,  0.1197,  0.2398, -0.1631,
          0.2643,  0.2600,  0.4690, -0.1605, -0.1686,  0.4946, -0.1465,  0.2930]])
new_state: tensor([[-0.0793,  0.1178,  0.1858,  0.3455, -0.0591, -0.4326,  0.3055,  0.4953,
          0.7401,  0.2162,  0.8774,  0.2052,  0.6515,  0.7138,  0.3181,  0.1857,
          0.2023, -0.5515, -0.1675, -0.1281, -0.7544,  0.0722,  0.2419,  0.3594,
          0.0914, -0.3769,  1.0543,  0.0902, -0.7001,  1.3764,  0.8045, -0.8003,
          1.0702, -0.2411,  0.2772, -0.1401,  0.0332, -0.6350, -0.2268, -0.4758,
          0.9753, -0.1643,  0.1915,  0.3010,  0.5533,  0.7793, -0.0659, -0.0368,
         -0.3274, -0.5995,  0.0166, -0.3013,  0.1707,  0.5926,  0.0775, -0.5520,
         -1.2984, -0.3516,  0.6577,  0.1597, -0.1633, -0.9306,  0.6562, -0.4986],
        [-0.2888, -0.5402,  0.3618, -0.3758, -0.4292,  0.2441, -0.0151, -0.5498,
         -0.3126, -0.9863,  0.5411, -0.7220, -0.0118, -0.4196, -0.2111,  0.0048,
          0.0315,  0.0121, -0.2206, -0.4298, -0.1132, -0.2583, -1.0390, -0.3636,
         -0.0949,  0.0693, -0.0139, -0.3107,  0.6298,  0.1541, -0.2680, -0.0444,
          0.0176, -0.3676, -0.1474, -0.6152,  0.1949,  0.1653,  0.2394, -0.1241,
          0.1030, -0.4065, -0.0616, -0.0619, -0.2224, -0.7680, -0.5856,  0.1086,
          0.4039, -0.5092, -0.1510,  0.1578, -0.5098, -0.4455,  0.9544, -0.6844,
          0.6009,  0.1375,  0.2802, -0.2261, -0.2693, -0.5190,  0.0066,  1.0594]])
______________ TestConsciousnessStateManager.test_adaptive_gating ______________

self = <test_consciousness_state_management.TestConsciousnessStateManager object at 0x7b4a4aeafe80>
device = device(type='cpu'), state_manager = ConsciousnessStateManager()

    def test_adaptive_gating(self, device, state_manager):
        batch_size = 2
        hidden_dim = 64
    
        state = torch.randn(batch_size, hidden_dim, device=device)
    
        state_manager.eval()
        with torch.no_grad():
            # Test adaptation to different input patterns
            # Case 1: Similar input to current state
            similar_input = state + torch.randn_like(state) * 0.1
            _, metrics1 = state_manager(state, similar_input, threshold=0.5, deterministic=True)
    
            # Case 2: Very different input
            different_input = torch.randn(batch_size, hidden_dim, device=device)
            _, metrics2 = state_manager(state, different_input, threshold=0.5, deterministic=True)
    
        # Memory gate should be more open (lower values) for different inputs
        assert torch.mean(metrics1['memory_gate']) > torch.mean(metrics2['memory_gate'])
    
        # Energy cost should be higher for more different inputs
        assert metrics2['energy_cost'].item() > metrics1['energy_cost'].item()
        # Test memory gate properties
>       assert metrics1['memory_gate'].shape == (batch_size, hidden_dim)  # Updated shape
E       assert torch.Size([2, 1]) == (2, 64)
E         
E         At index 1 diff: 1 != 64
E         Use -v to get more diff

tests/unit/state/test_consciousness_state_management.py:131: AssertionError
----------------------------- Captured stdout call -----------------------------
memory_gate: tensor([[0.5062],
        [0.5043]])
state: tensor([[ 0.0506, -0.2398,  1.6421,  0.2121,  0.4183, -0.3951,  0.0857, -0.7813,
         -0.4785,  0.0983,  0.4213, -1.8525,  0.8484,  0.3719, -0.3180,  1.6484,
         -0.1868, -0.5029, -0.8807, -0.2865, -1.7898,  0.7489,  1.3528,  0.4826,
          1.2925,  0.3711,  0.9781, -1.6529, -2.0106,  1.0006,  0.2524,  1.4082,
         -1.0446,  1.9176,  0.8499,  2.8470,  0.1242,  0.1507,  0.0764,  0.5535,
          0.1754,  0.1945,  0.2193, -0.1892,  0.2160,  0.1787, -0.1720,  1.1447,
         -0.2067,  0.0790, -0.5909,  0.3940,  2.8180, -0.7932, -0.8626,  0.3280,
          0.8058, -0.5039,  2.9940, -1.1779,  0.1657, -0.4336,  1.2542,  0.8841],
        [-0.8410,  0.6608,  0.0290, -0.3482,  1.6426,  0.7160,  0.1910, -1.3790,
         -0.9635, -0.8112,  0.7335, -2.6119,  1.9640, -0.2729,  0.7134,  1.5322,
         -2.0654, -1.4775,  1.4185, -0.9391,  1.8017, -0.0064,  2.7329, -0.5136,
         -0.4366,  0.3185, -0.2112,  0.8384, -2.1639, -1.0864,  0.7869, -1.5377,
         -0.7072, -1.6139,  0.9718,  0.1794,  2.0044,  0.4906, -0.9631, -0.6274,
          0.0555,  0.0521,  1.2565,  0.2417, -0.4799, -0.4519, -0.9609, -0.7552,
          0.2185, -0.1004, -0.3405, -0.1499,  0.1147, -1.5094, -0.6797, -0.1746,
         -2.1251, -0.8197,  0.5801,  0.9124,  0.6021,  1.8838,  1.6073,  0.0199]])
inputs: tensor([[ 0.0338, -0.2667,  1.6725,  0.3937,  0.1340, -0.4424,  0.0045, -0.7433,
         -0.4430,  0.0117,  0.3178, -1.9181,  0.6798,  0.2779, -0.2679,  1.7328,
         -0.2127, -0.5624, -0.9237, -0.3380, -1.4841,  0.7176,  1.4444,  0.4850,
          1.1608,  0.2265,  1.0889, -1.5520, -2.0845,  1.0676,  0.1799,  1.3778,
         -0.9500,  1.9225,  0.8332,  2.9720,  0.0696,  0.2008,  0.2611,  0.5784,
          0.0589,  0.0756,  0.1202, -0.1545,  0.3159,  0.0598, -0.2957,  1.2115,
         -0.3158,  0.0070, -0.3777,  0.3148,  2.7799, -0.8709, -0.8659,  0.3862,
          0.8072, -0.3920,  3.0520, -1.1034,  0.1750, -0.3997,  1.3462,  0.9382],
        [-0.7702,  0.8175,  0.1366, -0.2156,  1.6811,  0.8306,  0.2065, -1.3656,
         -0.9539, -0.7773,  0.7894, -2.5896,  1.9660, -0.3568,  0.6637,  1.3902,
         -2.2831, -1.5512,  1.5264, -0.8132,  1.8339, -0.1022,  2.7984, -0.4815,
         -0.3301,  0.3858, -0.1618,  0.9219, -2.1473, -1.0390,  0.8130, -1.3548,
         -0.7012, -1.5288,  0.9455,  0.0747,  2.1097,  0.3419, -0.9170, -0.5872,
         -0.0684,  0.0134,  1.1948,  0.2618, -0.4034, -0.3946, -0.8363, -0.5991,
          0.2124, -0.2520, -0.3034, -0.0122,  0.1790, -1.5616, -0.6506, -0.1741,
         -2.1074, -0.9239,  0.5375,  1.0369,  0.3538,  1.8186,  1.4305, -0.0317]])
candidate_state: tensor([[-0.1682,  0.6305,  0.1391, -0.0379, -0.1103,  0.3924, -0.1451, -0.0442,
         -0.1665,  0.4044,  0.4181, -0.1691,  0.1830, -0.1562,  1.3820,  0.1628,
          0.5870, -0.1204, -0.1648,  0.2042,  0.0785,  0.1517,  0.8893, -0.0054,
          0.3180, -0.0830,  0.0130,  0.3623, -0.1452,  0.9234,  0.0261, -0.1624,
         -0.1161, -0.1522,  0.0030, -0.1676, -0.1252, -0.0715,  0.0391, -0.1669,
         -0.1031,  0.5322,  0.0971,  0.1577, -0.1697,  0.4733, -0.1690,  0.5334,
         -0.1694, -0.1700,  0.0745, -0.1681, -0.1531,  0.3414, -0.1697,  0.2968,
         -0.1589,  0.3600, -0.0283, -0.1025, -0.1692,  0.1467,  0.0326,  0.0555],
        [ 0.0959,  0.0024, -0.1688,  0.1496,  0.2337,  0.2310, -0.1189, -0.1248,
          0.0291, -0.1688, -0.0878,  0.0423,  0.9503, -0.1578,  0.1605,  2.1299,
          0.8396,  0.2894, -0.0953,  0.8452,  0.9245, -0.1420,  0.5159, -0.1498,
         -0.0661, -0.1692,  0.0277,  0.6670,  0.1216, -0.1689,  1.2452, -0.1661,
          0.6279,  0.5342, -0.1416,  0.8534, -0.0500,  1.1048,  0.1970, -0.0544,
         -0.0465, -0.1682,  0.0205,  0.2069, -0.1674,  0.1215, -0.1691, -0.1669,
          0.1010,  0.2588, -0.1273,  0.1399, -0.0859,  0.1891,  0.7670,  0.8385,
          0.0037,  0.3226, -0.0525, -0.1687, -0.1443,  0.1874, -0.0850, -0.1620]])
new_state: tensor([[-5.7413e-02,  1.8997e-01,  8.9996e-01,  8.8667e-02,  1.5728e-01,
         -6.2581e-03, -2.8236e-02, -4.1736e-01, -3.2448e-01,  2.4944e-01,
          4.1975e-01, -1.0213e+00,  5.1983e-01,  1.1115e-01,  5.2137e-01,
          9.1485e-01,  1.9530e-01, -3.1400e-01, -5.2724e-01, -4.4224e-02,
         -8.6728e-01,  4.5400e-01,  1.1239e+00,  2.4165e-01,  8.1129e-01,
          1.4691e-01,  5.0157e-01, -6.5786e-01, -1.0895e+00,  9.6247e-01,
          1.4063e-01,  6.3267e-01, -5.8614e-01,  8.9561e-01,  4.3172e-01,
          1.3585e+00,  1.0695e-03,  4.0988e-02,  5.7962e-02,  1.9777e-01,
          3.7871e-02,  3.6124e-01,  1.5894e-01, -1.7939e-02,  2.5594e-02,
          3.2418e-01, -1.7053e-01,  8.4289e-01, -1.8827e-01, -4.3938e-02,
         -2.6239e-01,  1.1647e-01,  1.3510e+00, -2.3295e-01, -5.2048e-01,
          3.1261e-01,  3.2943e-01, -7.7295e-02,  1.5016e+00, -6.4692e-01,
          3.1950e-04, -1.4709e-01,  6.5099e-01,  4.7498e-01],
        [-3.7657e-01,  3.3442e-01, -6.9044e-02, -1.0146e-01,  9.4426e-01,
          4.7562e-01,  3.7356e-02, -7.5734e-01, -4.7149e-01, -4.9274e-01,
          3.2641e-01, -1.2963e+00,  1.4616e+00, -2.1585e-01,  4.3931e-01,
          1.8285e+00, -6.2549e-01, -6.0169e-01,  6.6817e-01, -5.4695e-02,
          1.3669e+00, -7.3631e-02,  1.6340e+00, -3.3325e-01, -2.5295e-01,
          7.6780e-02, -9.2802e-02,  7.5344e-01, -1.0310e+00, -6.3160e-01,
          1.0141e+00, -8.5782e-01, -4.5441e-02, -5.4919e-01,  4.1991e-01,
          5.1347e-01,  9.8607e-01,  7.9507e-01, -3.8805e-01, -3.4340e-01,
          4.9378e-03, -5.7109e-02,  6.4384e-01,  2.2446e-01, -3.2501e-01,
         -1.6764e-01, -5.6840e-01, -4.6361e-01,  1.6022e-01,  7.7690e-02,
         -2.3483e-01, -6.2226e-03,  1.5284e-02, -6.6753e-01,  3.7405e-02,
          3.2753e-01, -1.0699e+00, -2.5349e-01,  2.6655e-01,  3.7655e-01,
          2.3213e-01,  1.0429e+00,  7.6850e-01, -7.0287e-02]])
memory_gate: tensor([[0.4690],
        [0.4612]])
state: tensor([[ 0.0506, -0.2398,  1.6421,  0.2121,  0.4183, -0.3951,  0.0857, -0.7813,
         -0.4785,  0.0983,  0.4213, -1.8525,  0.8484,  0.3719, -0.3180,  1.6484,
         -0.1868, -0.5029, -0.8807, -0.2865, -1.7898,  0.7489,  1.3528,  0.4826,
          1.2925,  0.3711,  0.9781, -1.6529, -2.0106,  1.0006,  0.2524,  1.4082,
         -1.0446,  1.9176,  0.8499,  2.8470,  0.1242,  0.1507,  0.0764,  0.5535,
          0.1754,  0.1945,  0.2193, -0.1892,  0.2160,  0.1787, -0.1720,  1.1447,
         -0.2067,  0.0790, -0.5909,  0.3940,  2.8180, -0.7932, -0.8626,  0.3280,
          0.8058, -0.5039,  2.9940, -1.1779,  0.1657, -0.4336,  1.2542,  0.8841],
        [-0.8410,  0.6608,  0.0290, -0.3482,  1.6426,  0.7160,  0.1910, -1.3790,
         -0.9635, -0.8112,  0.7335, -2.6119,  1.9640, -0.2729,  0.7134,  1.5322,
         -2.0654, -1.4775,  1.4185, -0.9391,  1.8017, -0.0064,  2.7329, -0.5136,
         -0.4366,  0.3185, -0.2112,  0.8384, -2.1639, -1.0864,  0.7869, -1.5377,
         -0.7072, -1.6139,  0.9718,  0.1794,  2.0044,  0.4906, -0.9631, -0.6274,
          0.0555,  0.0521,  1.2565,  0.2417, -0.4799, -0.4519, -0.9609, -0.7552,
          0.2185, -0.1004, -0.3405, -0.1499,  0.1147, -1.5094, -0.6797, -0.1746,
         -2.1251, -0.8197,  0.5801,  0.9124,  0.6021,  1.8838,  1.6073,  0.0199]])
inputs: tensor([[-0.2376, -0.8244,  0.0904,  1.0773,  1.6568, -0.6134, -0.3269,  0.4040,
          0.7507,  0.1084, -0.1612, -0.3087, -1.3866,  0.8046, -2.2694, -0.1306,
         -0.2509,  1.1359,  0.7172, -0.3170, -0.5547, -0.9630, -1.0305,  0.0547,
         -0.6412,  2.2443,  0.4454, -0.3746,  1.0913,  0.7033, -0.0867,  1.2605,
         -1.7285,  1.3198, -2.2360, -0.2569, -3.3546, -0.3919, -0.6417, -0.1279,
         -0.3921, -0.4460, -0.8659,  0.6949, -0.2819, -0.5097, -0.6715, -2.1719,
         -0.9818, -2.2484,  0.9584, -0.8479,  1.7785,  0.5551, -0.4691,  1.7394,
          0.7342, -0.4807, -0.1376, -2.8601,  2.9471, -0.7846, -0.1871, -0.7135],
        [ 0.8606,  0.0349, -0.3735,  1.4755, -0.5934, -0.4573,  0.8228,  0.7281,
         -0.7496, -0.5768,  1.8176, -2.0233, -0.0195,  0.8657, -0.4995, -0.1977,
          0.1576,  1.1969,  1.3169,  0.7488, -0.2331,  1.8397,  0.4234, -1.8047,
         -0.1546, -0.7393,  0.3111,  0.8368, -0.2433, -1.0932,  1.6917,  1.4386,
         -0.2131, -0.8554,  0.6413, -0.2498,  1.1356,  1.7848,  0.9047,  0.5775,
         -0.4473, -1.7346,  0.1283,  1.9389,  1.3048, -0.0050,  0.4794, -0.8606,
         -0.4214,  0.4029,  0.4657, -1.2204,  0.2184,  0.4402, -1.8191,  2.0318,
         -0.7470,  0.0808,  0.5464,  0.0916, -1.6665, -0.3981, -1.3921,  0.0712]])
candidate_state: tensor([[-0.1572, -0.1330, -0.0768,  0.0047, -0.1263,  0.2731,  0.1852, -0.1591,
          0.2530, -0.1343, -0.0781,  0.4754,  1.5688,  0.1246,  0.5078,  0.8055,
         -0.0852, -0.1391,  0.2882, -0.1449, -0.0747, -0.1401, -0.0021,  0.2782,
          0.0717,  0.6166, -0.1678, -0.1639, -0.1700, -0.0290,  0.1495,  0.0968,
          0.4199, -0.1593, -0.1522,  0.1278,  1.0590,  0.3123,  1.2636,  0.2317,
          0.1405, -0.1617,  1.8838, -0.1248, -0.0811, -0.1627,  0.2076, -0.1527,
         -0.1416, -0.0373, -0.0943, -0.0851,  0.1925,  0.6153, -0.1226,  0.4642,
         -0.1663, -0.0485,  0.1012, -0.0961, -0.0811, -0.1168,  1.4008, -0.1170],
        [-0.0912,  0.5933, -0.0997, -0.1519, -0.1473, -0.0810, -0.1700, -0.1618,
         -0.0498, -0.0392, -0.1267,  0.2706, -0.1667, -0.1316, -0.0479, -0.1696,
          0.7547,  0.0547,  0.7994, -0.0702,  0.2734,  0.3543,  0.2105,  0.2028,
          0.6207,  0.0104, -0.0574, -0.0437, -0.1686, -0.1612, -0.1570,  0.4266,
         -0.0116,  0.2529, -0.1450,  0.7103,  0.7853, -0.1583,  0.5463, -0.1573,
          0.2063,  1.8892, -0.0067,  0.2862,  0.2535, -0.0541,  0.1101,  0.2193,
          1.6806,  0.1290,  0.3082,  0.3273, -0.0456, -0.1548, -0.0569, -0.1295,
          0.2808, -0.1299,  0.0923,  0.9996, -0.1251,  0.8161,  0.3616, -0.1024]])
new_state: tensor([[-0.0597, -0.1831,  0.7293,  0.1020,  0.1291, -0.0403,  0.1385, -0.4509,
         -0.0901, -0.0252,  0.1561, -0.6164,  1.2309,  0.2406,  0.1205,  1.2008,
         -0.1328, -0.3097, -0.2601, -0.2113, -0.8791,  0.2768,  0.6333,  0.3741,
          0.6443,  0.5015,  0.3696, -0.8622, -1.0332,  0.4539,  0.1977,  0.7118,
         -0.2670,  0.8148,  0.3178,  1.4032,  0.6206,  0.2365,  0.7068,  0.3826,
          0.1569,  0.0054,  1.1031, -0.1550,  0.0583, -0.0026,  0.0296,  0.4558,
         -0.1721,  0.0172, -0.3272,  0.1396,  1.4239, -0.0453, -0.4697,  0.4003,
          0.2896, -0.2621,  1.4579, -0.6035,  0.0347, -0.2654,  1.3320,  0.3526],
        [-0.4370,  0.6244, -0.0403, -0.2425,  0.6782,  0.2866, -0.0035, -0.7231,
         -0.4712, -0.3952,  0.2700, -1.0587,  0.8159, -0.1968,  0.3032,  0.6152,
         -0.5459, -0.6519,  1.0849, -0.4709,  0.9782,  0.1879,  1.3738, -0.1276,
          0.1331,  0.1525, -0.1283,  0.3631, -1.0888, -0.5879,  0.2783, -0.4793,
         -0.3324, -0.6081,  0.3700,  0.4655,  1.3475,  0.1410, -0.1498, -0.3741,
          0.1368,  1.0420,  0.5758,  0.2656, -0.0847, -0.2375, -0.3838, -0.2301,
          1.0063,  0.0232,  0.0090,  0.1073,  0.0283, -0.7795, -0.3441, -0.1503,
         -0.8287, -0.4480,  0.3172,  0.9594,  0.2103,  1.3085,  0.9361, -0.0460]])
=============================== warnings summary ===============================
../../.local/lib/python3.10/site-packages/torch/__init__.py:1144
  /home/kasinadhsarma/.local/lib/python3.10/site-packages/torch/__init__.py:1144: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)
    _C._set_default_tensor_type(t)

tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_model.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    inputs = {k: torch.tensor(v, dtype=torch.float32) for k, v in inputs.items()}

tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_model.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    state = torch.tensor(state, dtype=torch.float32)

tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_consistency
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_energy_efficiency
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_value_estimation
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    state = torch.tensor(state, dtype=torch.float32)

tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_state_consistency
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_updates
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_rl_optimization
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_energy_efficiency
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_value_estimation
tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
  /home/kasinadhsarma/experiment/cognition-l3-experiment/models/consciousness_state.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    inputs = torch.tensor(inputs, dtype=torch.float32)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_forward_pass
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_state_update
FAILED tests/test_consciousness.py::TestConsciousnessModel::test_model_attention_weights
FAILED tests/test_environment.py::EnvironmentTests::test_core_imports - Runti...
FAILED tests/test_environment.py::EnvironmentTests::test_framework_versions
FAILED tests/test_environment.py::EnvironmentTests::test_hardware_detection
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_scaled_dot_product
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_attention_mask
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_consciousness_broadcasting
FAILED tests/unit/attention/test_attention.py::TestAttentionMechanisms::test_global_workspace_integration
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cross_modal_attention
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_modality_specific_processing
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_integration_stability
FAILED tests/unit/integration/test_cognitive_integration.py::TestCognitiveProcessIntegration::test_cognitive_integration
FAILED tests/unit/integration/test_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_phi_metric_computation
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_information_flow
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_entropy_calculations
FAILED tests/unit/memory/test_integration.py::TestInformationIntegration::test_memory_integration
FAILED tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_state_updates
FAILED tests/unit/memory/test_memory_components.py::TestGRUCell::test_gru_reset_gate
FAILED tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_sequence_processing
FAILED tests/unit/memory/test_memory_components.py::TestWorkingMemory::test_memory_retention
FAILED tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_state_updates
FAILED tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_rl_optimization
FAILED tests/unit/state/test_consciousness_state_management.py::TestConsciousnessStateManager::test_adaptive_gating
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_sequence_processing
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_context_aware_gating
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_information_integration
ERROR tests/unit/memory/test_memory.py::TestMemoryComponents::test_memory_retention
============= 26 failed, 15 passed, 24 warnings, 4 errors in 2.05s =============
